{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e370c-40cb-4194-bf97-ad9e9ab745be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seqgen.seq_gen as g\n",
    "import random\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197faa56-4d2f-4453-bbda-32c5b4115b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d88d5-47ef-4b8f-9d41-e1d8fb70c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target_seqs = g.generate_synthetic_training_data(10, max_length=10, device=device, swap_times=0)\n",
    "input_seqs = torch.Tensor(features[:, :, 0]).to(torch.int64)\n",
    "coordinates = torch.Tensor(features[:, :, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f41f6-057b-41e5-9d52-1744666d4aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape, input_seqs.shape, coordinates.shape, target_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3288c872-f5fa-4510-a1a0-6937955777df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first three encoded input sequences\n",
    "input_seqs[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec1852-a509-4bbb-96e3-6e18f04d0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the coordinates of the tokens of the first input sequence\n",
    "coordinates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726fd55-586e-43e0-a8bb-2f508dd0a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first three encoded output sequences\n",
    "target_seqs[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e0ccc-ca5b-40fa-81f5-303ec14d8122",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "\n",
    "The embedding layers maps each token to a vector space of dimension $\\mathbb{R}^{D_{emb}}$.\n",
    "If we have an input sequence `[5,3,4]` and $D_{emb} = 2$ the output may look like this: `[[0.319, 0.841], [0.781, 0.682], [0.432,0.968]]`.\n",
    "\n",
    "The embedding layer expects an input sequence of type `int` where each integer in the input sequence represents a class. The total number of distinct possible classes of the input sequence is called the vocabulary size $N_{vocab}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2108243-7c86-40f0-9aaa-f05b03a02719",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = torch.nn.Embedding(num_embeddings=17, embedding_dim=2).to(device)\n",
    "x_emb = emb(input_seqs)\n",
    "x_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c784b8e-d48f-41d8-a649-fbc9a77e0795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show embedding of first input sequence\n",
    "x_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee930e3-ab5c-44e2-b352-75ad62665be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate embeddings and coordinates\n",
    "emb_cat = torch.cat([x_emb, coordinates], dim=2)\n",
    "emb_cat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243fbc0-7f29-4052-be95-48deca5693aa",
   "metadata": {},
   "source": [
    "## LSTM Layer\n",
    "\n",
    "The LSTM layer implements recursion in a neural net. It will need three hyperparameters:\n",
    "- **input_size**: This is the dimension of the input vectors that are run through the LSTM layer. If the vectors have been run to an embedding layer before input_size must be equal to the argument embedding_dim of the embedding layer\n",
    "- **hidden_size**: This is the dimension of the internal state vector $h_n$, which is identical to the dimension of the cell state $c_n$ and the dimension of the output vectors $out$. The hidden size can be freely chosen by you. Small values for hidden_size may leed to underfitting, but large values can cause overfitting.\n",
    "- **num_layers**: This parameter defines how many layers of LSTMs are stacked in the network. The more layers you stack the more complex patterns the LSTM is able to model, but this also comes with te risk of overfitting the data.\n",
    "\n",
    "There is also another important parameter:\n",
    "- **batch_first**: If the input tensor of the LSTM layer is of shape `(batch_size, sequence_length, embedding_dim)` you will have to set this parameter to True. Otherwise if the input is of shape `(sequence_length, embedding_dim, batch_size)` you will have to set this parameter to false.\n",
    "\n",
    "Now let's look at the outputs of the LSTM layer:\n",
    "- **output**: This is the predicted tensor of the LSTM layer which will be passed to the next layer. You may add a linear classification and a softmax layer after the LSTM layer. The output tensor is of shape `(batch_size, sequence_length, hidden_size)` if `batch_first` is set to true.\n",
    "- **h_n**: Hidden state, tensor of shape `(num_layers, batch_size, hidden_size)`\n",
    "- **c_n**: Cell state, tensor of shape `(num_layers, batch_size, hidden_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37afe240-626b-41fd-aa43-621c531bdae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size=4\n",
    "lstm = torch.nn.LSTM(input_size=6, hidden_size=hidden_size, num_layers=7, batch_first=True).to(device)\n",
    "lstm_output, (h_n, c_n) = lstm(emb_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3408b76e-1418-4648-9ad6-c97780f86c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_output.shape, h_n.shape, c_n.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2b09f-9782-4993-84f6-c059d98e246e",
   "metadata": {},
   "source": [
    "## Linear classifier\n",
    "\n",
    "After the tensors have been passed trough the LSTM layer it is time to implement a classification of these tensors. The linear layer's task is to take the output of the LSTM layer and map it to the output classes. In language models these classes would be the characters or words of the output vocabulary. There are two hyperparameters of the linear layer that we have to set:\n",
    "\n",
    "- **in_features**: This is the dimension of the vectors that represent the words in our sequences. When these vectors come from an LSTM layer the dimension of the input features is equal to the hidden_size value of the LSTM layer.\n",
    "- **out_features**: The dimension of the output vectors of the linear layer is equal to the number of characters / words of our output vocabulary. If we want to produce englisch sentences with our model and there are 5000 possible words in our vocabulary this parameter's value would be 5000.\n",
    "\n",
    "The output of the linear layer is of shape `(batch_size, sequence_length, target_vocab_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199b7a1-7a9c-4503-a73c-8e9712b6b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 5000\n",
    "linear = torch.nn.Linear(in_features=hidden_size, out_features=output_size).to(device)\n",
    "linear_output = linear(lstm_output)\n",
    "linear_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ac623-fd3d-4722-aa21-5bb4efc4b3f2",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "\n",
    "The purpose of the softmax layer is to compute a probability for each position and each word of the output vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123acfab-8a23-4ecc-b9d4-67207e617f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.LogSoftmax(dim=1)\n",
    "softmax_output = softmax(linear_output)\n",
    "softmax_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae84cc-67fe-4487-b119-51c1e6563d7c",
   "metadata": {},
   "source": [
    "# The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c25b8f-93b5-4493-81e4-fb6c1f3a67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqgen.model import seq2seq_lstm\n",
    "from seqgen.vocabulary import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7974696-6918-47ff-93b7-14cc4517dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "num_layers=2\n",
    "embedding_dim = 200\n",
    "hidden_size=200\n",
    "batch_size=50\n",
    "max_length=25\n",
    "bidirectional=True\n",
    "\n",
    "load_from_checkpoint = False\n",
    "checkpoint_file = \"model_2022-12-24_10-29-55.pt\"\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "encoder = seq2seq_lstm.EncoderRNN(vocab_size=len(vocab_in), embedding_dim=embedding_dim, num_layers=num_layers, hidden_size=hidden_size, bidirectional=bidirectional).to(features.device)\n",
    "decoder = seq2seq_lstm.DecoderRNN(embedding_dim=embedding_dim, num_layers=num_layers, hidden_size=hidden_size, vocab_size=len(vocab_out), bidirectional=bidirectional).to(features.device)\n",
    "\n",
    "# Initialize optimizer for encoder and decoder\n",
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Load model weights from checkpoint\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    encoder.load_state_dict(checkpoint['encoder_model_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_model_state_dict'])\n",
    "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "    num_layers = checkpoint['num_layers']\n",
    "    embedding_dim = checkpoint['embedding_dim']\n",
    "    hidden_size = checkpoint['hidden_size']\n",
    "    bidirectional = checkpoint['bidirectional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d63a2d8-bced-44ec-b32b-19d5f3055885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the encoder hidden state and cell state with zeros\n",
    "hn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "cn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "print(hn.shape, cn.shape)\n",
    "\n",
    "# Iterate over the sequence words and run every word through the encoder\n",
    "for i in range(input_seqs.shape[1]):\n",
    "    # Run the i-th word of the input sequence through the encoder.\n",
    "    # As a result we will get the prediction (output), the hidden state and the cell state.\n",
    "    # The hidden state and cell state will be used as inputs in the next round\n",
    "    print(f\"Run word {i+1} of all {input_seqs.shape[0]} sequences through the encoder\")\n",
    "    output, (hn, cn) = encoder(input_seqs[:, i].unsqueeze(dim=1), coordinates[:, i], (hn, cn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3c2a2-1b0a-404a-9d8c-0ae7fbc8485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape, hn.shape, cn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29054ba-e5b3-405f-bc5b-149f8862da81",
   "metadata": {},
   "source": [
    "# The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda91121-2c2d-4531-b7ee-d67096698123",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "\n",
    "# Iterate over words of target sequence and run words through the decoder.\n",
    "# This will produce a prediction for the next word in the sequence\n",
    "for i in range(0, target_seqs.size(1)):\n",
    "    print(f\"Run word {i+1} through decoder\")\n",
    "    output, (hn, cn) = decoder(\n",
    "        x=target_seqs[:, i].unsqueeze(dim=1),\n",
    "        coordinates=coordinates[:, i],\n",
    "        hidden=(hn, cn)\n",
    "    )\n",
    "    loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "\n",
    "print(\"LOSS\", loss.item() / max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f9ad9-7ae1-4b49-99b8-490eacfd5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "for epoch in range(100000):\n",
    "    # With a certain chance present the model the true predictions\n",
    "    # instead of its own predictions in the next iteration\n",
    "    use_teacher_forcing_prob = 0.5\n",
    "    use_teacher_forcing = random.random() < use_teacher_forcing_prob\n",
    "    \n",
    "    # Get a batch of trianing data\n",
    "    features, target_seqs = g.generate_synthetic_training_data(batch_size, max_length=max_length, continue_prob=0.99, device=device, swap_times=8)\n",
    "    features = features.to(device)\n",
    "    target_seqs = target_seqs.to(device)\n",
    "    input_seqs = torch.Tensor(features[:, :, 0]).to(torch.int64)\n",
    "    coordinates = torch.Tensor(features[:, :, 1:])\n",
    "\n",
    "    # Initialize the encoder hidden state and cell state with zeros\n",
    "    hn_enc = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "    cn_enc = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "    \n",
    "    # Set gradients of all model parameters to zero\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "    \n",
    "    ####################\n",
    "    #     ENCODING     #\n",
    "    ####################\n",
    "\n",
    "    # Iterate over the sequence words and run every word through the encoder\n",
    "    for i in range(input_seqs.shape[1]):\n",
    "        # Run the i-th word of the input sequence through the encoder.\n",
    "        # As a result we will get the prediction (output), the hidden state (hn) and the cell state (cn).\n",
    "        # The hidden state and cell state will be used as inputs in the next round\n",
    "        output, (hn_enc, cn_enc) = encoder(\n",
    "            input_seqs[:, i].unsqueeze(dim=1),\n",
    "            coordinates[:, i],\n",
    "            (hn_enc, cn_enc)\n",
    "        )\n",
    "        \n",
    "    ####################\n",
    "    #     DECODING     #\n",
    "    ####################\n",
    "\n",
    "    # The first words that we be presented to the model is the '<start>' token\n",
    "    prediction = target_seqs[:, 0]\n",
    "    \n",
    "    # The initial hidden state of the decoder is the final hidden state of the decoder\n",
    "    hn_dec, cn_dec = hn_enc, cn_enc\n",
    "    \n",
    "    # Iterate over words of target sequence and run words through the decoder.\n",
    "    # This will produce a prediction for the next word in the sequence\n",
    "    for i in range(1, target_seqs.size(1)):\n",
    "        # Run word i through decoder and get word i+1 and the new hidden state as outputs\n",
    "        if use_teacher_forcing:\n",
    "            output, (hn_dec, cn_dec) = decoder(\n",
    "                target_seqs[:, i-1].unsqueeze(dim=1),\n",
    "                coordinates[:, i-1],\n",
    "                (hn_dec, cn_dec)\n",
    "            )\n",
    "        else:\n",
    "            output, (hn_dec, cn_dec) = decoder(\n",
    "                prediction.unsqueeze(dim=1),\n",
    "                coordinates[:, i-1],\n",
    "                (hn_dec, cn_dec)\n",
    "            )\n",
    "\n",
    "            # Get the predicted classes of the model\n",
    "            topv, topi = output.topk(1)\n",
    "            prediction = topi.squeeze()    \n",
    "        loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    if not epoch % 100:\n",
    "        print(f\"LOSS after epoch {epoch}\", loss.item() / target_seqs.size(1))\n",
    "\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights of encoder and decoder\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e3605-8f59-42ee-b260-5bb500518d00",
   "metadata": {},
   "source": [
    "#### Save model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67bec2-ccfd-4492-bd1a-e51fad94fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "model_data = {\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length\n",
    "}\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'encoder_model_state_dict': encoder.state_dict(),\n",
    "    'decoder_model_state_dict': decoder.state_dict(),\n",
    "    'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "    'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"bidirectional\": bidirectional,\n",
    "}, \"model_\" + date_time + \".pt\")\n",
    "\n",
    "\n",
    "with open(\"training_\" + date_time + '.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56c813-13d3-45a2-b9e1-47fed03bcf3c",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "We run our input sequences through the model and get output seuences. Then we decode the output sequences with the Vocabulary class and get our final latex code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb8ff7-8d9a-4c28-8f95-264703cfe636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_seqs, coordinates, target_seqs):\n",
    "    vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "    vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "    predictions = torch.zeros(target_seqs.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize the encoder hidden state and cell state with zeros\n",
    "        hn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "        cn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "\n",
    "        # Iterate over the sequence words and run every word through the encoder\n",
    "        for i in range(input_seqs.shape[1]):\n",
    "            output, (hn, cn) = encoder(\n",
    "                input_seqs[:, i].unsqueeze(dim=1),\n",
    "                coordinates[:, i],\n",
    "                (hn, cn)\n",
    "            )\n",
    "\n",
    "        # Predict tokens of the target sequence by running the hidden state through\n",
    "        # the decoder\n",
    "        for i in range(0, target_seqs.size(1)):\n",
    "            output, (hn, cn) = decoder(\n",
    "                target_seqs[:, i].unsqueeze(dim=1),\n",
    "                coordinates[:, i],\n",
    "                (hn, cn)\n",
    "            )\n",
    "            # Select the indices of the most likely tokens\n",
    "            predicted_char = torch.argmax(output, dim=2)\n",
    "            predictions[:, i] = torch.argmax(output, dim=2).squeeze()\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4224ca-ab02-437b-a0ea-5704b1c01daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict(input_seqs[0:1], coordinates[0:1], target_seqs[0:1])\n",
    "input_seqs[0:1], prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476c3b2-d8ad-4506-8ace-814d9e9eefcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_swapped = g.random_swap(input_seqs[0], i=2).unsqueeze(dim=0)\n",
    "coords_swapped = g.random_swap(coordinates[0], i=2).unsqueeze(dim=0)\n",
    "prediction_swapped = predict(in_swapped, coords_swapped, target_seqs[0:1])\n",
    "in_swapped, prediction_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0408b19-3288-48ac-905b-88eb6ef4017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs[0:1] == in_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb4a03-69bd-45e1-960f-244da1c18417",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction == prediction_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf18d6-94da-42ce-8bad-86a9f2d16d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random sequence and its prediction from the model\n",
    "import random\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "predictions = predict(input_seqs, coordinates, target_seqs)\n",
    "\n",
    "i = random.randint(0, predictions.size(0))\n",
    "print(\"MODEL INPUT\", vocab_in.decode_sequence(input_seqs[i].cpu().numpy()))\n",
    "print(\"MODEL OUTPUT\", vocab_out.decode_sequence(predictions[i].cpu().numpy()))\n",
    "print(\"TARGET OUTPUT\", vocab_out.decode_sequence(target_seqs[i][1:].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeae19b-77d4-4715-8068-9822af23009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = vocab_out.decode_sequence(predictions[i].cpu().numpy())\n",
    "prediction = list(filter(lambda x: x != '<end>', prediction))\n",
    "prediction = \"\".join(prediction)\n",
    "print(\"MODEL OUTPUT\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b7aff-cecf-476a-bd3c-ae737ff8aa66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
