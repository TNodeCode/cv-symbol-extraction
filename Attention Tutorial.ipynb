{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d1acfd-2d96-47ff-9496-feb7fabd9541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4773886c-f7ff-40a9-ae92-759124c181a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
    "        super().__init__()\n",
    " \n",
    "        # The input dimension will the the concatenation of\n",
    "        # encoder_hidden_dim (hidden) and  decoder_hidden_dim(encoder_outputs)\n",
    "        self.attn_hidden_vector = nn.Linear(encoder_hidden_dim + decoder_hidden_dim, decoder_hidden_dim)\n",
    " \n",
    "        # We need source len number of values for n batch as the dimension\n",
    "        # of the attention weights. The attn_hidden_vector will have the\n",
    "        # dimension of [source len, batch size, decoder hidden dim]\n",
    "        # If we set the output dim of this Linear layer to 1 then the\n",
    "        # effective output dimension will be [source len, batch size]\n",
    "        self.attn_scoring_fn = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
    " \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden = [1, batch size, decoder hidden dim]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    " \n",
    "        # We need to calculate the attn_hidden for each source words.\n",
    "        # Instead of repeating this using a loop, we can duplicate\n",
    "        # hidden src_len number of times and perform the operations.\n",
    "        hidden = hidden.repeat(src_len, 1, 1)\n",
    " \n",
    "        # Calculate Attention Hidden values\n",
    "        attn_hidden = torch.tanh(self.attn_hidden_vector(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    " \n",
    "        # Calculate the Scoring function. Remove 3rd dimension.\n",
    "        attn_scoring_vector = self.attn_scoring_fn(attn_hidden).squeeze(2)\n",
    " \n",
    "        # The attn_scoring_vector has dimension of [source len, batch size]\n",
    "        # Since we need to calculate the softmax per record in the batch\n",
    "        # we will switch the dimension to [batch size,source len]\n",
    "        attn_scoring_vector = attn_scoring_vector.permute(1, 0)\n",
    " \n",
    "        # Softmax function for normalizing the weights to\n",
    "        # probability distribution\n",
    "        return F.softmax(attn_scoring_vector, dim=1)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_len, embedding_dim, encoder_hidden_dim, n_layers=1, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    " \n",
    "        self.embedding = nn.Embedding(vocab_len, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, encoder_hidden_dim, n_layers, dropout=dropout_prob)\n",
    " \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    " \n",
    "    def forward(self, input_batch):\n",
    "        embedded = self.dropout(self.embedding(input_batch))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    " \n",
    "        return outputs, hidden\n",
    "\n",
    "class OneStepDecoder(nn.Module):\n",
    "    def __init__(self, input_output_dim, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, attention, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    " \n",
    "        self.output_dim = input_output_dim\n",
    "        self.attention = attention\n",
    " \n",
    "        self.embedding = nn.Embedding(input_output_dim, embedding_dim)\n",
    " \n",
    "        # Add the encoder_hidden_dim and embedding_dim\n",
    "        self.rnn = nn.GRU(encoder_hidden_dim + embedding_dim, decoder_hidden_dim)\n",
    "        # Combine all the features for better prediction\n",
    "        self.fc = nn.Linear(encoder_hidden_dim + decoder_hidden_dim + embedding_dim, input_output_dim)\n",
    " \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # Add the source len dimension\n",
    "        input = input.unsqueeze(0)\n",
    " \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    " \n",
    "        # Calculate the attention weights\n",
    "        a = self.attention(hidden, encoder_outputs).unsqueeze(1)\n",
    " \n",
    "        # We need to perform the batch wise dot product.\n",
    "        # Hence need to shift the batch dimension to the front.\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    " \n",
    "        # Use PyTorch's bmm function to calculate the weight W.\n",
    "        W = torch.bmm(a, encoder_outputs)\n",
    " \n",
    "        # Revert the batch dimension.\n",
    "        W = W.permute(1, 0, 2)\n",
    " \n",
    "        # concatenate the previous output with W\n",
    "        rnn_input = torch.cat((embedded, W), dim=2)\n",
    " \n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    " \n",
    "        # Remove the sentence length dimension and pass them to the Linear layer\n",
    "        predicted_token = self.fc(torch.cat((output.squeeze(0), W.squeeze(0), embedded.squeeze(0)), dim=1))\n",
    " \n",
    "        return predicted_token, hidden, a.squeeze(1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, one_step_decoder, device):\n",
    "        super().__init__()\n",
    "        self.one_step_decoder = one_step_decoder\n",
    "        self.device = device\n",
    " \n",
    "    def forward(self, target, encoder_outputs, hidden, teacher_forcing_ratio=0.5):\n",
    "        batch_size = target.shape[1]\n",
    "        trg_len = target.shape[0]\n",
    "        trg_vocab_size = self.one_step_decoder.output_dim\n",
    " \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        input = target[0, :]\n",
    " \n",
    "        for t in range(1, trg_len):\n",
    "            # Pass the encoder_outputs. For the first time step the \n",
    "            # hidden state comes from the encoder model.\n",
    "            output, hidden, a = self.one_step_decoder(input, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    " \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    " \n",
    "            input = target[t] if teacher_force else top1\n",
    " \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c633c26-eb23-468e-aa7d-3e4867f73472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqgen.model import seq2seq_lstm as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "46fa1cb1-013a-4324-804e-d2cb3d78a75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.,  2.,  2.,  2.],\n",
       "          [ 4.,  4.,  4.,  4.],\n",
       "          [ 6.,  6.,  6.,  6.],\n",
       "          [ 8.,  8.,  8.,  8.],\n",
       "          [10., 10., 10., 10.]],\n",
       " \n",
       "         [[ 4.,  4.,  4.,  4.],\n",
       "          [ 4.,  4.,  4.,  4.],\n",
       "          [ 4.,  4.,  4.,  4.],\n",
       "          [ 4.,  4.,  4.,  4.],\n",
       "          [ 4.,  4.,  4.,  4.]]]),\n",
       " torch.Size([2, 5, 4]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations = 2.0*torch.tensor([[[1,1,1,1],[2,2,2,2],[3,3,3,3],[4,4,4,4],[5,5,5,5]],[[2,2,2,2],[2,2,2,2],[2,2,2,2],[2,2,2,2],[2,2,2,2]]])\n",
    "annotations, annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "71bfc82c-7735-4770-be4f-2346b84b9bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4.],\n",
       "         [2., 3., 4., 5.]]),\n",
       " torch.Size([2, 4]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn = 1.0*torch.tensor([[1,2,3,4],[2,3,4,5]])\n",
    "hn, hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a8464052-aebe-4c5e-88b4-7ff6b31d69ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 2., 3., 4.],\n",
       "          [1., 2., 3., 4.],\n",
       "          [1., 2., 3., 4.],\n",
       "          [1., 2., 3., 4.],\n",
       "          [1., 2., 3., 4.]],\n",
       " \n",
       "         [[2., 3., 4., 5.],\n",
       "          [2., 3., 4., 5.],\n",
       "          [2., 3., 4., 5.],\n",
       "          [2., 3., 4., 5.],\n",
       "          [2., 3., 4., 5.]]]),\n",
       " torch.Size([2, 5, 4]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn_rep = s.repeat_hidden_state(hn, 5)\n",
    "hn_rep, hn_rep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6e35a781-9a4a-46dd-b4e3-b17dd6c29d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1., 1., 1.],\n",
       "          [2., 2., 2., 2., 2.],\n",
       "          [3., 3., 3., 3., 3.],\n",
       "          [4., 4., 4., 4., 4.]],\n",
       " \n",
       "         [[2., 2., 2., 2., 2.],\n",
       "          [3., 3., 3., 3., 3.],\n",
       "          [4., 4., 4., 4., 4.],\n",
       "          [5., 5., 5., 5., 5.]]]),\n",
       " tensor([[[ 2.,  2.,  2.,  2.],\n",
       "          [ 4.,  4.,  4.,  4.],\n",
       "          [ 6.,  6.,  6.,  6.],\n",
       "          [ 8.,  8.,  8.,  8.],\n",
       "          [10., 10., 10., 10.]],\n",
       " \n",
       "         [[ 4.,  4.,  4.,  4.],\n",
       "          [ 4.,  4.,  4.,  4.],\n",
       "          [ 4.,  4.,  4.,  4.],\n",
       "          [ 4.,  4.,  4.,  4.],\n",
       "          [ 4.,  4.,  4.,  4.]]]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn_rep.permute(0, 2, 1), annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b1347410-20fe-464e-8f7b-c6e2975386ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.,  2.,  3.,  4.,  2.,  2.,  2.,  2.],\n",
       "          [ 1.,  2.,  3.,  4.,  4.,  4.,  4.,  4.],\n",
       "          [ 1.,  2.,  3.,  4.,  6.,  6.,  6.,  6.],\n",
       "          [ 1.,  2.,  3.,  4.,  8.,  8.,  8.,  8.],\n",
       "          [ 1.,  2.,  3.,  4., 10., 10., 10., 10.]],\n",
       " \n",
       "         [[ 2.,  3.,  4.,  5.,  4.,  4.,  4.,  4.],\n",
       "          [ 2.,  3.,  4.,  5.,  4.,  4.,  4.,  4.],\n",
       "          [ 2.,  3.,  4.,  5.,  4.,  4.,  4.,  4.],\n",
       "          [ 2.,  3.,  4.,  5.,  4.,  4.,  4.,  4.],\n",
       "          [ 2.,  3.,  4.,  5.,  4.,  4.,  4.,  4.]]]),\n",
       " torch.Size([2, 5, 8]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn_st = torch.cat([hn_rep, annotations], dim=2)\n",
    "hn_st, hn_st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36ab1b8f-cbdf-4dbe-82ed-20c6ac8f0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = nn.Linear(8, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29dd1bf2-7cfb-4fa7-bb07-5d5fbdc36fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.2170,  1.2324, -2.5259, -2.9899],\n",
       "         [ 3.4794,  1.8973, -3.4077, -3.6192],\n",
       "         [ 4.7417,  2.5621, -4.2896, -4.2486],\n",
       "         [ 6.0041,  3.2270, -5.1714, -4.8779],\n",
       "         [ 7.2665,  3.8918, -6.0532, -5.5072]],\n",
       "\n",
       "        [[ 3.4373,  2.4055, -3.7759, -4.3160],\n",
       "         [ 3.4373,  2.4055, -3.7759, -4.3160],\n",
       "         [ 3.4373,  2.4055, -3.7759, -4.3160],\n",
       "         [ 3.4373,  2.4055, -3.7759, -4.3160],\n",
       "         [ 3.4373,  2.4055, -3.7759, -4.3160]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin(hn_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "099b9ffd-240d-4595-841f-fcb3ebb9e4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 30.,  30.,  30.,  30.],\n",
       "         [ 60.,  60.,  60.,  60.],\n",
       "         [ 90.,  90.,  90.,  90.],\n",
       "         [120., 120., 120., 120.]],\n",
       "\n",
       "        [[ 40.,  40.,  40.,  40.],\n",
       "         [ 60.,  60.,  60.,  60.],\n",
       "         [ 80.,  80.,  80.,  80.],\n",
       "         [100., 100., 100., 100.]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(hn_rep.permute(0,2,1), annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2c505-a3a6-4ff7-b530-6243c05e0942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
