{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307e370c-40cb-4194-bf97-ad9e9ab745be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seqgen.seq_gen as g\n",
    "import random\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "197faa56-4d2f-4453-bbda-32c5b4115b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "print(\"Device\", device)\n",
    "\n",
    "# model hyperparameters\n",
    "lr = 1e-2\n",
    "num_layers=3\n",
    "embedding_dim = 128\n",
    "hidden_size=128\n",
    "bidirectional=True\n",
    "\n",
    "# Synthetic data generator hyperparameters\n",
    "batch_size=8\n",
    "max_length=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a78d88d5-47ef-4b8f-9d41-e1d8fb70c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target_seqs = g.generate_synthetic_training_data(batch_size, max_length=max_length, device=device, swap_times=0)\n",
    "input_seqs = torch.Tensor(features[:, :, 0]).to(torch.int64)\n",
    "coordinates = torch.Tensor(features[:, :, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "437f41f6-057b-41e5-9d52-1744666d4aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 20, 5]),\n",
       " torch.Size([8, 20]),\n",
       " torch.Size([8, 20, 4]),\n",
       " torch.Size([8, 20]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, input_seqs.shape, coordinates.shape, target_seqs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae84cc-67fe-4487-b119-51c1e6563d7c",
   "metadata": {},
   "source": [
    "# The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1c25b8f-93b5-4493-81e4-fb6c1f3a67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqgen.model import seq2seq_lstm\n",
    "from seqgen.vocabulary import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7974696-6918-47ff-93b7-14cc4517dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint = False\n",
    "checkpoint_file = \"model_2022-12-27_07-07-47.pt\"\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "encoder = seq2seq_lstm.EncoderRNN(vocab_size=len(vocab_in), embedding_dim=embedding_dim, num_layers=num_layers, max_length=max_length, hidden_size=hidden_size, bidirectional=bidirectional, pos_encoding=False, device=device).to(features.device)\n",
    "decoder = seq2seq_lstm.AttnDecoderRNN(embedding_dim=embedding_dim, num_layers=num_layers, hidden_size=hidden_size, vocab_size=len(vocab_out), max_length=max_length, bidirectional=bidirectional, pos_encoding=False, device=device).to(features.device)\n",
    "#positions = seq2seq_lstm.get_position_encoding(max_length, embedding_dim, device=device)\n",
    "positions = seq2seq_lstm.get_coordinate_encoding(coordinates, d=embedding_dim, device=device)\n",
    "\n",
    "# Load model weights from checkpoint\n",
    "if load_from_checkpoint:\n",
    "    num_layers = checkpoint['num_layers']\n",
    "    embedding_dim = checkpoint['embedding_dim']\n",
    "    hidden_size = checkpoint['hidden_size']\n",
    "    bidirectional = checkpoint['bidirectional']\n",
    "\n",
    "# Initialize optimizer for encoder and decoder\n",
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Load model weights from checkpoint\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    encoder.load_state_dict(checkpoint['encoder_model_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_model_state_dict'])\n",
    "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d63a2d8-bced-44ec-b32b-19d5f3055885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run word 1 of all 8 sequences through the encoder\n",
      "Run word 2 of all 8 sequences through the encoder\n",
      "Run word 3 of all 8 sequences through the encoder\n",
      "Run word 4 of all 8 sequences through the encoder\n",
      "Run word 5 of all 8 sequences through the encoder\n",
      "Run word 6 of all 8 sequences through the encoder\n",
      "Run word 7 of all 8 sequences through the encoder\n",
      "Run word 8 of all 8 sequences through the encoder\n",
      "Run word 9 of all 8 sequences through the encoder\n",
      "Run word 10 of all 8 sequences through the encoder\n",
      "Run word 11 of all 8 sequences through the encoder\n",
      "Run word 12 of all 8 sequences through the encoder\n",
      "Run word 13 of all 8 sequences through the encoder\n",
      "Run word 14 of all 8 sequences through the encoder\n",
      "Run word 15 of all 8 sequences through the encoder\n",
      "Run word 16 of all 8 sequences through the encoder\n",
      "Run word 17 of all 8 sequences through the encoder\n",
      "Run word 18 of all 8 sequences through the encoder\n",
      "Run word 19 of all 8 sequences through the encoder\n",
      "Run word 20 of all 8 sequences through the encoder\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encoder hidden state and cell state with zeros\n",
    "hn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "cn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "\n",
    "_hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "\n",
    "# Iterate over the sequence words and run every word through the encoder\n",
    "for i in range(input_seqs.shape[1]):\n",
    "    # Run the i-th word of the input sequence through the encoder.\n",
    "    # As a result we will get the prediction (output), the hidden state and the cell state.\n",
    "    # The hidden state and cell state will be used as inputs in the next round\n",
    "    print(f\"Run word {i+1} of all {input_seqs.shape[0]} sequences through the encoder\")\n",
    "    output, (hn, cn) = encoder(input_seqs[:, i].unsqueeze(dim=1), coordinates[:, i], positions[:, i:i+1], (hn, cn))\n",
    "    encoder_outputs[:, i:i+1, :] = output\n",
    "    encoder_hidden_states[:, i, :] = seq2seq_lstm.concat_hidden_states(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbb3c2a2-1b0a-404a-9d8c-0ae7fbc8485c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 256]),\n",
       " torch.Size([6, 8, 128]),\n",
       " torch.Size([6, 8, 128]),\n",
       " torch.Size([8, 20, 768]),\n",
       " torch.Size([8, 20, 256]),\n",
       " torch.Size([8, 20, 4]),\n",
       " torch.Size([8, 20, 260]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, hn.shape, cn.shape, encoder_hidden_states.shape, encoder_outputs.shape, coordinates.shape, torch.cat([encoder_outputs, coordinates], dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbb0b4ee-75d7-4141-8e00-89a84a2290bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 384])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate hidden state vectors from multiple hidden state layers\n",
    "hn_cat = seq2seq_lstm.concat_hidden_states(hn)\n",
    "hn_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9acfa7-5a92-4bea-95e1-5f5270c6a309",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a45b8095-ce78-4aad-9446-91121ef1e510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 8, 128]), torch.Size([8, 20, 384]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.shape, encoder_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b559e95-7a85-4153-b1ea-2d9016798f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 20, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_st = seq2seq_lstm.combine_encoder_annotations_and_hidden_state(hn, encoder_hidden_states)\n",
    "h_st.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29054ba-e5b3-405f-bc5b-149f8862da81",
   "metadata": {},
   "source": [
    "# The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda91121-2c2d-4531-b7ee-d67096698123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run word 1 through decoder\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8x512 and 256x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, target_seqs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun word \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m through decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     output, (hn, cn), attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_seqs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoordinates\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39msqueeze(), target_seqs[:, i])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOSS\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m max_length)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\PycharmProjects\\UdacityProjects\\YoloImagePreparation\\seqgen\\model\\seq2seq_lstm.py:296\u001b[0m, in \u001b[0;36mAttnDecoderRNN.forward\u001b[1;34m(self, x, coordinates, position, hidden, encoder_outputs)\u001b[0m\n\u001b[0;32m    294\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((embedded, attn_applied), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m _log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerge encoder outputs with attention appied\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 296\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_combine\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m _log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter attention combine\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    298\u001b[0m output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x512 and 256x128)"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "\n",
    "# This will produce a prediction for the next word in the sequence\n",
    "for i in range(0, target_seqs.size(1)):\n",
    "    print(f\"Run word {i+1} through decoder\")\n",
    "    output, (hn, cn), attn_weights = decoder(\n",
    "        x=target_seqs[:, i].unsqueeze(dim=1),\n",
    "        coordinates=coordinates[:, i],\n",
    "        position=positions[:, i:i+1],\n",
    "        hidden=(hn, cn),\n",
    "        encoder_outputs=encoder_hidden_states\n",
    "    )\n",
    "    loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "\n",
    "print(\"LOSS\", loss.item() / max_length)\n",
    "output.shape, hn.shape, cn.shape, attn_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb661cbf-c1ba-4bf3-883a-b2300d594dde",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f9ad9-7ae1-4b49-99b8-490eacfd5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "for epoch in range(200000):\n",
    "    # With a certain chance present the model the true predictions\n",
    "    # instead of its own predictions in the next iteration\n",
    "    use_teacher_forcing_prob = 0.5\n",
    "    use_teacher_forcing = random.random() < use_teacher_forcing_prob\n",
    "    \n",
    "    # Get a batch of trianing data\n",
    "    features, target_seqs = g.generate_synthetic_training_data(batch_size, max_length=max_length, continue_prob=0.99, device=device, swap_times=0)\n",
    "    features = features.to(device)\n",
    "    target_seqs = target_seqs.to(device)\n",
    "    input_seqs = torch.Tensor(features[:, :, 0]).to(torch.int64)\n",
    "    coordinates = torch.Tensor(features[:, :, 1:])\n",
    "\n",
    "    # Initialize the encoder hidden state and cell state with zeros\n",
    "    hn_enc = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "    cn_enc = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "    \n",
    "    # Initialize encoder outputs tensor\n",
    "    _hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "    encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "    encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "    \n",
    "    # Set gradients of all model parameters to zero\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "    \n",
    "    ####################\n",
    "    #     ENCODING     #\n",
    "    ####################\n",
    "\n",
    "    # Iterate over the sequence words and run every word through the encoder\n",
    "    for i in range(input_seqs.shape[1]):\n",
    "        # Run the i-th word of the input sequence through the encoder.\n",
    "        # As a result we will get the prediction (output), the hidden state (hn) and the cell state (cn).\n",
    "        # The hidden state and cell state will be used as inputs in the next round\n",
    "        output, (hn_enc, cn_enc) = encoder(\n",
    "            input_seqs[:, i].unsqueeze(dim=1),\n",
    "            coordinates[:, i],\n",
    "            positions[:, i:i+1],\n",
    "            (hn_enc, cn_enc)\n",
    "        )\n",
    "        # Save encoder outputs and states for current word\n",
    "        encoder_outputs[:, i:i+1, :] = output\n",
    "        encoder_hidden_states[:, i:i+1, :] = hn_enc[-1].unsqueeze(dim=1)\n",
    "        \n",
    "    ####################\n",
    "    #     DECODING     #\n",
    "    ####################\n",
    "\n",
    "    # The first words that we be presented to the model is the '<start>' token\n",
    "    prediction = target_seqs[:, 0]\n",
    "    \n",
    "    # The initial hidden state of the decoder is the final hidden state of the decoder\n",
    "    hn_dec, cn_dec = hn_enc, cn_enc\n",
    "    \n",
    "    # Iterate over words of target sequence and run words through the decoder.\n",
    "    # This will produce a prediction for the next word in the sequence\n",
    "    for i in range(1, target_seqs.size(1)):\n",
    "        # Run word i through decoder and get word i+1 and the new hidden state as outputs\n",
    "        if use_teacher_forcing:\n",
    "            output, (hn_dec, cn_dec), attn_weights = decoder(\n",
    "                target_seqs[:, i-1].unsqueeze(dim=1),\n",
    "                coordinates[:, i-1],\n",
    "                positions[:, i-1:i],\n",
    "                (hn_dec, cn_dec),\n",
    "                encoder_outputs=encoder_hidden_states\n",
    "            )\n",
    "        else:\n",
    "            output, (hn_dec, cn_dec), attn_weights = decoder(\n",
    "                prediction.unsqueeze(dim=1),\n",
    "                coordinates[:, i-1],\n",
    "                positions[:, i-1:i],\n",
    "                (hn_dec, cn_dec),\n",
    "                encoder_outputs=encoder_hidden_states\n",
    "            )\n",
    "\n",
    "            # Get the predicted classes of the model\n",
    "            topv, topi = output.topk(1)\n",
    "            prediction = topi.squeeze()    \n",
    "        loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    if not epoch % 100:\n",
    "        print(f\"LOSS after epoch {epoch}\", loss.item() / target_seqs.size(1))\n",
    "\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights of encoder and decoder\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e3605-8f59-42ee-b260-5bb500518d00",
   "metadata": {},
   "source": [
    "#### Save model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67bec2-ccfd-4492-bd1a-e51fad94fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "model_data = {\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length\n",
    "}\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'encoder_model_state_dict': encoder.state_dict(),\n",
    "    'decoder_model_state_dict': decoder.state_dict(),\n",
    "    'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "    'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"bidirectional\": bidirectional,\n",
    "}, \"model_attn_\" + date_time + \".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56c813-13d3-45a2-b9e1-47fed03bcf3c",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "We run our input sequences through the model and get output seuences. Then we decode the output sequences with the Vocabulary class and get our final latex code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb8ff7-8d9a-4c28-8f95-264703cfe636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_seqs, coordinates, target_seqs):\n",
    "    vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "    vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "    predictions = torch.zeros(target_seqs.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize the encoder hidden state and cell state with zeros\n",
    "        hn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "        cn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "\n",
    "        # Iterate over the sequence words and run every word through the encoder\n",
    "        for i in range(input_seqs.shape[1]):\n",
    "            output, (hn, cn) = encoder(\n",
    "                input_seqs[:, i].unsqueeze(dim=1),\n",
    "                coordinates[:, i],\n",
    "                (hn, cn)\n",
    "            )\n",
    "\n",
    "        # Predict tokens of the target sequence by running the hidden state through\n",
    "        # the decoder\n",
    "        for i in range(0, target_seqs.size(1)):\n",
    "            output, (hn, cn) = decoder(\n",
    "                target_seqs[:, i].unsqueeze(dim=1),\n",
    "                coordinates[:, i],\n",
    "                (hn, cn)\n",
    "            )\n",
    "            # Select the indices of the most likely tokens\n",
    "            predicted_char = torch.argmax(output, dim=2)\n",
    "            predictions[:, i] = torch.argmax(output, dim=2).squeeze()\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4224ca-ab02-437b-a0ea-5704b1c01daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict(input_seqs[0:1], coordinates[0:1], target_seqs[0:1])\n",
    "input_seqs[0:1], prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476c3b2-d8ad-4506-8ace-814d9e9eefcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_swapped = g.random_swap(input_seqs[0], i=2).unsqueeze(dim=0)\n",
    "coords_swapped = g.random_swap(coordinates[0], i=2).unsqueeze(dim=0)\n",
    "prediction_swapped = predict(in_swapped, coords_swapped, target_seqs[0:1])\n",
    "in_swapped, prediction_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0408b19-3288-48ac-905b-88eb6ef4017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs[0:1] == in_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb4a03-69bd-45e1-960f-244da1c18417",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction == prediction_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf18d6-94da-42ce-8bad-86a9f2d16d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random sequence and its prediction from the model\n",
    "import random\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "predictions = predict(input_seqs, coordinates, target_seqs)\n",
    "\n",
    "i = random.randint(0, predictions.size(0))\n",
    "print(\"MODEL INPUT\", vocab_in.decode_sequence(input_seqs[i].cpu().numpy()))\n",
    "print(\"MODEL OUTPUT\", vocab_out.decode_sequence(predictions[i].cpu().numpy()))\n",
    "print(\"TARGET OUTPUT\", vocab_out.decode_sequence(target_seqs[i][1:].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeae19b-77d4-4715-8068-9822af23009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = vocab_out.decode_sequence(predictions[i].cpu().numpy())\n",
    "prediction = list(filter(lambda x: x != '<end>', prediction))\n",
    "prediction = \"\".join(prediction)\n",
    "print(\"MODEL OUTPUT\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b7aff-cecf-476a-bd3c-ae737ff8aa66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
