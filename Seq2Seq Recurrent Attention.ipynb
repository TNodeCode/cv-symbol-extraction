{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307e370c-40cb-4194-bf97-ad9e9ab745be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seqgen.seq_gen as g\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from seqgen.model import rnn, embedding, attention\n",
    "from seqgen.vocabulary import *\n",
    "from seqgen.preprocess import *\n",
    "from seqgen.datasets.sequences import *\n",
    "from seqgen.datasets.realdata import RealSequencesDataset\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "197faa56-4d2f-4453-bbda-32c5b4115b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccbe91c-6286-4398-ad71-ee251ae1af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "\n",
    "cell_type=rnn.CellType.LSTM\n",
    "encoder_embedding_type=embedding.EmbeddingType.COORDS_DIRECT\n",
    "decoder_embedding_type=embedding.EmbeddingType.POS_SUBSPACE\n",
    "attention_type=attention.AttentionType.DOT\n",
    "\n",
    "use_real_dataset=True\n",
    "num_layers=3\n",
    "embedding_dim=64\n",
    "hidden_size=64\n",
    "batch_size=128\n",
    "max_length=50\n",
    "bidirectional=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78d88d5-47ef-4b8f-9d41-e1d8fb70c635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 50]),\n",
       " torch.Size([128, 50, 4]),\n",
       " torch.Size([128, 50]),\n",
       " torch.Size([128, 50]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "if use_real_dataset:\n",
    "    dataset = RealSequencesDataset(filename=\"data/train/label.txt\", vocab_in=vocab_in, vocab_out=vocab_out, max_length=max_length-2, batch_size=batch_size, device=device)\n",
    "else:\n",
    "    dataset = SyntheticSequenceDataset(vocab_in, vocab_out, max_length, batch_size, continue_prob=0.95, additional_eos=True, device=device)\n",
    "    \n",
    "positions = torch.arange(max_length).repeat(batch_size, 1).to(device)\n",
    "\n",
    "input_seqs, coordinates, target_seqs = dataset[0]\n",
    "input_seqs.shape, coordinates.shape, target_seqs.shape, positions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae84cc-67fe-4487-b119-51c1e6563d7c",
   "metadata": {},
   "source": [
    "# The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7974696-6918-47ff-93b7-14cc4517dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint = False\n",
    "checkpoint_file = \"model_2023-01-15_09-17-53.pt\"\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "encoder = rnn.RecurrentEncoder(\n",
    "    cell_type=cell_type,\n",
    "    embedding_type=encoder_embedding_type,\n",
    "    vocab_size=len(vocab_in),\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    max_length=max_length,\n",
    "    num_layers=num_layers,\n",
    "    dropout=0.1,\n",
    "    bidirectional=bidirectional,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "decoder = rnn.RecurrentAttentionDecoder(\n",
    "    cell_type=cell_type,\n",
    "    embedding_type=decoder_embedding_type,\n",
    "    attention_type=attention_type,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=len(vocab_out),\n",
    "    max_length=max_length,\n",
    "    batch_size=batch_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=0.1,\n",
    "    bidirectional=bidirectional,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Initialize optimizer for encoder and decoder\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Load model weights from checkpoint\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    cell_type=checkpoint['cell_type']\n",
    "    attention_type=checkpoint['attention_type']\n",
    "    encoder_embedding_type=checkpoint['encoder_embedding_type']\n",
    "    decoder_embedding_typecheckpoint['decoder_embedding_type']\n",
    "    encoder.load_state_dict(checkpoint['encoder_model_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_model_state_dict'])\n",
    "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "    num_layers = checkpoint['num_layers']\n",
    "    embedding_dim = checkpoint['embedding_dim']\n",
    "    hidden_size = checkpoint['hidden_size']\n",
    "    bidirectional = checkpoint['bidirectional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d63a2d8-bced-44ec-b32b-19d5f3055885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run word 1 of all 128 sequences through the encoder\n",
      "Run word 2 of all 128 sequences through the encoder\n",
      "Run word 3 of all 128 sequences through the encoder\n",
      "Run word 4 of all 128 sequences through the encoder\n",
      "Run word 5 of all 128 sequences through the encoder\n",
      "Run word 6 of all 128 sequences through the encoder\n",
      "Run word 7 of all 128 sequences through the encoder\n",
      "Run word 8 of all 128 sequences through the encoder\n",
      "Run word 9 of all 128 sequences through the encoder\n",
      "Run word 10 of all 128 sequences through the encoder\n",
      "Run word 11 of all 128 sequences through the encoder\n",
      "Run word 12 of all 128 sequences through the encoder\n",
      "Run word 13 of all 128 sequences through the encoder\n",
      "Run word 14 of all 128 sequences through the encoder\n",
      "Run word 15 of all 128 sequences through the encoder\n",
      "Run word 16 of all 128 sequences through the encoder\n",
      "Run word 17 of all 128 sequences through the encoder\n",
      "Run word 18 of all 128 sequences through the encoder\n",
      "Run word 19 of all 128 sequences through the encoder\n",
      "Run word 20 of all 128 sequences through the encoder\n",
      "Run word 21 of all 128 sequences through the encoder\n",
      "Run word 22 of all 128 sequences through the encoder\n",
      "Run word 23 of all 128 sequences through the encoder\n",
      "Run word 24 of all 128 sequences through the encoder\n",
      "Run word 25 of all 128 sequences through the encoder\n",
      "Run word 26 of all 128 sequences through the encoder\n",
      "Run word 27 of all 128 sequences through the encoder\n",
      "Run word 28 of all 128 sequences through the encoder\n",
      "Run word 29 of all 128 sequences through the encoder\n",
      "Run word 30 of all 128 sequences through the encoder\n",
      "Run word 31 of all 128 sequences through the encoder\n",
      "Run word 32 of all 128 sequences through the encoder\n",
      "Run word 33 of all 128 sequences through the encoder\n",
      "Run word 34 of all 128 sequences through the encoder\n",
      "Run word 35 of all 128 sequences through the encoder\n",
      "Run word 36 of all 128 sequences through the encoder\n",
      "Run word 37 of all 128 sequences through the encoder\n",
      "Run word 38 of all 128 sequences through the encoder\n",
      "Run word 39 of all 128 sequences through the encoder\n",
      "Run word 40 of all 128 sequences through the encoder\n",
      "Run word 41 of all 128 sequences through the encoder\n",
      "Run word 42 of all 128 sequences through the encoder\n",
      "Run word 43 of all 128 sequences through the encoder\n",
      "Run word 44 of all 128 sequences through the encoder\n",
      "Run word 45 of all 128 sequences through the encoder\n",
      "Run word 46 of all 128 sequences through the encoder\n",
      "Run word 47 of all 128 sequences through the encoder\n",
      "Run word 48 of all 128 sequences through the encoder\n",
      "Run word 49 of all 128 sequences through the encoder\n",
      "Run word 50 of all 128 sequences through the encoder\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encoder hidden state and cell state with zeros\n",
    "hn = encoder.initHidden(input_seqs.shape[0], device=dataset.device)\n",
    "cn = encoder.initHidden(input_seqs.shape[0], device=dataset.device)\n",
    "hidden = (hn, cn) if cell_type == rnn.CellType.LSTM else hn\n",
    "\n",
    "_hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "\n",
    "# Iterate over the sequence words and run every word through the encoder\n",
    "for i in range(input_seqs.shape[1]):\n",
    "    # Run the i-th word of the input sequence through the encoder.\n",
    "    # As a result we will get the prediction (output), the hidden state and the cell state.\n",
    "    # The hidden state and cell state will be used as inputs in the next round\n",
    "    print(f\"Run word {i+1} of all {input_seqs.shape[0]} sequences through the encoder\")\n",
    "    output, hidden = encoder(\n",
    "        input_seqs[:, i].unsqueeze(dim=1),\n",
    "        coordinates[:, i],\n",
    "        hidden\n",
    "    )\n",
    "    encoder_outputs[:, i:i+1, :] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbb3c2a2-1b0a-404a-9d8c-0ae7fbc8485c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 128]),\n",
       " torch.Size([6, 128, 64]),\n",
       " torch.Size([6, 128, 64]),\n",
       " torch.Size([128, 50, 128]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, hn.shape, cn.shape, encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29054ba-e5b3-405f-bc5b-149f8862da81",
   "metadata": {},
   "source": [
    "# The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eda91121-2c2d-4531-b7ee-d67096698123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run word 1 through decoder torch.Size([128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 2 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 3 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 4 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 5 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 6 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 7 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 8 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 9 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 10 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 11 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 12 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 13 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 14 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 15 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 16 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 17 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 18 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 19 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 20 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 21 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 22 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 23 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 24 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 25 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 26 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 27 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 28 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 29 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 30 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 31 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 32 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 33 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 34 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 35 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 36 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 37 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 38 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 39 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 40 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 41 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 42 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 43 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 44 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 45 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 46 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 47 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 48 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 49 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "Run word 50 through decoder torch.Size([6, 128, 64]) torch.Size([128, 50, 384])\n",
      "LOSS 5.198253173828125\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "\n",
    "# Iterate over words of target sequence and run words through the decoder.\n",
    "# This will produce a prediction for the next word in the sequence\n",
    "for i in range(0, target_seqs.size(1)):\n",
    "    print(f\"Run word {i+1} through decoder\", hn[0].shape if cell_type == rnn.CellType.LSTM else hn.shape, encoder_hidden_states.shape)\n",
    "    output, hn, _ = decoder(\n",
    "        x=target_seqs[:, i].unsqueeze(dim=1),\n",
    "        positions=positions[:, i:i+1],\n",
    "        annotations=encoder_outputs,\n",
    "        hidden=hidden\n",
    "    )\n",
    "    loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "\n",
    "print(\"LOSS\", loss.item() / max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c59e1-3885-4a12-8696-9fcf3da9cf9e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a5f9ad9-7ae1-4b49-99b8-490eacfd5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS after epoch 0 5.054454040527344 ACCURACY 0.0\n",
      "LOSS after epoch 100 1.6105517578125 ACCURACY 0.6921938659199804\n",
      "LOSS after epoch 200 1.3887765502929688 ACCURACY 0.7354830872791354\n",
      "LOSS after epoch 300 1.3115267944335938 ACCURACY 0.7491103193652816\n",
      "LOSS after epoch 400 0.9655345916748047 ACCURACY 0.7547098092571832\n",
      "LOSS after epoch 500 1.2592845153808594 ACCURACY 0.7733721177605912\n",
      "LOSS after epoch 600 0.9983010864257813 ACCURACY 0.7725334690650925\n",
      "LOSS after epoch 700 0.8760892486572266 ACCURACY 0.8149585323780775\n",
      "LOSS after epoch 800 0.6421528625488281 ACCURACY 0.8523341688793152\n",
      "LOSS after epoch 900 0.21059833526611327 ACCURACY 0.8777471140213311\n",
      "LOSS after epoch 1000 0.17809185028076172 ACCURACY 0.8989269617851824\n",
      "LOSS after epoch 1100 0.44169475555419924 ACCURACY 0.9109406745340675\n",
      "LOSS after epoch 1200 0.1563927173614502 ACCURACY 0.9226897191163153\n",
      "LOSS after epoch 1300 0.43213146209716796 ACCURACY 0.9333466071821749\n",
      "LOSS after epoch 1400 0.29591222763061525 ACCURACY 0.9351992854382842\n",
      "LOSS after epoch 1500 0.35836814880371093 ACCURACY 0.9447959058173001\n",
      "LOSS after epoch 1600 0.20896965026855469 ACCURACY 0.9416262626182288\n",
      "LOSS after epoch 1700 0.10136825561523438 ACCURACY 0.9442841065954417\n",
      "LOSS after epoch 1800 0.08513784408569336 ACCURACY 0.9508147190324963\n",
      "LOSS after epoch 1900 0.09631303787231445 ACCURACY 0.946922817742452\n",
      "LOSS after epoch 2000 0.3465001678466797 ACCURACY 0.9473947567120194\n",
      "LOSS after epoch 2100 0.34659103393554686 ACCURACY 0.9489732006844133\n",
      "LOSS after epoch 2200 0.08285879135131836 ACCURACY 0.9587069379910826\n",
      "LOSS after epoch 2300 0.08984869956970215 ACCURACY 0.9546779196243733\n",
      "LOSS after epoch 2400 0.22665794372558593 ACCURACY 0.9550605721212924\n",
      "LOSS after epoch 2500 0.0778393268585205 ACCURACY 0.9544339779950678\n",
      "LOSS after epoch 2600 0.062896409034729 ACCURACY 0.9618048327788711\n",
      "LOSS after epoch 2700 0.05340385913848877 ACCURACY 0.9619881875254214\n",
      "LOSS after epoch 2800 0.09839999198913574 ACCURACY 0.9584869119897484\n",
      "LOSS after epoch 2900 0.05528994083404541 ACCURACY 0.9606345521565527\n",
      "LOSS after epoch 3000 0.23868133544921874 ACCURACY 0.9648517083562911\n",
      "LOSS after epoch 3100 0.06693994522094726 ACCURACY 0.9579384420718998\n",
      "LOSS after epoch 3200 0.17600366592407227 ACCURACY 0.9580309165082872\n",
      "LOSS after epoch 3300 0.05943315982818603 ACCURACY 0.960787613708526\n",
      "LOSS after epoch 3400 0.24744644165039062 ACCURACY 0.9599792589619756\n",
      "LOSS after epoch 3500 0.05214035034179688 ACCURACY 0.9593112109042704\n",
      "LOSS after epoch 3600 0.05809428691864014 ACCURACY 0.9665768357738852\n",
      "LOSS after epoch 3700 0.05363776683807373 ACCURACY 0.957056746184826\n",
      "LOSS after epoch 3800 0.06750692367553711 ACCURACY 0.9637212874181569\n",
      "LOSS after epoch 3900 0.05697556018829346 ACCURACY 0.966672498099506\n",
      "LOSS after epoch 4000 0.24744152069091796 ACCURACY 0.9531441168673337\n",
      "LOSS after epoch 4100 0.06734447479248047 ACCURACY 0.9675286845490336\n",
      "LOSS after epoch 4200 0.22258853912353516 ACCURACY 0.9647831490449608\n",
      "LOSS after epoch 4300 0.21286582946777344 ACCURACY 0.9663647819310427\n",
      "LOSS after epoch 4400 0.05471220016479492 ACCURACY 0.9673309813253581\n",
      "LOSS after epoch 4500 0.06122349739074707 ACCURACY 0.9669913770258427\n",
      "LOSS after epoch 4600 0.2848689651489258 ACCURACY 0.9623708406090736\n",
      "LOSS after epoch 4700 0.06349274158477783 ACCURACY 0.9680181621387601\n",
      "LOSS after epoch 4800 0.33839313507080077 ACCURACY 0.9642011987604201\n",
      "LOSS after epoch 4900 0.19945707321166992 ACCURACY 0.9655038131214678\n",
      "LOSS after epoch 5000 0.07520747661590577 ACCURACY 0.9686367857828736\n",
      "LOSS after epoch 5100 0.0474302339553833 ACCURACY 0.9672209679521621\n",
      "LOSS after epoch 5200 0.18397916793823244 ACCURACY 0.9651099985651672\n",
      "LOSS after epoch 5300 0.06292261600494385 ACCURACY 0.9734135713055729\n",
      "LOSS after epoch 5400 0.22524190902709962 ACCURACY 0.969205981567502\n",
      "LOSS after epoch 5500 0.05199508190155029 ACCURACY 0.9692107652872801\n",
      "LOSS after epoch 5600 0.04902949810028076 ACCURACY 0.9651514534838498\n",
      "LOSS after epoch 5700 0.24178911209106446 ACCURACY 0.9649792600795627\n",
      "LOSS after epoch 5800 0.044756784439086914 ACCURACY 0.9710092343948782\n",
      "LOSS after epoch 5900 0.1684989356994629 ACCURACY 0.9696237114444375\n",
      "LOSS after epoch 6000 0.03829495906829834 ACCURACY 0.9701801537536084\n",
      "LOSS after epoch 6100 0.0390008807182312 ACCURACY 0.9680691834166646\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m        Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Python310\\lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Das System kann den angegebenen Pfad nicht finden: 'C:\\\\Users\\\\tilof\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_14380\\\\149049740.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m encoder(\n\u001b[0;32m     41\u001b[0m         x\u001b[38;5;241m=\u001b[39minput_seqs[:, i]\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     42\u001b[0m         coordinates\u001b[38;5;241m=\u001b[39mcoordinates[:, i],\n\u001b[0;32m     43\u001b[0m         hidden\u001b[38;5;241m=\u001b[39mhidden\n\u001b[0;32m     44\u001b[0m     )\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Save encoder outputs and states for current word\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     encoder_outputs[:, i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m     47\u001b[0m     encoder_hidden_states[:, i, :] \u001b[38;5;241m=\u001b[39m rnn\u001b[38;5;241m.\u001b[39mconcat_hidden_states(hn)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m####################\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m#     DECODING     #\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m####################\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\traceback.py:213\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    212\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m format_list(\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\traceback.py:227\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    226\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 227\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32mC:\\Python310\\lib\\traceback.py:379\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    376\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    377\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 379\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\IPython\\core\\compilerop.py:193\u001b[0m, in \u001b[0;36mcheck_linecache_ipython\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"Call linecache.checkcache() safely protecting our cached values.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# First call the original checkcache as intended\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkcache_ori\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Then, update back the cache with our data, so that tracebacks related\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# to our compiled codes can be produced.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m linecache\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mupdate(linecache\u001b[38;5;241m.\u001b[39m_ipython_cache)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(10000):\n",
    "    # With a certain chance present the model the true predictions\n",
    "    # instead of its own predictions in the next iteration\n",
    "    use_teacher_forcing_prob = 0.5\n",
    "    use_teacher_forcing = random.random() < use_teacher_forcing_prob\n",
    "    \n",
    "    # Get a batch of trianing data\n",
    "    input_seqs, coordinates, target_seqs = dataset[0]\n",
    "\n",
    "    # Initialize the encoder hidden state and cell state with zeros\n",
    "    hn = encoder.initHidden(input_seqs.shape[0], device=dataset.device)\n",
    "    cn = encoder.initHidden(input_seqs.shape[0], device=dataset.device)\n",
    "    hidden = (hn, cn) if cell_type == rnn.CellType.LSTM else hn\n",
    "    \n",
    "    # Initialize encoder outputs tensor\n",
    "    last_n_states = 2 if bidirectional else 1\n",
    "    _hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "    encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "    encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "    \n",
    "    # Set gradients of all model parameters to zero\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "    \n",
    "    ####################\n",
    "    #     ENCODING     #\n",
    "    ####################\n",
    "\n",
    "    # Iterate over the sequence words and run every word through the encoder\n",
    "    for i in range(input_seqs.shape[1]):\n",
    "        # Run the i-th word of the input sequence through the encoder.\n",
    "        # As a result we will get the prediction (output), the hidden state (hn).\n",
    "        # The hidden state and cell state will be used as inputs in the next round\n",
    "        output, hidden = encoder(\n",
    "            x=input_seqs[:, i].unsqueeze(dim=1),\n",
    "            coordinates=coordinates[:, i],\n",
    "            hidden=hidden\n",
    "        )\n",
    "        # Save encoder outputs and states for current word\n",
    "        encoder_outputs[:, i:i+1, :] = output\n",
    "        encoder_hidden_states[:, i, :] = rnn.concat_hidden_states(hn)\n",
    "\n",
    "    ####################\n",
    "    #     DECODING     #\n",
    "    ####################\n",
    "    \n",
    "    accuracy = 0.0\n",
    "\n",
    "    # The first words that we be presented to the model is the '<start>' token\n",
    "    prediction = target_seqs[:, 0]\n",
    "    \n",
    "    # Iterate over words of target sequence and run words through the decoder.\n",
    "    # This will produce a prediction for the next word in the sequence\n",
    "    for i in range(1, target_seqs.size(1)):\n",
    "        # Run word i through decoder and get word i+1 and the new hidden state as outputs\n",
    "        if use_teacher_forcing:\n",
    "            output, hidden, _ = decoder(\n",
    "                x=target_seqs[:, i-1].unsqueeze(dim=1),\n",
    "                positions=positions[:, i-1:i],\n",
    "                annotations=encoder_outputs,\n",
    "                hidden=hidden\n",
    "            )\n",
    "            # Get the predicted classes of the model\n",
    "            topv, topi = output.topk(1)\n",
    "        else:\n",
    "            output, hidden, _ = decoder(\n",
    "                x=prediction.unsqueeze(dim=1),\n",
    "                positions=positions[:, i-1:i],\n",
    "                annotations=encoder_outputs,\n",
    "                hidden=hidden\n",
    "            )\n",
    "            # Get the predicted classes of the model\n",
    "            topv, topi = output.topk(1)\n",
    "            prediction = topi.squeeze()    \n",
    "        loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "        accuracy += float((topi.squeeze() == target_seqs[:, i]).sum() / (target_seqs.size(0)*(target_seqs.size(1)-1)))\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print_every = 100\n",
    "    if not epoch % print_every:\n",
    "        _accuracy = sum(accuracies[-print_every:]) / print_every\n",
    "        print(f\"LOSS after epoch {epoch}\", loss.item() / (target_seqs.size(1)), \"ACCURACY\", _accuracy)\n",
    "\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    accuracy = 0.0\n",
    "\n",
    "    # Update weights of encoder and decoder\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e3605-8f59-42ee-b260-5bb500518d00",
   "metadata": {},
   "source": [
    "#### Save model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67bec2-ccfd-4492-bd1a-e51fad94fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "model_data = {\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"cell_type\": cell_type,\n",
    "    \"encoder_embedding_type\": encoder_embedding_type,\n",
    "    \"decoder_embedding_type\": decoder_embedding_type,\n",
    "    \"attention_type\": attention_type,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length\n",
    "}\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f\"{cell_type}_{num_layers}layers_encemb-{encoder_embedding_type}_decemb-{decoder_embedding_type}_attn-{attention_type}\"\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'encoder_model_state_dict': encoder.state_dict(),\n",
    "    'decoder_model_state_dict': decoder.state_dict(),\n",
    "    'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "    'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"cell_type\": cell_type,\n",
    "    \"encoder_embedding_type\": encoder_embedding_type,\n",
    "    \"decoder_embedding_type\": decoder_embedding_type,\n",
    "    \"attention_type\": attention_type,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"bidirectional\": bidirectional,\n",
    "}, filename + \".pt\")\n",
    "\n",
    "\n",
    "with open(filename + '.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "    \n",
    "print(str(date_time), \"Saved model: \" + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56c813-13d3-45a2-b9e1-47fed03bcf3c",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "We run our input sequences through the model and get output seuences. Then we decode the output sequences with the Vocabulary class and get our final latex code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb8ff7-8d9a-4c28-8f95-264703cfe636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_seqs, coordinates, target_seqs):\n",
    "    vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "    vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "    predictions = torch.zeros(target_seqs.shape)\n",
    "    attention_matrix = torch.zeros((input_seqs.shape[0], input_seqs.shape[1], input_seqs.shape[1]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize the encoder hidden state and cell state with zeros\n",
    "        hn = encoder.initHidden(input_seqs.shape[0], device=dataset.device)\n",
    "        \n",
    "        # Initialize the encoder hidden state and cell state with \n",
    "        last_n_states = 2 if bidirectional else 1\n",
    "        _hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "        encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "\n",
    "        # Iterate over the sequence words and run every word through the encoder\n",
    "        for i in range(input_seqs.size(1)):\n",
    "            output, hn = encoder(\n",
    "                input_seqs[:, i].unsqueeze(dim=1),\n",
    "                coordinates[:, i],\n",
    "                hn\n",
    "            )\n",
    "            encoder_outputs[:, i:i+1, :] = output\n",
    "            encoder_hidden_states[:, i, :] = rnn.concat_hidden_states(hn)\n",
    "        \n",
    "        # Predict tokens of the target sequence by running the hidden state through\n",
    "        # the decoder\n",
    "        for i in range(0, target_seqs.size(1)):\n",
    "            output, hn, attention = decoder(\n",
    "                x=target_seqs[:, i-1].unsqueeze(dim=1),\n",
    "                coordinates=coordinates[:, i-1],\n",
    "                annotations=encoder_outputs,\n",
    "                hidden=hn\n",
    "            )\n",
    "            # Select the indices of the most likely tokens\n",
    "            predicted_char = torch.argmax(output, dim=1)\n",
    "            predictions[:, i] = torch.argmax(output, dim=1).squeeze()\n",
    "            attention_matrix[:, :, i:i+1] = attention\n",
    "        \n",
    "        return predictions, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4224ca-ab02-437b-a0ea-5704b1c01daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, attention_matrix = predict(input_seqs, coordinates, target_seqs)\n",
    "prediction.shape, attention_matrix.shape, input_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6896f-5818-4718-a584-0aa59e9226de",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, prediction.size(0)-1)\n",
    "seq_in = vocab_in.decode_sequence(input_seqs[idx].cpu().numpy())\n",
    "seq_out = vocab_out.decode_sequence(predictions[idx].cpu().numpy())\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.matshow(attention_matrix[idx], cmap='bone')\n",
    "ax.set_xticklabels([seq_out[j] for j in range(prediction.size(1))], rotation=45)\n",
    "ax.set_yticklabels([seq_in[j] for j in range(prediction.size(1))])\n",
    "#ax.tick_params(labelsize=15)\n",
    "ax.set(xlabel='Output Sequence', ylabel='Input Sequence')\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(prediction.size(1)))\n",
    "ax.yaxis.set_major_locator(plt.MaxNLocator(prediction.size(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476c3b2-d8ad-4506-8ace-814d9e9eefcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_swapped = g.random_swap(input_seqs[0], i=2).unsqueeze(dim=0)\n",
    "coords_swapped = g.random_swap(coordinates[0], i=2).unsqueeze(dim=0)\n",
    "prediction_swapped = predict(in_swapped, coords_swapped, target_seqs[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0408b19-3288-48ac-905b-88eb6ef4017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs[0:1] == in_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb4a03-69bd-45e1-960f-244da1c18417",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction == prediction_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf18d6-94da-42ce-8bad-86a9f2d16d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random sequence and its prediction from the model\n",
    "import random\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "predictions, attention_matrix = predict(input_seqs, coordinates, target_seqs)\n",
    "\n",
    "i = random.randint(0, predictions.size(0))\n",
    "print(\"MODEL INPUT\", vocab_in.decode_sequence(input_seqs[i].cpu().numpy()))\n",
    "print(\"MODEL OUTPUT\", vocab_out.decode_sequence(predictions[i].cpu().numpy()))\n",
    "print(\"TARGET OUTPUT\", vocab_out.decode_sequence(target_seqs[i][1:].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeae19b-77d4-4715-8068-9822af23009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = vocab_out.decode_sequence(predictions[i].cpu().numpy())\n",
    "prediction = list(filter(lambda x: x != '<end>', prediction))\n",
    "prediction = \"\".join(prediction)\n",
    "print(\"MODEL OUTPUT\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05457f9c-56b4-4aaa-ae20-c4e8b120ad62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
