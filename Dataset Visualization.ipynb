{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1926c2db-80e0-431a-a125-cbd31b3af4e8",
   "metadata": {},
   "source": [
    "# Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261b1b7-3709-447c-83ac-103505c0affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "from seqgen.vocabulary import *\n",
    "from seqgen.datasets.sequences import *\n",
    "from seqgen.preprocess import *\n",
    "from seqgen.dataset import *\n",
    "from seqgen.visualize import *\n",
    "from seqgen.seq_gen import add_noise_to_coordinates\n",
    "from seqgen.symbol_replacement import replace_symbols\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79e0f3-6501-47a7-afe0-e4ab6672052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "max_length=25\n",
    "img_width=1120\n",
    "img_height=224\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "basedir = \"dataset-generation/archive/latex_images\"\n",
    "dataset = SyntheticSequenceDataset(vocab_in, vocab_out, max_length, batch_size, continue_prob=0.9999, device=\"cpu\")\n",
    "class_samples = get_class_samples(basedir)\n",
    "\n",
    "input_seqs, coordinates, target_seqs = dataset[0]\n",
    "coordinates[:, :, [0,2]] *= img_width\n",
    "coordinates[:, :, [1,3]] *= img_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73db257-e6cc-4667-a92a-e13b4eb397ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(batch_size*2, figsize=(50, 80))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    ax[2*i].imshow(create_img_array_from_coordinates(coordinates[i], img_height, img_width, max_length))\n",
    "    ax[2*i+1].imshow(create_cv2img_array_from_coordinates(input_seqs[i], coordinates[i], img_height, img_width, class_samples, vocab_in, max_length, basedir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a734bb-1080-4b00-aae3-4e46360d9121",
   "metadata": {},
   "source": [
    "## Image Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa477f8d-1de9-4508-ab48-ec03458048c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = create_images_from_input_seqs(input_seqs, coordinates, batch_size, img_height, img_width, class_samples, vocab_in, max_length, basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ebc415-e293-46e0-aca5-14d71ff8a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].shape\n",
    "plt.imshow(images[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170b776-4e12-41da-aa5b-c6cfd4af8def",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = torch.tensor(images)\n",
    "image_patches = create_image_patches(image_tensor, patch_size=224, flatten_patches=False)\n",
    "image_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a5412-ca49-4643-bd88-85b9534b844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patches_dim0, n_patches_dim1 = 1, 5\n",
    "fix, ax = plt.subplots(n_patches_dim0, n_patches_dim1, figsize=(32,8))\n",
    "\n",
    "for i in range(n_patches_dim0):\n",
    "    for j in range(n_patches_dim1):\n",
    "        idx = i*n_patches_dim1+j\n",
    "        if n_patches_dim0 > 1:\n",
    "            ax[i, j].imshow(image_patches[0][idx])\n",
    "        else:\n",
    "            ax[j].imshow(image_patches[0][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a324620-a5f0-4ded-9532-0d794baa4f94",
   "metadata": {},
   "source": [
    "## Display real sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9838c65-1a29-4fa5-8ae8-e4fbce8455db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqgen.datasets.realdata import RealSequencesDataset\n",
    "dataset = RealSequencesDataset(filename=\"data/val/label.txt\", vocab_in=vocab_in, vocab_out=vocab_out, max_length=50, batch_size=10)\n",
    "\n",
    "input_seqs, coordinates, target_seqs = dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed4ac7-43a8-453f-9af9-6bfca220b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2*batch_size, figsize=(50, 80))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    coords = np.array(coordinates[i])\n",
    "    coords = np.array(normalize_coordinates(np.array([coords]), contains_class=False)).squeeze()\n",
    "    coords[:, [0,2]] *= img_width\n",
    "    coords[:, [1,3]] *= img_height\n",
    "    coords = torch.tensor(coords)\n",
    "    ax[2*i].imshow(create_img_array_from_coordinates(coords, img_height, img_width).numpy())\n",
    "    ax[2*i+1].imshow(create_cv2img_array_from_coordinates(input_seqs[i], coords, img_height, img_width, vocab_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c698ae2-7570-403d-8fda-2a866594a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2*batch_size, figsize=(50, 80))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    coords = add_noise_to_coordinates(coordinates[i])\n",
    "    coords = np.array(normalize_coordinates(np.array(coords), contains_class=False)).squeeze()\n",
    "    coords[:, [0,2]] *= img_width\n",
    "    coords[:, [1,3]] *= img_height\n",
    "    coords = torch.tensor(coords)\n",
    "    ax[2*i].imshow(create_img_array_from_coordinates(coords, img_height, img_width).numpy())\n",
    "    ax[2*i+1].imshow(create_cv2img_array_from_coordinates(input_seqs[i], coords, img_height, img_width, vocab_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1ad73-5b11-4633-969b-4561ba5121c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2*batch_size, figsize=(50, 80))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    coords = add_noise_to_coordinates(coordinates[i])\n",
    "    coords = np.array(normalize_coordinates(np.array(coords), contains_class=False)).squeeze()\n",
    "    coords[:, [0,2]] *= img_width\n",
    "    coords[:, [1,3]] *= img_height\n",
    "    coords = torch.tensor(coords)\n",
    "    in_seq, out_seq = replace_symbols(input_seqs[i], target_seqs[i], vocab_in, vocab_out)\n",
    "    ax[2*i].imshow(create_img_array_from_coordinates(coords, img_height, img_width).numpy())\n",
    "    ax[2*i+1].imshow(create_cv2img_array_from_coordinates(in_seq, coords, img_height, img_width, vocab_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43793a0f-34a2-42ad-b2ec-330e317ef597",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93578f-7c3c-48a6-9d7c-9f3dfafbb1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
