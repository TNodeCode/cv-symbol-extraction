{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "307e370c-40cb-4194-bf97-ad9e9ab745be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import seqgen.seq_gen as g\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from seqgen.model import seq2seq_lstm\n",
    "from seqgen.vocabulary import *\n",
    "from seqgen.model import transformer\n",
    "from seqgen.datasets.sequences import *\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "197faa56-4d2f-4453-bbda-32c5b4115b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bccbe91c-6286-4398-ad71-ee251ae1af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-2\n",
    "num_layers=1\n",
    "embedding_dim=32\n",
    "batch_size=512\n",
    "max_length=50\n",
    "heads=8\n",
    "dropout=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a78d88d5-47ef-4b8f-9d41-e1d8fb70c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "dataset = SyntheticSequenceDataset(vocab_in, vocab_out, max_length, batch_size, continue_prob=0.99, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "437f41f6-057b-41e5-9d52-1744666d4aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 51]), torch.Size([512, 51, 4]), torch.Size([512, 51]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqs, coordinates, target_seqs = dataset[0]\n",
    "input_seqs.shape, coordinates.shape, target_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be4f2ee2-b6ee-4b52-80c1-9a0f76d147ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  6, 16,  6,  7, 11, 12,  3,  6,  8, 10,  6,  4, 13, 15,  5,  6,  3,\n",
      "        10, 12,  8,  7,  4, 14, 13,  4,  9, 12, 16, 14,  8, 12, 14, 16,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n",
      "tensor([ 0,  4, 16,  5, 18, 13, 12, 16, 18,  6, 16,  6, 16,  8,  6, 16,  6, 13,\n",
      "        15,  8, 12, 16,  6, 15, 11, 15,  8, 15, 10, 16, 18,  3, 15, 17, 14, 14,\n",
      "        16, 14,  4, 10, 15,  9, 15, 12,  7,  7, 12,  3,  4,  1])\n",
      "tensor([ 4, 16,  5, 18, 13, 12, 16, 18,  6, 16,  6, 16,  8,  6, 16,  6, 13, 15,\n",
      "         8, 12, 16,  6, 15, 11, 15,  8, 15, 10, 16, 18,  3, 15, 17, 14, 14, 16,\n",
      "        14,  4, 10, 15,  9, 15, 12,  7,  7, 12,  3,  4,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "print(input_seqs[0, :-1])\n",
    "print(target_seqs[0, :-1])\n",
    "print(target_seqs[0, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c23a55-7c29-4a35-b0c4-09dd2a92d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutate_tokens(input_seq):\n",
    "    # Get the first index where tensor has an SOS or EOS token\n",
    "    sos_idx = list(input_seq).index(0)\n",
    "    eos_idx = list(input_seq).index(1)\n",
    "    # permutate all elements that are not SOS or EOS\n",
    "    idx_permuted = torch.cat([torch.arange(0, sos_idx+1), (torch.randperm(eos_idx - sos_idx - 1) + sos_idx+1), torch.arange(eos_idx, max_length+1)])\n",
    "    return idx_permuted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae84cc-67fe-4487-b119-51c1e6563d7c",
   "metadata": {},
   "source": [
    "# The Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7974696-6918-47ff-93b7-14cc4517dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint = False\n",
    "checkpoint_file = \"model_2023-01-15_09-17-53.pt\"\n",
    "\n",
    "# Transformer model\n",
    "model = transformer.Transformer(\n",
    "    src_vocab_size=len(vocab_in),\n",
    "    trg_vocab_size=len(vocab_out),\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_layers=num_layers,\n",
    "    heads=heads,\n",
    "    dropout=dropout,\n",
    "    src_pad_idx=1e10,\n",
    "    trg_pad_idx=1e10,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Initialize optimizer for encoder and decoder\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Load model weights from checkpoint\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40e27831-a686-4ab8-aa8e-0588faf059c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the feature sequences through the model\n",
    "output = model(input_seqs, target_seqs, coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb3c2a2-1b0a-404a-9d8c-0ae7fbc8485c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 51, 25]), torch.Size([512, 51, 1]), torch.Size([512, 51, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the predicted classes of the model\n",
    "topv, topi = output.topk(1, dim=2)\n",
    "output.shape, topi.shape, topv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd81330-0cf9-4ac9-a900-1a3ebbbba760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2522048950195312"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = 0.0\n",
    "for i in range(max_length):\n",
    "    loss += criterion(output[:, i, :], target_seqs[:, i])\n",
    "loss.item() / max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c59e1-3885-4a12-8696-9fcf3da9cf9e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f9ad9-7ae1-4b49-99b8-490eacfd5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS after epoch 0 5.782796223958333 LR 0.01 ACCURACY 0.03133370513096452\n",
      "LOSS after epoch 10 2.0091868381874236 LR 0.01 ACCURACY 0.2857700885739177\n",
      "LOSS after epoch 20 1.9814607208850337 LR 0.01 ACCURACY 0.30780054137576374\n",
      "LOSS after epoch 30 1.8554617189893536 LR 0.01 ACCURACY 0.3315409750211984\n",
      "LOSS after epoch 40 1.728349573471967 LR 0.01 ACCURACY 0.36774952001869676\n",
      "LOSS after epoch 50 1.6991493374693627 LR 0.01 ACCURACY 0.40435267696157096\n",
      "LOSS after epoch 60 1.5569861916934742 LR 0.01 ACCURACY 0.4239716183394194\n",
      "LOSS after epoch 70 1.5916507197361367 LR 0.01 ACCURACY 0.42978714760392905\n",
      "LOSS after epoch 80 1.482731538660386 LR 0.01 ACCURACY 0.4398317909799516\n",
      "LOSS after epoch 90 1.4392969468060661 LR 0.01 ACCURACY 0.45207668729126455\n",
      "LOSS after epoch 100 1.4778864243451286 LR 0.01 ACCURACY 0.451084181945771\n",
      "LOSS after epoch 110 1.4708017087450214 LR 0.01 ACCURACY 0.4498764343559742\n",
      "LOSS after epoch 120 1.4116333606196385 LR 0.01 ACCURACY 0.4600964598823339\n",
      "LOSS after epoch 130 1.4226249246036304 LR 0.01 ACCURACY 0.4583306741900742\n",
      "LOSS after epoch 140 1.3669319900811887 LR 0.01 ACCURACY 0.46736288173124196\n",
      "LOSS after epoch 150 1.3728513530656403 LR 0.01 ACCURACY 0.4703164848499\n",
      "LOSS after epoch 160 1.3449540979721968 LR 0.01 ACCURACY 0.4755460770335048\n",
      "LOSS after epoch 170 1.3912854662128524 LR 0.01 ACCURACY 0.46987404180690645\n",
      "LOSS after epoch 180 1.3616298600739123 LR 0.01 ACCURACY 0.47799744862131777\n",
      "LOSS after epoch 190 1.3785786348230697 LR 0.01 ACCURACY 0.47409518477506934\n",
      "LOSS after epoch 200 1.3454088697246476 LR 0.01 ACCURACY 0.4749362234026194\n",
      "LOSS after epoch 210 1.378813201305913 LR 0.01 ACCURACY 0.47901785662397744\n",
      "LOSS after epoch 220 1.3870483099245559 LR 0.01 ACCURACY 0.47385602528229354\n",
      "LOSS after epoch 230 1.3624264586205577 LR 0.01 ACCURACY 0.4728954081423581\n",
      "LOSS after epoch 240 1.3118552413641238 LR 0.01 ACCURACY 0.47753906254656614\n",
      "LOSS after epoch 250 1.2844594319661458 LR 0.01 ACCURACY 0.4812619573436677\n",
      "LOSS after epoch 260 1.3085503671683518 LR 0.01 ACCURACY 0.4781489140819758\n",
      "LOSS after epoch 270 1.3127779493144913 LR 0.01 ACCURACY 0.4830636170692742\n",
      "LOSS after epoch 280 1.3096377802830117 LR 0.01 ACCURACY 0.4808354591485113\n",
      "LOSS after epoch 290 1.3233195286171109 LR 0.01 ACCURACY 0.4838249356485903\n",
      "LOSS after epoch 300 1.2955422494925706 LR 0.01 ACCURACY 0.4856584827881306\n",
      "LOSS after epoch 310 1.279304205202589 LR 0.01 ACCURACY 0.488133769761771\n",
      "LOSS after epoch 320 1.3079087500478708 LR 0.01 ACCURACY 0.4846819195896387\n",
      "LOSS after epoch 330 1.2990540149165135 LR 0.01 ACCURACY 0.48930165814235804\n",
      "LOSS after epoch 340 1.2706007115981157 LR 0.01 ACCURACY 0.49429607689380645\n",
      "LOSS after epoch 350 1.3337631225585938 LR 0.01 ACCURACY 0.4920400186441839\n",
      "LOSS after epoch 360 1.2820558734968597 LR 0.01 ACCURACY 0.49188855178654195\n",
      "LOSS after epoch 370 1.2868715174057905 LR 0.01 ACCURACY 0.49332350082695486\n",
      "LOSS after epoch 380 1.2535747453278185 LR 0.01 ACCURACY 0.49424824574962256\n",
      "LOSS after epoch 390 1.3065800386316635 LR 0.01 ACCURACY 0.49345503831282256\n",
      "LOSS after epoch 400 1.3003750969381893 LR 0.01 ACCURACY 0.4901586421765387\n",
      "LOSS after epoch 410 1.276015038583793 LR 0.01 ACCURACY 0.49124282570555805\n",
      "LOSS after epoch 420 1.2748745188993567 LR 0.01 ACCURACY 0.4972058353945613\n",
      "LOSS after epoch 430 1.2384982389562271 LR 0.01 ACCURACY 0.49086415860801935\n",
      "LOSS after epoch 440 1.2804315604415595 LR 0.01 ACCURACY 0.4945392211899161\n",
      "LOSS after epoch 450 1.270917855057062 LR 0.01 ACCURACY 0.5011599173769354\n",
      "LOSS after epoch 460 1.200548209396063 LR 0.01 ACCURACY 0.49931042566895484\n",
      "LOSS after epoch 470 1.248005885703891 LR 0.01 ACCURACY 0.5018215879797936\n",
      "LOSS after epoch 480 1.2552421420228248 LR 0.01 ACCURACY 0.49347895523533225\n",
      "LOSS after epoch 490 1.2806643317727482 LR 0.01 ACCURACY 0.49144611014053224\n",
      "LOSS after epoch 500 1.2664937038047641 LR 0.01 ACCURACY 0.4939612559042871\n",
      "LOSS after epoch 510 1.230898464427275 LR 0.01 ACCURACY 0.49737723311409354\n",
      "LOSS after epoch 520 1.242151522168926 LR 0.01 ACCURACY 0.5025071757845581\n",
      "LOSS after epoch 530 1.2204871084175857 LR 0.01 ACCURACY 0.5028659112751483\n",
      "LOSS after epoch 540 1.2580451217352175 LR 0.01 ACCURACY 0.49441167125478386\n",
      "LOSS after epoch 550 1.2686166202320772 LR 0.01 ACCURACY 0.4932836414314806\n",
      "LOSS after epoch 560 1.3025752048866421 LR 0.01 ACCURACY 0.49434390929527583\n",
      "LOSS after epoch 570 1.2696605009191178 LR 0.01 ACCURACY 0.4929129475727677\n",
      "LOSS after epoch 580 1.2510693120021446 LR 0.01 ACCURACY 0.49726562574505806\n",
      "LOSS after epoch 590 1.2081841113520604 LR 0.01 ACCURACY 0.503483737912029\n",
      "LOSS after epoch 600 1.237537758023131 LR 0.01 ACCURACY 0.5028818568214775\n",
      "LOSS after epoch 610 1.2075163897346048 LR 0.01 ACCURACY 0.5047193877398968\n",
      "LOSS after epoch 620 1.2053448546166514 LR 0.01 ACCURACY 0.5105070157907903\n",
      "LOSS after epoch 630 1.208095625335095 LR 0.01 ACCURACY 0.5062938450369984\n",
      "LOSS after epoch 640 1.2005741642970664 LR 0.01 ACCURACY 0.5069116703234613\n",
      "LOSS after epoch 650 1.1893428727692248 LR 0.01 ACCURACY 0.5093909434042871\n",
      "LOSS after epoch 660 1.2408716538373161 LR 0.01 ACCURACY 0.5014110340736806\n",
      "LOSS after epoch 670 1.2029103297813266 LR 0.01 ACCURACY 0.5090481500141323\n",
      "LOSS after epoch 680 1.2208160699582566 LR 0.01 ACCURACY 0.5082469707354903\n",
      "LOSS after epoch 690 1.2414970397949219 LR 0.01 ACCURACY 0.507118941936642\n",
      "LOSS after epoch 700 1.1854458977194393 LR 0.01 ACCURACY 0.5051578453741967\n",
      "LOSS after epoch 710 1.2526221181832107 LR 0.01 ACCURACY 0.5044363833963871\n",
      "LOSS after epoch 720 1.2732063742244946 LR 0.01 ACCURACY 0.49085618774406614\n",
      "LOSS after epoch 730 1.203994077794692 LR 0.01 ACCURACY 0.5074896370992065\n",
      "LOSS after epoch 740 1.2133712020574832 LR 0.01 ACCURACY 0.5131457260809839\n",
      "LOSS after epoch 750 1.2407424777161842 LR 0.01 ACCURACY 0.5077846002765\n",
      "LOSS after epoch 760 1.2174677381328507 LR 0.01 ACCURACY 0.49924266608431933\n",
      "LOSS after epoch 770 1.1770310495413987 LR 0.01 ACCURACY 0.5062420275062323\n",
      "LOSS after epoch 780 1.1800936530618107 LR 0.01 ACCURACY 0.5124840570613742\n",
      "LOSS after epoch 790 1.2092528997683059 LR 0.01 ACCURACY 0.5134885202161967\n",
      "LOSS after epoch 800 1.242404862946155 LR 0.01 ACCURACY 0.5097377231344581\n",
      "LOSS after epoch 810 1.1813992519004672 LR 0.01 ACCURACY 0.513807397801429\n",
      "LOSS after epoch 820 1.2156463323854934 LR 0.01 ACCURACY 0.5125478311441839\n",
      "LOSS after epoch 830 1.1896364548627067 LR 0.01 ACCURACY 0.5105668037198484\n",
      "LOSS after epoch 840 1.2143185185451133 LR 0.01 ACCURACY 0.5116350444965064\n",
      "LOSS after epoch 850 1.1974220275878906 LR 0.01 ACCURACY 0.5126355223357677\n",
      "LOSS after epoch 860 1.2013819825415517 LR 0.01 ACCURACY 0.5090441662818194\n",
      "LOSS after epoch 870 1.1584325304218368 LR 0.01 ACCURACY 0.5137117335572838\n",
      "LOSS after epoch 880 1.1774128932578891 LR 0.01 ACCURACY 0.5160435265861452\n",
      "LOSS after epoch 890 1.1905489154890472 LR 0.01 ACCURACY 0.5133649548515677\n",
      "LOSS after epoch 900 1.1727606081495099 LR 0.01 ACCURACY 0.5120017534121871\n",
      "LOSS after epoch 910 1.2317533306047028 LR 0.01 ACCURACY 0.5037547838874161\n",
      "LOSS after epoch 920 1.1750471077713311 LR 0.01 ACCURACY 0.5124681128188968\n",
      "LOSS after epoch 930 1.2545786838905484 LR 0.01 ACCURACY 0.509893175587058\n",
      "LOSS after epoch 940 1.1555293962067248 LR 0.01 ACCURACY 0.5194754462689162\n",
      "LOSS after epoch 950 1.2153362947351791 LR 0.01 ACCURACY 0.5098891899921\n",
      "LOSS after epoch 960 1.2146067899816178 LR 0.01 ACCURACY 0.49012675499543545\n",
      "LOSS after epoch 970 1.2035446166992188 LR 0.01 ACCURACY 0.505608259793371\n",
      "LOSS after epoch 980 1.1821765525668275 LR 0.01 ACCURACY 0.5140425689518452\n",
      "LOSS after epoch 990 1.1955226823395373 LR 0.01 ACCURACY 0.5179966507479549\n",
      "LOSS after epoch 1000 1.1329204334932215 LR 0.01 ACCURACY 0.5241828752681613\n",
      "LOSS after epoch 1010 1.1554188447840072 LR 0.01 ACCURACY 0.5130261478945612\n",
      "LOSS after epoch 1020 1.1788660685221355 LR 0.01 ACCURACY 0.516908482182771\n",
      "LOSS after epoch 1030 1.2061436971028645 LR 0.01 ACCURACY 0.5160953449085355\n",
      "LOSS after epoch 1040 1.2192690980200673 LR 0.01 ACCURACY 0.5144132644869387\n",
      "LOSS after epoch 1050 1.2097128325817632 LR 0.01 ACCURACY 0.5070950250141323\n",
      "LOSS after epoch 1060 1.210272321514055 LR 0.01 ACCURACY 0.5036511493846774\n",
      "LOSS after epoch 1070 1.1606941971124387 LR 0.01 ACCURACY 0.5180285388603807\n",
      "LOSS after epoch 1080 1.1653610678280102 LR 0.01 ACCURACY 0.5173349783755838\n",
      "LOSS after epoch 1090 1.1991708044912301 LR 0.01 ACCURACY 0.5181202157400548\n",
      "LOSS after epoch 1100 1.1832703235102635 LR 0.01 ACCURACY 0.5086256387643516\n",
      "LOSS after epoch 1110 1.1752860125373392 LR 0.01 ACCURACY 0.5187619583681226\n",
      "LOSS after epoch 1120 1.198523577521829 LR 0.01 ACCURACY 0.518757970444858\n",
      "LOSS after epoch 1130 1.1184794856052773 LR 0.01 ACCURACY 0.5219826203770935\n",
      "LOSS after epoch 1140 1.176390853582644 LR 0.01 ACCURACY 0.5261918023228646\n",
      "LOSS after epoch 1150 1.1730151456945084 LR 0.01 ACCURACY 0.5184948968701064\n",
      "LOSS after epoch 1160 1.2191239899280024 LR 0.01 ACCURACY 0.5105309303849935\n",
      "LOSS after epoch 1170 1.177444906795726 LR 0.01 ACCURACY 0.5171516270376741\n",
      "LOSS after epoch 1180 1.1683637581619561 LR 0.01 ACCURACY 0.5201371160335839\n",
      "LOSS after epoch 1190 1.1573464636709176 LR 0.01 ACCURACY 0.5200653685256839\n",
      "LOSS after epoch 1200 1.147664088828891 LR 0.01 ACCURACY 0.5245695148594678\n",
      "LOSS after epoch 1210 1.1673112009085862 LR 0.01 ACCURACY 0.5235411331057549\n",
      "LOSS after epoch 1220 1.1278623693129595 LR 0.01 ACCURACY 0.5308234991505743\n",
      "LOSS after epoch 1230 1.1543546190448837 LR 0.01 ACCURACY 0.5305484677664936\n",
      "LOSS after epoch 1240 1.1921057607613357 LR 0.01 ACCURACY 0.5210259870626033\n",
      "LOSS after epoch 1250 1.2197329203287761 LR 0.01 ACCURACY 0.5133888707496226\n",
      "LOSS after epoch 1260 1.116115270876417 LR 0.01 ACCURACY 0.5238839264959096\n",
      "LOSS after epoch 1270 1.151407428816253 LR 0.01 ACCURACY 0.5234175688587129\n",
      "LOSS after epoch 1280 1.1270021924785538 LR 0.01 ACCURACY 0.5358458208851516\n",
      "LOSS after epoch 1290 1.1449828802370559 LR 0.01 ACCURACY 0.5271324928849935\n",
      "LOSS after epoch 1300 1.1498522290996476 LR 0.01 ACCURACY 0.5262595648877323\n",
      "LOSS after epoch 1310 1.2782833622951133 LR 0.01 ACCURACY 0.5133290805853903\n",
      "LOSS after epoch 1320 1.1993582482431449 LR 0.01 ACCURACY 0.508980389405042\n",
      "LOSS after epoch 1330 1.177629358628217 LR 0.01 ACCURACY 0.521727517619729\n",
      "LOSS after epoch 1340 1.1383226432052314 LR 0.01 ACCURACY 0.5218311536125839\n",
      "LOSS after epoch 1350 1.1302339142444087 LR 0.01 ACCURACY 0.5328204691410064\n",
      "LOSS after epoch 1360 1.1599130817488128 LR 0.01 ACCURACY 0.5306839895434677\n",
      "LOSS after epoch 1370 1.1333596472646676 LR 0.01 ACCURACY 0.5215202481485903\n",
      "LOSS after epoch 1380 1.1967083052092908 LR 0.01 ACCURACY 0.5160475122742355\n",
      "LOSS after epoch 1390 1.1574757893880208 LR 0.01 ACCURACY 0.5300382643006742\n",
      "LOSS after epoch 1400 1.1048373802035463 LR 0.01 ACCURACY 0.5325494239106774\n",
      "LOSS after epoch 1410 1.1622357087976791 LR 0.01 ACCURACY 0.5244100749492645\n",
      "LOSS after epoch 1420 1.1221569285673254 LR 0.01 ACCURACY 0.5289859676733613\n",
      "LOSS after epoch 1430 1.1221057667451746 LR 0.01 ACCURACY 0.5289979252032936\n",
      "LOSS after epoch 1440 1.1725614211138558 LR 0.01 ACCURACY 0.5252431422472\n",
      "LOSS after epoch 1450 1.1756795247395833 LR 0.01 ACCURACY 0.5172433035448194\n",
      "LOSS after epoch 1460 1.1075563617781097 LR 0.01 ACCURACY 0.5284319165162742\n",
      "LOSS after epoch 1470 1.129871218812232 LR 0.01 ACCURACY 0.5270846590399743\n",
      "LOSS after epoch 1480 1.1482225305893843 LR 0.01 ACCURACY 0.5290736599825323\n",
      "LOSS after epoch 1490 1.1421279159246707 LR 0.01 ACCURACY 0.5267817273736\n",
      "LOSS after epoch 1500 1.147568197811351 LR 0.01 ACCURACY 0.5229073651134968\n",
      "LOSS after epoch 1510 1.1171889211617263 LR 0.01 ACCURACY 0.5262994232587517\n",
      "LOSS after epoch 1520 1.1164731044395297 LR 0.01 ACCURACY 0.527618778962642\n",
      "LOSS after epoch 1530 1.2036905475691253 LR 0.01 ACCURACY 0.5249601394869388\n",
      "LOSS after epoch 1540 1.1987909055223651 LR 0.01 ACCURACY 0.5128427933901548\n",
      "LOSS after epoch 1550 1.1269633723240273 LR 0.01 ACCURACY 0.5196388706564903\n",
      "LOSS after epoch 1560 1.1167310078938801 LR 0.01 ACCURACY 0.5323979575186968\n",
      "LOSS after epoch 1570 1.11624392341165 LR 0.01 ACCURACY 0.5308912600390613\n",
      "LOSS after epoch 1580 1.1039597006405102 LR 0.01 ACCURACY 0.534406884200871\n",
      "LOSS after epoch 1590 1.1342805600633807 LR 0.01 ACCURACY 0.5328364145942033\n",
      "LOSS after epoch 1600 1.1478543749042587 LR 0.01 ACCURACY 0.5301100110635162\n",
      "LOSS after epoch 1610 1.122122371897978 LR 0.01 ACCURACY 0.5344826188869775\n",
      "LOSS after epoch 1620 1.1329838621850108 LR 0.01 ACCURACY 0.5338767500594258\n",
      "LOSS after epoch 1630 1.1375285877900965 LR 0.01 ACCURACY 0.5294762415811419\n",
      "LOSS after epoch 1640 1.0942666296865426 LR 0.01 ACCURACY 0.5331632619723677\n",
      "LOSS after epoch 1650 1.0840671015720742 LR 0.01 ACCURACY 0.5310467144474387\n",
      "LOSS after epoch 1660 1.1067674674239814 LR 0.01 ACCURACY 0.5334223510697484\n",
      "LOSS after epoch 1670 1.095537597057866 LR 0.01 ACCURACY 0.5369698638096452\n",
      "LOSS after epoch 1680 1.0953186633540135 LR 0.01 ACCURACY 0.5357979883439838\n",
      "LOSS after epoch 1690 1.1931150847790288 LR 0.01 ACCURACY 0.5205636166967451\n",
      "LOSS after epoch 1700 1.124006981943168 LR 0.01 ACCURACY 0.5247568545863033\n",
      "LOSS after epoch 1710 1.1191334443933822 LR 0.01 ACCURACY 0.5362842773087323\n",
      "LOSS after epoch 1720 1.130708283069087 LR 0.01 ACCURACY 0.5343231809325516\n",
      "LOSS after epoch 1730 1.094667023303462 LR 0.01 ACCURACY 0.533645564969629\n",
      "LOSS after epoch 1740 1.094811383415671 LR 0.01 ACCURACY 0.5344268151558935\n",
      "LOSS after epoch 1750 1.144138261383655 LR 0.01 ACCURACY 0.5331074594520032\n",
      "LOSS after epoch 1760 1.174504971971699 LR 0.01 ACCURACY 0.5079360645264387\n",
      "LOSS after epoch 1770 1.1477557163612515 LR 0.01 ACCURACY 0.5222576501779258\n",
      "LOSS after epoch 1780 1.105380937164905 LR 0.01 ACCURACY 0.5309630079194904\n",
      "LOSS after epoch 1790 1.1557151196049709 LR 0.01 ACCURACY 0.5351602330803871\n",
      "LOSS after epoch 1800 1.0865585476744408 LR 0.01 ACCURACY 0.5344786330126226\n",
      "LOSS after epoch 1810 1.1062350553624771 LR 0.01 ACCURACY 0.5371332884766161\n",
      "LOSS after epoch 1820 1.0998955520929075 LR 0.01 ACCURACY 0.5392817268148065\n",
      "LOSS after epoch 1830 1.0993754069010417 LR 0.01 ACCURACY 0.5400749339722097\n",
      "LOSS after epoch 1840 1.1032192005830652 LR 0.01 ACCURACY 0.5381497118622065\n",
      "LOSS after epoch 1850 1.1150235194785922 LR 0.01 ACCURACY 0.5299067272804677\n",
      "LOSS after epoch 1860 1.1470236684761794 LR 0.01 ACCURACY 0.5399912283755839\n",
      "LOSS after epoch 1870 1.118322634229473 LR 0.01 ACCURACY 0.5395886453799903\n",
      "LOSS after epoch 1880 1.074648240033318 LR 0.01 ACCURACY 0.5350526114925742\n",
      "LOSS after epoch 1890 1.0748478010589002 LR 0.01 ACCURACY 0.5350007943809032\n",
      "LOSS after epoch 1900 1.0966104245653339 LR 0.01 ACCURACY 0.5429966491647065\n",
      "LOSS after epoch 1910 1.101371540742762 LR 0.01 ACCURACY 0.5411232447251677\n",
      "LOSS after epoch 1920 1.0841898450664444 LR 0.01 ACCURACY 0.5365035051479936\n",
      "LOSS after epoch 1930 1.1105235230688955 LR 0.01 ACCURACY 0.5389469044283033\n",
      "LOSS after epoch 1940 1.071108425364775 LR 0.01 ACCURACY 0.5387117330916226\n",
      "LOSS after epoch 1950 1.1105031780168122 LR 0.01 ACCURACY 0.5412388374097645\n",
      "LOSS after epoch 1960 1.0766003926595051 LR 0.01 ACCURACY 0.5393335433676839\n",
      "LOSS after epoch 1970 1.1104836557425706 LR 0.01 ACCURACY 0.5390385820530355\n",
      "LOSS after epoch 1980 1.1237973231895297 LR 0.01 ACCURACY 0.531835934985429\n",
      "LOSS after epoch 1990 1.1769679200415517 LR 0.01 ACCURACY 0.5370575544424355\n",
      "LOSS after epoch 2000 1.098719129375383 LR 0.01 ACCURACY 0.5286949907429517\n",
      "LOSS after epoch 2010 1.1536817363664216 LR 0.01 ACCURACY 0.506592792738229\n",
      "LOSS after epoch 2020 1.101240868661918 LR 0.01 ACCURACY 0.5258689395152032\n",
      "LOSS after epoch 2030 1.1318160412358302 LR 0.01 ACCURACY 0.5333147295750678\n",
      "LOSS after epoch 2040 1.0645563462201286 LR 0.01 ACCURACY 0.5369539191946388\n",
      "LOSS after epoch 2050 1.0634678859336704 LR 0.01 ACCURACY 0.5485012728720904\n",
      "LOSS after epoch 2060 1.0724171657188266 LR 0.01 ACCURACY 0.5439891567453742\n",
      "LOSS after epoch 2070 1.1453160304649204 LR 0.01 ACCURACY 0.5389827787876129\n",
      "LOSS after epoch 2080 1.0652821110744102 LR 0.01 ACCURACY 0.5449258571490645\n",
      "LOSS after epoch 2090 1.087419547286688 LR 0.01 ACCURACY 0.5438456605188549\n",
      "LOSS after epoch 2100 1.094920513676662 LR 0.01 ACCURACY 0.5451889331452549\n",
      "LOSS after epoch 2110 1.1127308864219516 LR 0.01 ACCURACY 0.540055004414171\n",
      "LOSS after epoch 2120 1.1056722753188188 LR 0.01 ACCURACY 0.5404097547754645\n",
      "LOSS after epoch 2130 1.0985378190582873 LR 0.01 ACCURACY 0.5406927591189742\n",
      "LOSS after epoch 2140 1.1351599599800857 LR 0.01 ACCURACY 0.5209064081311225\n",
      "LOSS after epoch 2150 1.0955159804400276 LR 0.01 ACCURACY 0.5314094366505742\n",
      "LOSS after epoch 2160 1.0756125356636794 LR 0.01 ACCURACY 0.5399633264169097\n",
      "LOSS after epoch 2170 1.0951218698538987 LR 0.01 ACCURACY 0.5405731796287\n",
      "LOSS after epoch 2180 1.100558037851371 LR 0.01 ACCURACY 0.5366031545214355\n",
      "LOSS after epoch 2190 1.0823965633616728 LR 0.01 ACCURACY 0.544180482532829\n",
      "LOSS after epoch 2200 1.0894065557741652 LR 0.01 ACCURACY 0.5392139635980129\n",
      "LOSS after epoch 2210 1.0628927642223882 LR 0.01 ACCURACY 0.5419602968730033\n",
      "LOSS after epoch 2220 1.108397689520144 LR 0.01 ACCURACY 0.5470384228043258\n",
      "LOSS after epoch 2230 1.0605298958572686 LR 0.01 ACCURACY 0.5464963305741548\n",
      "LOSS after epoch 2240 1.1038682227041208 LR 0.01 ACCURACY 0.5464644424617291\n",
      "LOSS after epoch 2250 1.085979087679994 LR 0.01 ACCURACY 0.5431162288412452\n",
      "LOSS after epoch 2260 1.0860715379901962 LR 0.01 ACCURACY 0.548863997682929\n",
      "LOSS after epoch 2270 1.0673367369408702 LR 0.01 ACCURACY 0.5460578736849129\n",
      "LOSS after epoch 2280 1.0478887370988434 LR 0.01 ACCURACY 0.5494260184466839\n",
      "LOSS after epoch 2290 1.1486051970837163 LR 0.01 ACCURACY 0.537671395484358\n",
      "LOSS after epoch 2300 1.1195970423081343 LR 0.01 ACCURACY 0.5322863494977355\n",
      "LOSS after epoch 2310 1.0614920223460478 LR 0.01 ACCURACY 0.5453922166489065\n",
      "LOSS after epoch 2320 1.0601830575980393 LR 0.01 ACCURACY 0.5460100427269936\n",
      "LOSS after epoch 2330 1.1013761034198837 LR 0.01 ACCURACY 0.5405532488599419\n",
      "LOSS after epoch 2340 1.0844944972617954 LR 0.01 ACCURACY 0.5438018144108355\n",
      "LOSS after epoch 2350 1.0674515518487668 LR 0.01 ACCURACY 0.5481744235381484\n",
      "LOSS after epoch 2360 1.057873669792624 LR 0.01 ACCURACY 0.5479073641821742\n",
      "LOSS after epoch 2370 1.072982339298024 LR 0.01 ACCURACY 0.5456273874267936\n",
      "LOSS after epoch 2380 1.10297565834195 LR 0.01 ACCURACY 0.5469746473245323\n",
      "LOSS after epoch 2390 1.1332005519492954 LR 0.01 ACCURACY 0.5378547496162355\n",
      "LOSS after epoch 2400 1.0890047409955192 LR 0.01 ACCURACY 0.538002228550613\n",
      "LOSS after epoch 2410 1.0836918400783164 LR 0.01 ACCURACY 0.5426139961928129\n",
      "LOSS after epoch 2420 1.0765522975547641 LR 0.01 ACCURACY 0.545216834358871\n",
      "LOSS after epoch 2430 1.0796870811312806 LR 0.01 ACCURACY 0.5516143156215548\n",
      "LOSS after epoch 2440 1.0914581149232154 LR 0.01 ACCURACY 0.5495376257225871\n",
      "LOSS after epoch 2450 1.054068546669156 LR 0.01 ACCURACY 0.5519491366110743\n",
      "LOSS after epoch 2460 1.0766233556410845 LR 0.01 ACCURACY 0.5541852660477161\n",
      "LOSS after epoch 2470 1.075227326037837 LR 0.01 ACCURACY 0.5486088955774904\n",
      "LOSS after epoch 2480 1.045991112204159 LR 0.01 ACCURACY 0.548461414128542\n",
      "LOSS after epoch 2490 1.0820281084846048 LR 0.01 ACCURACY 0.5432955970056355\n",
      "LOSS after epoch 2500 1.0667405969956343 LR 0.01 ACCURACY 0.5461535369046032\n",
      "LOSS after epoch 2510 1.0790906419941024 LR 0.01 ACCURACY 0.5530652097426355\n",
      "LOSS after epoch 2520 1.0868385913325291 LR 0.01 ACCURACY 0.5465999660082161\n",
      "LOSS after epoch 2530 1.0541473089479934 LR 0.01 ACCURACY 0.5556720332242548\n",
      "LOSS after epoch 2540 1.182188445446538 LR 0.01 ACCURACY 0.5436264333315194\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(50000):    \n",
    "    # Get a batch of training data\n",
    "    input_seqs, coordinates, target_seqs = dataset[0]\n",
    "    #positions_targets = seq2seq_lstm.get_position_encoding(max_length, embedding_dim, device=device).repeat(batch_size, 1, 1)\n",
    "    \n",
    "    # Set gradients of all model parameters to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "    accuracy = 0.0\n",
    "\n",
    "    #####################\n",
    "    #    TRANSFORMER    #\n",
    "    #####################\n",
    "    \n",
    "    # Run the input sequences through the model\n",
    "    output = model(input_seqs[:, :-1], target_seqs[:, :-1], coordinates[:, :-1])\n",
    "    \n",
    "    # Iterate over sequence positions to compute the loss\n",
    "    for i in range(max_length-1):\n",
    "        # Get the predicted classes of the model\n",
    "        topv, topi = output[:, i, :].topk(1)\n",
    "        loss += criterion(output[:, i, :], target_seqs[:, i+1])\n",
    "        accuracy += float((topi.squeeze() == target_seqs[:, i+1]).sum() / (target_seqs.size(0)*(target_seqs.size(1)-2)))\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print_every = 10\n",
    "    if not epoch % print_every:\n",
    "        _accuracy = sum(accuracies[-print_every:]) / print_every\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"LOSS after epoch {epoch}\", loss.item() / (target_seqs.size(1)), \"LR\", lr, \"ACCURACY\", _accuracy)\n",
    "\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    accuracy = 0.0\n",
    "\n",
    "    # Update weights of encoder and decoder\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e3605-8f59-42ee-b260-5bb500518d00",
   "metadata": {},
   "source": [
    "#### Save model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb67bec2-ccfd-4492-bd1a-e51fad94fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "model_data = {\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"heads\": heads,\n",
    "    \"dropout\": dropout,\n",
    "}\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"heads\": heads,\n",
    "    \"dropout\": dropout,\n",
    "}, \"transformer_\" + date_time + \".pt\")\n",
    "\n",
    "\n",
    "with open(\"training_\" + date_time + '.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56c813-13d3-45a2-b9e1-47fed03bcf3c",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "We run our input sequences through the model and get output seuences. Then we decode the output sequences with the Vocabulary class and get our final latex code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb8ff7-8d9a-4c28-8f95-264703cfe636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_seqs, coordinates, target_seqs):\n",
    "    vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "    vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        coordinate_encoding = seq2seq_lstm.get_coordinate_encoding(coordinates, d=embedding_dim, max_length=max_length)\n",
    "        output = model(input_seqs, target_seqs, coordinate_encoding)\n",
    "        # Get the predicted classes of the model\n",
    "        topv, topi = output.topk(1, dim=2)\n",
    "        \n",
    "        return topi.squeeze()\n",
    "    \n",
    "def predict_sequentially(input_seqs, coordinates):\n",
    "    prediction = torch.zeros((input_seqs.size(0), input_seqs.size(1)-1)).to(torch.int64)\n",
    "    for i in range(max_length-1):\n",
    "        output = predict(input_seqs, coordinates, prediction)\n",
    "        prediction[:, i] = output[:, i]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4224ca-ab02-437b-a0ea-5704b1c01daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict_sequentially(input_seqs, coordinates)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf18d6-94da-42ce-8bad-86a9f2d16d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random sequence and its prediction from the model\n",
    "import random\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "predictions = predict(input_seqs, coordinates, target_seqs)\n",
    "\n",
    "i = random.randint(0, predictions.size(0)-1)\n",
    "print(\"MODEL INPUT\", vocab_in.decode_sequence(input_seqs[i, 1:].cpu().numpy()))\n",
    "print(\"MODEL OUTPUT\", vocab_out.decode_sequence(predictions[i, :-1].cpu().numpy()))\n",
    "print(\"TARGET OUTPUT\", vocab_out.decode_sequence(target_seqs[i, 1:].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeae19b-77d4-4715-8068-9822af23009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = vocab_out.decode_sequence(predictions[i].cpu().numpy())\n",
    "prediction = list(filter(lambda x: x != '<end>', prediction))\n",
    "prediction = \"\".join(prediction)\n",
    "print(\"MODEL OUTPUT\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4141cd30-1b5f-4a10-b844-08b2cbb28576",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sequentially(input_seqs[0:3], coordinates[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e1244-c9b2-4f1e-8e7c-58ad12ccd273",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seqs[0:3, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6130ca-cb78-49d1-b167-b6d0817afbbc",
   "metadata": {},
   "source": [
    "## Prediction for permutated sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472c7a7-3c27-46d4-ba18-e4cbf9d52204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_permutated_batch(input_seq, coordinates):\n",
    "    seqs = torch.zeros((5, input_seq.size(0))).to(torch.int64)\n",
    "    coords = torch.zeros((5, coordinates.size(0), coordinates.size(1)))\n",
    "    for i in range(5):\n",
    "        idx_permutated = permutate_tokens(input_seq)\n",
    "        seqs[i, :] = input_seq[idx_permutated]\n",
    "        coords[i, :] = coordinates[idx_permutated]\n",
    "    return seqs, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a272c96c-9ac8-4ea7-9f84-03c28c81db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_permutated, coords_permutated = generate_permutated_batch(input_seqs[0], coordinates[0])\n",
    "input_permutated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571029b-a0e2-4574-a70e-f02ca965611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sequentially(input_permutated, coords_permutated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c5daf2-f175-4e54-bdb0-2f9c1844b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seqs[0, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983b254-e462-48ce-bc7e-94c2fba39a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
