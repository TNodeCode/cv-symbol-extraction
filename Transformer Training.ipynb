{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307e370c-40cb-4194-bf97-ad9e9ab745be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seqgen.seq_gen as g\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from seqgen.model import seq2seq_lstm\n",
    "from seqgen.vocabulary import *\n",
    "from seqgen.model import transformer\n",
    "from seqgen.datasets.sequences import *\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "197faa56-4d2f-4453-bbda-32c5b4115b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccbe91c-6286-4398-ad71-ee251ae1af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "num_layers=3\n",
    "embedding_dim=32\n",
    "batch_size=64\n",
    "max_length=50\n",
    "heads=8\n",
    "dropout=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78d88d5-47ef-4b8f-9d41-e1d8fb70c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "dataset = SyntheticSequenceDataset(vocab_in, vocab_out, max_length, batch_size, continue_prob=0.99, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437f41f6-057b-41e5-9d52-1744666d4aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilof\\PycharmProjects\\UdacityProjects\\YoloImagePreparation\\seqgen\\datasets\\sequences.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
      "C:\\Users\\tilof\\PycharmProjects\\UdacityProjects\\YoloImagePreparation\\seqgen\\datasets\\sequences.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coordinates = torch.tensor(features[:, :, 1:])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 51]),\n",
       " torch.Size([64, 51, 4]),\n",
       " torch.Size([64, 51]),\n",
       " torch.Size([64, 51, 32]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqs, coordinates, target_seqs = dataset[0]\n",
    "coordinate_encoding = seq2seq_lstm.get_coordinate_encoding(coordinates, d=embedding_dim, max_length=max_length)\n",
    "input_seqs.shape, coordinates.shape, target_seqs.shape, coordinate_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be4f2ee2-b6ee-4b52-80c1-9a0f76d147ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 16, 12,  4, 16,  8,  8, 15,  8,  8,  3, 11, 14,  4,  8, 10, 12, 16,\n",
      "        12,  6,  7,  5, 11, 13,  3,  6,  5, 10, 11, 11,  6,  4,  5, 15, 11,  6,\n",
      "        11, 14,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n",
      "tensor([ 0,  3, 10,  4, 11, 13,  8, 17,  8,  8,  3, 11, 11, 11,  8, 17, 18, 12,\n",
      "        18,  6,  7,  5,  6,  5, 12, 18,  5,  6, 11,  8,  6, 10, 14, 12, 11,  4,\n",
      "         4, 14,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n",
      "tensor([ 3, 10,  4, 11, 13,  8, 17,  8,  8,  3, 11, 11, 11,  8, 17, 18, 12, 18,\n",
      "         6,  7,  5,  6,  5, 12, 18,  5,  6, 11,  8,  6, 10, 14, 12, 11,  4,  4,\n",
      "        14,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "print(input_seqs[0, :-1])\n",
    "print(target_seqs[0, :-1])\n",
    "print(target_seqs[0, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c23a55-7c29-4a35-b0c4-09dd2a92d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutate_tokens(input_seq):\n",
    "    # Get the first index where tensor has an SOS or EOS token\n",
    "    sos_idx = list(input_seq).index(0)\n",
    "    eos_idx = list(input_seq).index(1)\n",
    "    # permutate all elements that are not SOS or EOS\n",
    "    idx_permuted = torch.cat([torch.arange(0, sos_idx+1), (torch.randperm(eos_idx - sos_idx - 1) + sos_idx+1), torch.arange(eos_idx, max_length+1)])\n",
    "    return idx_permuted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae84cc-67fe-4487-b119-51c1e6563d7c",
   "metadata": {},
   "source": [
    "# The Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7974696-6918-47ff-93b7-14cc4517dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint = False\n",
    "checkpoint_file = \"model_2023-01-15_09-17-53.pt\"\n",
    "\n",
    "# Transformer model\n",
    "model = transformer.Transformer(\n",
    "    src_vocab_size=len(vocab_in),\n",
    "    trg_vocab_size=len(vocab_out),\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_layers=num_layers,\n",
    "    heads=heads,\n",
    "    dropout=dropout,\n",
    "    src_pad_idx=1e10,\n",
    "    trg_pad_idx=1e10,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Initialize optimizer for encoder and decoder\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Load model weights from checkpoint\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40e27831-a686-4ab8-aa8e-0588faf059c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the feature sequences through the model\n",
    "output = model(input_seqs, target_seqs, coordinate_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb3c2a2-1b0a-404a-9d8c-0ae7fbc8485c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 51, 25]), torch.Size([64, 51, 1]), torch.Size([64, 51, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the predicted classes of the model\n",
    "topv, topi = output.topk(1, dim=2)\n",
    "output.shape, topi.shape, topv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd81330-0cf9-4ac9-a900-1a3ebbbba760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6018032836914062"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = 0.0\n",
    "for i in range(max_length):\n",
    "    loss += criterion(output[:, i, :], target_seqs[:, i])\n",
    "loss.item() / max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c59e1-3885-4a12-8696-9fcf3da9cf9e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f9ad9-7ae1-4b49-99b8-490eacfd5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS after epoch 0 3.3296122831456803 LR 0.001 ACCURACY 0.003730867357808165\n",
      "LOSS after epoch 10 2.6482301599839153 LR 0.001 ACCURACY 0.17551020530227107\n",
      "LOSS after epoch 20 2.262874827665441 LR 0.001 ACCURACY 0.2623724506585859\n",
      "LOSS after epoch 30 2.156153510598575 LR 0.001 ACCURACY 0.24626913435931783\n",
      "LOSS after epoch 40 2.1638811896829044 LR 0.001 ACCURACY 0.2733737262664363\n",
      "LOSS after epoch 50 1.933803633147595 LR 0.001 ACCURACY 0.26594387901131994\n",
      "LOSS after epoch 60 2.082317875880821 LR 0.001 ACCURACY 0.267442603391828\n",
      "LOSS after epoch 70 2.086745617436428 LR 0.001 ACCURACY 0.256664542274666\n",
      "LOSS after epoch 80 1.9946737850413603 LR 0.001 ACCURACY 0.26575255258067043\n",
      "LOSS after epoch 90 2.0696295943914675 LR 0.001 ACCURACY 0.2607142875553109\n",
      "LOSS after epoch 100 1.9815061980602788 LR 0.001 ACCURACY 0.28431122513720763\n",
      "LOSS after epoch 110 1.9188772463331036 LR 0.001 ACCURACY 0.28737244994263167\n",
      "LOSS after epoch 120 2.127329059675628 LR 0.001 ACCURACY 0.2806760215375107\n",
      "LOSS after epoch 130 2.097477034026501 LR 0.001 ACCURACY 0.27471301142068116\n",
      "LOSS after epoch 140 1.9423856548234528 LR 0.001 ACCURACY 0.2941645419545239\n",
      "LOSS after epoch 150 1.9842822504978554 LR 0.001 ACCURACY 0.2750637768767774\n",
      "LOSS after epoch 160 2.064479005103018 LR 0.001 ACCURACY 0.26693239936721513\n",
      "LOSS after epoch 170 2.104124630198759 LR 0.001 ACCURACY 0.26820790935016703\n",
      "LOSS after epoch 180 2.026124393238741 LR 0.001 ACCURACY 0.27197066459339114\n",
      "LOSS after epoch 190 1.8747905656403185 LR 0.001 ACCURACY 0.29158163361425976\n",
      "LOSS after epoch 200 2.086993647556679 LR 0.001 ACCURACY 0.2652423476421973\n",
      "LOSS after epoch 210 2.1857929603726256 LR 0.001 ACCURACY 0.2874681128480006\n",
      "LOSS after epoch 220 1.8141759236653645 LR 0.001 ACCURACY 0.30647321488067975\n",
      "LOSS after epoch 230 1.890901154162837 LR 0.001 ACCURACY 0.2897002559766406\n",
      "LOSS after epoch 240 1.912458531996783 LR 0.001 ACCURACY 0.30631377625104506\n",
      "LOSS after epoch 250 1.9639762429630054 LR 0.001 ACCURACY 0.28928571569849737\n",
      "LOSS after epoch 260 1.8551810769473804 LR 0.001 ACCURACY 0.2910395417828113\n",
      "LOSS after epoch 270 1.9794942818435968 LR 0.001 ACCURACY 0.30274234772659836\n",
      "LOSS after epoch 280 1.971570931228937 LR 0.001 ACCURACY 0.3037946437485516\n",
      "LOSS after epoch 290 1.9569301231234681 LR 0.001 ACCURACY 0.3091836744104512\n",
      "LOSS after epoch 300 1.8617435530120252 LR 0.001 ACCURACY 0.291135204967577\n",
      "LOSS after epoch 310 1.9838880651137407 LR 0.001 ACCURACY 0.298565052455524\n",
      "LOSS after epoch 320 2.0433623370002296 LR 0.001 ACCURACY 0.3143813777074683\n",
      "LOSS after epoch 330 1.9585173363779105 LR 0.001 ACCURACY 0.3167729605920613\n",
      "LOSS after epoch 340 1.7899306054208792 LR 0.001 ACCURACY 0.3161670928937383\n",
      "LOSS after epoch 350 1.9765240538354014 LR 0.001 ACCURACY 0.34215561352903023\n",
      "LOSS after epoch 360 1.9452184041341145 LR 0.001 ACCURACY 0.30229591962415725\n",
      "LOSS after epoch 370 1.9522127637676163 LR 0.001 ACCURACY 0.3243941332271788\n",
      "LOSS after epoch 380 1.894845401539522 LR 0.001 ACCURACY 0.3324936238117516\n",
      "LOSS after epoch 390 1.9052653593175553 LR 0.001 ACCURACY 0.31501913332613185\n",
      "LOSS after epoch 400 1.9085352280560661 LR 0.001 ACCURACY 0.34068877616664395\n",
      "LOSS after epoch 410 1.9053437476064645 LR 0.001 ACCURACY 0.334183674689848\n",
      "LOSS after epoch 420 2.029265011058134 LR 0.001 ACCURACY 0.31205357301514597\n",
      "LOSS after epoch 430 2.056905709060968 LR 0.001 ACCURACY 0.3132334200083278\n",
      "LOSS after epoch 440 1.937736810422411 LR 0.001 ACCURACY 0.313520409289049\n",
      "LOSS after epoch 450 1.93832831289254 LR 0.001 ACCURACY 0.3335778069566004\n",
      "LOSS after epoch 460 1.9541346232096355 LR 0.001 ACCURACY 0.3059630109695718\n",
      "LOSS after epoch 470 1.8679743748085171 LR 0.001 ACCURACY 0.3373405621852726\n",
      "LOSS after epoch 480 2.0480802947399663 LR 0.001 ACCURACY 0.31466836808249354\n",
      "LOSS after epoch 490 2.0256470324946383 LR 0.001 ACCURACY 0.32270408294862135\n",
      "LOSS after epoch 500 1.8642301372453278 LR 0.001 ACCURACY 0.34052933764178306\n",
      "LOSS after epoch 510 1.9677647609336704 LR 0.001 ACCURACY 0.33514030736987477\n",
      "LOSS after epoch 520 1.7427052516563266 LR 0.001 ACCURACY 0.3276147961500101\n",
      "LOSS after epoch 530 1.9851324044021905 LR 0.001 ACCURACY 0.3354591847630218\n",
      "LOSS after epoch 540 1.9276591282264859 LR 0.001 ACCURACY 0.33810586859472097\n",
      "LOSS after epoch 550 1.979075263528263 LR 0.001 ACCURACY 0.3260841847222764\n",
      "LOSS after epoch 560 1.7852130964690565 LR 0.001 ACCURACY 0.3207270417828113\n",
      "LOSS after epoch 570 1.9124209834080117 LR 0.001 ACCURACY 0.331505102943629\n",
      "LOSS after epoch 580 1.9967245962105544 LR 0.001 ACCURACY 0.3104910722002387\n",
      "LOSS after epoch 590 1.9854882932176776 LR 0.001 ACCURACY 0.32165178707800807\n",
      "LOSS after epoch 600 1.9221346986060048 LR 0.001 ACCURACY 0.3316645415034145\n",
      "LOSS after epoch 610 1.971720527200138 LR 0.001 ACCURACY 0.33182398065691815\n",
      "LOSS after epoch 620 1.9374771118164062 LR 0.001 ACCURACY 0.32624362274073065\n",
      "LOSS after epoch 630 1.8052351708505667 LR 0.001 ACCURACY 0.34518495004158467\n",
      "LOSS after epoch 640 1.9290686214671415 LR 0.001 ACCURACY 0.3163265316514298\n",
      "LOSS after epoch 650 1.9939521340762867 LR 0.001 ACCURACY 0.3140625008381903\n",
      "LOSS after epoch 660 1.8817556044634651 LR 0.001 ACCURACY 0.33485331767005844\n",
      "LOSS after epoch 670 1.8121020747166054 LR 0.001 ACCURACY 0.3198660724447109\n",
      "LOSS after epoch 680 1.6293949800379135 LR 0.001 ACCURACY 0.34441964377183465\n",
      "LOSS after epoch 690 1.828259785970052 LR 0.001 ACCURACY 0.34652423625811934\n",
      "LOSS after epoch 700 1.9481269986021752 LR 0.001 ACCURACY 0.32952806280227376\n",
      "LOSS after epoch 710 1.9305701162300857 LR 0.001 ACCURACY 0.3439413275686093\n",
      "LOSS after epoch 720 1.9895538629270066 LR 0.001 ACCURACY 0.32353316370281393\n",
      "LOSS after epoch 730 1.8302836698644303 LR 0.001 ACCURACY 0.33058035807916897\n",
      "LOSS after epoch 740 1.9750170240215226 LR 0.001 ACCURACY 0.32927296065026895\n",
      "LOSS after epoch 750 1.7878533157647825 LR 0.001 ACCURACY 0.3430165826925077\n",
      "LOSS after epoch 760 1.8971322751512714 LR 0.001 ACCURACY 0.3273278065607883\n",
      "LOSS after epoch 770 1.9725053076650583 LR 0.001 ACCURACY 0.31973852089140564\n",
      "LOSS after epoch 780 1.8291346232096355 LR 0.001 ACCURACY 0.3417410724214278\n",
      "LOSS after epoch 790 1.839185677322687 LR 0.001 ACCURACY 0.32008928650757296\n",
      "LOSS after epoch 800 1.8908695894129135 LR 0.001 ACCURACY 0.3355229606269859\n",
      "LOSS after epoch 810 1.8602072023877911 LR 0.001 ACCURACY 0.3250000014202669\n",
      "LOSS after epoch 820 1.8798847572476256 LR 0.001 ACCURACY 0.33807398033095526\n",
      "LOSS after epoch 830 1.8796023200539982 LR 0.001 ACCURACY 0.3468431132729165\n",
      "LOSS after epoch 840 1.8768862556008732 LR 0.001 ACCURACY 0.3267538272542879\n",
      "LOSS after epoch 850 1.949780782063802 LR 0.001 ACCURACY 0.31989796068519355\n",
      "LOSS after epoch 860 1.8660936542585784 LR 0.001 ACCURACY 0.3339604605571367\n",
      "LOSS after epoch 870 1.9330396465226716 LR 0.001 ACCURACY 0.3351084189955145\n",
      "LOSS after epoch 880 1.8728461172066482 LR 0.001 ACCURACY 0.3406887769699097\n",
      "LOSS after epoch 890 2.0594344793581496 LR 0.001 ACCURACY 0.33443877670215444\n",
      "LOSS after epoch 900 2.023449467677696 LR 0.001 ACCURACY 0.33816964339930566\n",
      "LOSS after epoch 910 1.7326706531001073 LR 0.001 ACCURACY 0.3492028071777895\n",
      "LOSS after epoch 920 1.8297026391122855 LR 0.001 ACCURACY 0.3312500009662472\n",
      "LOSS after epoch 930 1.8304841284658395 LR 0.001 ACCURACY 0.36562500121071934\n",
      "LOSS after epoch 940 1.7717373417873008 LR 0.001 ACCURACY 0.3488839298253879\n",
      "LOSS after epoch 950 1.9376917820350796 LR 0.001 ACCURACY 0.33207908291369675\n",
      "LOSS after epoch 960 1.8396236943263633 LR 0.001 ACCURACY 0.34400510273408147\n",
      "LOSS after epoch 970 2.0354512532552085 LR 0.001 ACCURACY 0.3465880118892528\n",
      "LOSS after epoch 980 1.8776658002068014 LR 0.001 ACCURACY 0.344419643713627\n",
      "LOSS after epoch 990 1.7186683205997242 LR 0.001 ACCURACY 0.34021046035923064\n",
      "LOSS after epoch 1000 1.864332311293658 LR 0.001 ACCURACY 0.34384566436056047\n",
      "LOSS after epoch 1010 1.9397897159352022 LR 0.001 ACCURACY 0.33450255227508024\n",
      "LOSS after epoch 1020 1.8452663047640931 LR 0.001 ACCURACY 0.3556122462498024\n",
      "LOSS after epoch 1030 1.8975225710401349 LR 0.001 ACCURACY 0.33753188932314515\n",
      "LOSS after epoch 1040 1.837167926863128 LR 0.001 ACCURACY 0.3506696439930238\n",
      "LOSS after epoch 1050 1.8211032643037683 LR 0.001 ACCURACY 0.3590242359787226\n",
      "LOSS after epoch 1060 1.8973087983972885 LR 0.001 ACCURACY 0.3650191334774718\n",
      "LOSS after epoch 1070 1.8527384739296109 LR 0.001 ACCURACY 0.3603954095509835\n",
      "LOSS after epoch 1080 1.6470835068646599 LR 0.001 ACCURACY 0.37235331723932175\n",
      "LOSS after epoch 1090 1.9545024797028185 LR 0.001 ACCURACY 0.3403698985115625\n",
      "LOSS after epoch 1100 1.7588855519014246 LR 0.001 ACCURACY 0.3494260210893117\n",
      "LOSS after epoch 1110 1.8226479923023897 LR 0.001 ACCURACY 0.3656887765740976\n",
      "LOSS after epoch 1120 1.8089171764897365 LR 0.001 ACCURACY 0.3641900516115129\n",
      "LOSS after epoch 1130 1.887013603659237 LR 0.001 ACCURACY 0.3634885214269161\n",
      "LOSS after epoch 1140 1.9509295295266544 LR 0.001 ACCURACY 0.360778063046746\n",
      "LOSS after epoch 1150 1.8105613858092064 LR 0.001 ACCURACY 0.3440369911957532\n",
      "LOSS after epoch 1160 1.944793701171875 LR 0.001 ACCURACY 0.3674107153667137\n",
      "LOSS after epoch 1170 1.7527607936485141 LR 0.001 ACCURACY 0.36234056219691413\n",
      "LOSS after epoch 1180 1.7465109731636794 LR 0.001 ACCURACY 0.3836096952436492\n",
      "LOSS after epoch 1190 1.716342851227405 LR 0.001 ACCURACY 0.36454081697156654\n",
      "LOSS after epoch 1200 1.9257431030273438 LR 0.001 ACCURACY 0.36686862334609033\n",
      "LOSS after epoch 1210 1.864172991584329 LR 0.001 ACCURACY 0.35765306195244195\n",
      "LOSS after epoch 1220 1.8673212387982536 LR 0.001 ACCURACY 0.36913265343755486\n",
      "LOSS after epoch 1230 1.7534719728956036 LR 0.001 ACCURACY 0.3559948984649964\n",
      "LOSS after epoch 1240 1.7605825685987286 LR 0.001 ACCURACY 0.3728635209845379\n",
      "LOSS after epoch 1250 1.9251888499540442 LR 0.001 ACCURACY 0.35947066453518345\n",
      "LOSS after epoch 1260 1.8505229575961244 LR 0.001 ACCURACY 0.3665816336288117\n",
      "LOSS after epoch 1270 1.878113540948606 LR 0.001 ACCURACY 0.36479591896058994\n",
      "LOSS after epoch 1280 1.8818425197227329 LR 0.001 ACCURACY 0.34477040972560646\n",
      "LOSS after epoch 1290 1.7485249837239583 LR 0.001 ACCURACY 0.3896683685714379\n",
      "LOSS after epoch 1300 1.7759698606004901 LR 0.001 ACCURACY 0.3753826536005363\n",
      "LOSS after epoch 1310 1.7166601442823224 LR 0.001 ACCURACY 0.36415816474473106\n",
      "LOSS after epoch 1320 1.8458678301642923 LR 0.001 ACCURACY 0.35688775619491936\n",
      "LOSS after epoch 1330 1.737627665201823 LR 0.001 ACCURACY 0.3677614802843891\n",
      "LOSS after epoch 1340 1.7395670273724724 LR 0.001 ACCURACY 0.3604272966971621\n",
      "LOSS after epoch 1350 1.9210873772116268 LR 0.001 ACCURACY 0.37088648048229517\n",
      "LOSS after epoch 1360 1.5927527932559742 LR 0.001 ACCURACY 0.37152423582738264\n",
      "LOSS after epoch 1370 1.7140440099379595 LR 0.001 ACCURACY 0.3766581636737101\n",
      "LOSS after epoch 1380 1.7452088898303462 LR 0.001 ACCURACY 0.37327806211542336\n",
      "LOSS after epoch 1390 1.8261072495404411 LR 0.001 ACCURACY 0.36240433843340725\n",
      "LOSS after epoch 1400 1.5524369782092524 LR 0.001 ACCURACY 0.381632654205896\n",
      "LOSS after epoch 1410 1.8019184785730697 LR 0.001 ACCURACY 0.3671875005820766\n",
      "LOSS after epoch 1420 1.750340929218367 LR 0.001 ACCURACY 0.3679209200665355\n",
      "LOSS after epoch 1430 1.775977190803079 LR 0.001 ACCURACY 0.3834502559737302\n",
      "LOSS after epoch 1440 1.8387427236519607 LR 0.001 ACCURACY 0.3682716849958524\n",
      "LOSS after epoch 1450 1.7281340056774663 LR 0.001 ACCURACY 0.37063137865625323\n",
      "LOSS after epoch 1460 1.7093015184589462 LR 0.001 ACCURACY 0.3698660719790496\n",
      "LOSS after epoch 1470 1.7952952665441178 LR 0.001 ACCURACY 0.3806760207051411\n",
      "LOSS after epoch 1480 1.6006423351811427 LR 0.001 ACCURACY 0.38287627685349435\n",
      "LOSS after epoch 1490 1.8627780091528798 LR 0.001 ACCURACY 0.3883609712123871\n",
      "LOSS after epoch 1500 1.8482253130744486 LR 0.001 ACCURACY 0.3586415824829601\n",
      "LOSS after epoch 1510 1.8315311506682752 LR 0.001 ACCURACY 0.3793048481224105\n",
      "LOSS after epoch 1520 1.7704959944182752 LR 0.001 ACCURACY 0.3637436232878827\n",
      "LOSS after epoch 1530 1.9123906154258579 LR 0.001 ACCURACY 0.36218112288042903\n",
      "LOSS after epoch 1540 1.5713002821978401 LR 0.001 ACCURACY 0.3752551028504968\n",
      "LOSS after epoch 1550 1.9001150692210478 LR 0.001 ACCURACY 0.37850765356561167\n",
      "LOSS after epoch 1560 1.7509218103745405 LR 0.001 ACCURACY 0.37436224615667013\n",
      "LOSS after epoch 1570 1.730733086081112 LR 0.001 ACCURACY 0.3870535725960508\n",
      "LOSS after epoch 1580 1.720970453000536 LR 0.001 ACCURACY 0.3716198988608085\n",
      "LOSS after epoch 1590 1.8426880182004441 LR 0.001 ACCURACY 0.3556441338034347\n",
      "LOSS after epoch 1600 1.759305617388557 LR 0.001 ACCURACY 0.3845344404224306\n",
      "LOSS after epoch 1610 1.7923759011661304 LR 0.001 ACCURACY 0.3631058683502488\n",
      "LOSS after epoch 1620 1.7510045369466145 LR 0.001 ACCURACY 0.3777423472609371\n",
      "LOSS after epoch 1630 1.7567013011259192 LR 0.001 ACCURACY 0.3794323983020149\n",
      "LOSS after epoch 1640 1.6188501096239276 LR 0.001 ACCURACY 0.40102040958590807\n",
      "LOSS after epoch 1650 1.684931287578508 LR 0.001 ACCURACY 0.3938137762423139\n",
      "LOSS after epoch 1660 1.6403344846239276 LR 0.001 ACCURACY 0.37595663344254715\n",
      "LOSS after epoch 1670 1.5904623293409161 LR 0.001 ACCURACY 0.3754783172393218\n",
      "LOSS after epoch 1680 1.890377867455576 LR 0.001 ACCURACY 0.3848852048628032\n",
      "LOSS after epoch 1690 1.5577479343788296 LR 0.001 ACCURACY 0.3868303580209613\n",
      "LOSS after epoch 1700 1.6508492862477022 LR 0.001 ACCURACY 0.36846301110927016\n",
      "LOSS after epoch 1710 1.7156716141046262 LR 0.001 ACCURACY 0.372959185030777\n",
      "LOSS after epoch 1720 1.8650366091260724 LR 0.001 ACCURACY 0.37707270464161413\n",
      "LOSS after epoch 1730 1.7046305338541667 LR 0.001 ACCURACY 0.37895408291369675\n",
      "LOSS after epoch 1740 1.5321899114870559 LR 0.001 ACCURACY 0.38641581691335886\n",
      "LOSS after epoch 1750 1.7341900993795956 LR 0.001 ACCURACY 0.3867665818776004\n",
      "LOSS after epoch 1760 1.6625005684646905 LR 0.001 ACCURACY 0.3820790823432617\n",
      "LOSS after epoch 1770 1.8121037202722885 LR 0.001 ACCURACY 0.38121811326127497\n",
      "LOSS after epoch 1780 1.7776380052753524 LR 0.001 ACCURACY 0.3691326541826129\n",
      "LOSS after epoch 1790 1.6628651338465072 LR 0.001 ACCURACY 0.39512117428239435\n",
      "LOSS after epoch 1800 1.6779493444106157 LR 0.001 ACCURACY 0.40031887870281935\n",
      "LOSS after epoch 1810 1.5238470937691482 LR 0.001 ACCURACY 0.37920918499585243\n",
      "LOSS after epoch 1820 1.6723104738721661 LR 0.001 ACCURACY 0.4001913266722113\n",
      "LOSS after epoch 1830 1.8328634523877911 LR 0.001 ACCURACY 0.3726084190304391\n",
      "LOSS after epoch 1840 1.8717042511584712 LR 0.001 ACCURACY 0.3788265316281468\n",
      "LOSS after epoch 1850 1.77336898504519 LR 0.001 ACCURACY 0.3854591840878129\n",
      "LOSS after epoch 1860 1.767692865109911 LR 0.001 ACCURACY 0.3674426022800617\n",
      "LOSS after epoch 1870 1.924534517176011 LR 0.001 ACCURACY 0.3786670922418125\n",
      "LOSS after epoch 1880 1.7153127333697151 LR 0.001 ACCURACY 0.3845982153317891\n",
      "LOSS after epoch 1890 1.7015793744255514 LR 0.001 ACCURACY 0.3839923487743363\n",
      "LOSS after epoch 1900 1.6638394524069393 LR 0.001 ACCURACY 0.3922512763878331\n",
      "LOSS after epoch 1910 1.8574736352060355 LR 0.001 ACCURACY 0.391230868070852\n",
      "LOSS after epoch 1920 1.727382734710095 LR 0.001 ACCURACY 0.38762755224015566\n",
      "LOSS after epoch 1930 1.805557699764476 LR 0.001 ACCURACY 0.3739158166805282\n",
      "LOSS after epoch 1940 1.826988369810815 LR 0.001 ACCURACY 0.37767857185099274\n",
      "LOSS after epoch 1950 1.870889850691253 LR 0.001 ACCURACY 0.37264030715450647\n",
      "LOSS after epoch 1960 1.7631889792049633 LR 0.001 ACCURACY 0.38606505242642014\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(50000):    \n",
    "    # Get a batch of training data\n",
    "    input_seqs, coordinates, target_seqs = dataset[0]\n",
    "    positions_coords = seq2seq_lstm.get_coordinate_encoding(coordinates, max_length=max_length, d=embedding_dim, device=device)\n",
    "    positions_targets = seq2seq_lstm.get_position_encoding(max_length, embedding_dim, device=device).repeat(batch_size, 1, 1)\n",
    "    \n",
    "    # Set gradients of all model parameters to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "    accuracy = 0.0\n",
    "\n",
    "    #####################\n",
    "    #    TRANSFORMER    #\n",
    "    #####################\n",
    "    \n",
    "    # Run the input sequences through the model\n",
    "    output = model(input_seqs[:, :-1], target_seqs[:, :-1], positions_coords[:, :-1])\n",
    "    \n",
    "    # Iterate over sequence positions to compute the loss\n",
    "    for i in range(max_length-1):\n",
    "        # Get the predicted classes of the model\n",
    "        topv, topi = output[:, i, :].topk(1)\n",
    "        loss += criterion(output[:, i, :], target_seqs[:, i+1])\n",
    "        accuracy += float((topi.squeeze() == target_seqs[:, i+1]).sum() / (target_seqs.size(0)*(target_seqs.size(1)-2)))\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print_every = 10\n",
    "    if not epoch % print_every:\n",
    "        _accuracy = sum(accuracies[-print_every:]) / print_every\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"LOSS after epoch {epoch}\", loss.item() / (target_seqs.size(1)), \"LR\", lr, \"ACCURACY\", _accuracy)\n",
    "\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    accuracy = 0.0\n",
    "\n",
    "    # Update weights of encoder and decoder\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e3605-8f59-42ee-b260-5bb500518d00",
   "metadata": {},
   "source": [
    "#### Save model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67bec2-ccfd-4492-bd1a-e51fad94fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "model_data = {\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"heads\": heads,\n",
    "    \"dropout\": dropout,\n",
    "}\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"heads\": heads,\n",
    "    \"dropout\": dropout,\n",
    "}, \"transformer_\" + date_time + \".pt\")\n",
    "\n",
    "\n",
    "with open(\"training_\" + date_time + '.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56c813-13d3-45a2-b9e1-47fed03bcf3c",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "We run our input sequences through the model and get output seuences. Then we decode the output sequences with the Vocabulary class and get our final latex code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb8ff7-8d9a-4c28-8f95-264703cfe636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_seqs, coordinates, target_seqs):\n",
    "    vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "    vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        coordinate_encoding = seq2seq_lstm.get_coordinate_encoding(coordinates, d=embedding_dim, max_length=max_length)\n",
    "        output = model(input_seqs, target_seqs, coordinate_encoding)\n",
    "        # Get the predicted classes of the model\n",
    "        topv, topi = output.topk(1, dim=2)\n",
    "        \n",
    "        return topi.squeeze()\n",
    "    \n",
    "def predict_sequentially(input_seqs, coordinates):\n",
    "    prediction = torch.zeros((input_seqs.size(0), input_seqs.size(1)-1)).to(torch.int64)\n",
    "    for i in range(max_length-1):\n",
    "        output = predict(input_seqs, coordinates, prediction)\n",
    "        prediction[:, i] = output[:, i]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4224ca-ab02-437b-a0ea-5704b1c01daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict_sequentially(input_seqs, coordinates)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf18d6-94da-42ce-8bad-86a9f2d16d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random sequence and its prediction from the model\n",
    "import random\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "predictions = predict(input_seqs, coordinates, target_seqs)\n",
    "\n",
    "i = random.randint(0, predictions.size(0)-1)\n",
    "print(\"MODEL INPUT\", vocab_in.decode_sequence(input_seqs[i, 1:].cpu().numpy()))\n",
    "print(\"MODEL OUTPUT\", vocab_out.decode_sequence(predictions[i, :-1].cpu().numpy()))\n",
    "print(\"TARGET OUTPUT\", vocab_out.decode_sequence(target_seqs[i, 1:].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeae19b-77d4-4715-8068-9822af23009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = vocab_out.decode_sequence(predictions[i].cpu().numpy())\n",
    "prediction = list(filter(lambda x: x != '<end>', prediction))\n",
    "prediction = \"\".join(prediction)\n",
    "print(\"MODEL OUTPUT\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4141cd30-1b5f-4a10-b844-08b2cbb28576",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sequentially(input_seqs[0:3], coordinates[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e1244-c9b2-4f1e-8e7c-58ad12ccd273",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seqs[0:3, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6130ca-cb78-49d1-b167-b6d0817afbbc",
   "metadata": {},
   "source": [
    "## Prediction for permutated sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472c7a7-3c27-46d4-ba18-e4cbf9d52204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_permutated_batch(input_seq, coordinates):\n",
    "    seqs = torch.zeros((5, input_seq.size(0))).to(torch.int64)\n",
    "    coords = torch.zeros((5, coordinates.size(0), coordinates.size(1)))\n",
    "    for i in range(5):\n",
    "        idx_permutated = permutate_tokens(input_seq)\n",
    "        seqs[i, :] = input_seq[idx_permutated]\n",
    "        coords[i, :] = coordinates[idx_permutated]\n",
    "    return seqs, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a272c96c-9ac8-4ea7-9f84-03c28c81db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_permutated, coords_permutated = generate_permutated_batch(input_seqs[0], coordinates[0])\n",
    "input_permutated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571029b-a0e2-4574-a70e-f02ca965611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sequentially(input_permutated, coords_permutated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c5daf2-f175-4e54-bdb0-2f9c1844b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seqs[0, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983b254-e462-48ce-bc7e-94c2fba39a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
