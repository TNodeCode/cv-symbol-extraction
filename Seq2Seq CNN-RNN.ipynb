{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e370c-40cb-4194-bf97-ad9e9ab745be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seqgen.seq_gen as g\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from seqgen.model import rnn, embedding, attention, cnn\n",
    "from seqgen.vocabulary import *\n",
    "from seqgen.preprocess import *\n",
    "from seqgen.dataset import *\n",
    "from seqgen.visualize import *\n",
    "from seqgen.datasets.sequences import *\n",
    "from seqgen.datasets.realdata import RealSequencesDataset\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197faa56-4d2f-4453-bbda-32c5b4115b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccbe91c-6286-4398-ad71-ee251ae1af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "\n",
    "cell_type=rnn.CellType.LSTM\n",
    "encoder_embedding_type=embedding.EmbeddingType.NONE\n",
    "decoder_embedding_type=embedding.EmbeddingType.POS_SUBSPACE\n",
    "attention_type=attention.AttentionType.DOT\n",
    "\n",
    "use_real_dataset=False\n",
    "num_layers=3\n",
    "embedding_dim=64\n",
    "hidden_size=64\n",
    "batch_size=8\n",
    "max_length=50\n",
    "bidirectional=True\n",
    "\n",
    "img_width=1120\n",
    "img_height=224\n",
    "patch_size=224\n",
    "n_patches_dim0, n_patches_dim1 = img_height // patch_size, img_width // patch_size\n",
    "n_patches = n_patches_dim0*n_patches_dim1\n",
    "img_dir = \"dataset-generation/archive/latex_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d88d5-47ef-4b8f-9d41-e1d8fb70c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "if use_real_dataset:\n",
    "    dataset = RealSequencesDataset(filename=\"data/train/label.txt\", vocab_in=vocab_in, vocab_out=vocab_out, max_length=max_length-2, batch_size=batch_size, device=device)\n",
    "else:\n",
    "    dataset = SyntheticSequenceDataset(vocab_in, vocab_out, max_length-1, batch_size, continue_prob=0.95, additional_eos=True, device=device)\n",
    "    \n",
    "positions = torch.arange(max_length).repeat(batch_size, 1).to(device)\n",
    "\n",
    "input_seqs, coordinates, target_seqs = dataset[0]\n",
    "coordinates[:, :, [0,2]] *= img_width\n",
    "coordinates[:, :, [1,3]] *= img_height\n",
    "input_seqs.shape, coordinates.shape, target_seqs.shape, positions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724f0f9-12cb-494b-86d9-c8c9a9286f8c",
   "metadata": {},
   "source": [
    "## Create Images from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b819f361-f176-4975-8d8b-07f551f6f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_samples = get_class_samples(img_dir)\n",
    "images = create_images_from_input_seqs(input_seqs, coordinates, batch_size, img_height, img_width, class_samples, vocab_in, max_length, img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafbe96d-87da-430d-aea7-495c9e9d3303",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(min(batch_size, 2), figsize=(10, 4))\n",
    "\n",
    "for i in range(min(batch_size, 2)):\n",
    "    ax[i].imshow(create_cv2img_array_from_coordinates(input_seqs[i], coordinates[i], img_height, img_width, class_samples, vocab_in, max_length, img_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476339a-9ad8-4dd1-b396-95434dd5cba4",
   "metadata": {},
   "source": [
    "### Image patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c64f10a-6041-4768-802f-67da47f56f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = torch.tensor(images)\n",
    "image_patches = create_image_patches(image_tensor, patch_size=patch_size, flatten_patches=False)\n",
    "image_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24c6e7-0fd2-45ef-8175-fb680f3d0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 2\n",
    "fix, ax = plt.subplots(n_patches_dim0*n_batches, n_patches_dim1, figsize=(20,8))\n",
    "\n",
    "for b in range(n_batches):\n",
    "    for i in range(n_patches_dim0):\n",
    "        for j in range(n_patches_dim1):\n",
    "            idx = i*n_patches_dim1+j\n",
    "            if n_patches_dim0*n_batches > 1:\n",
    "                ax[b*n_patches_dim0+i, j].imshow(image_patches[b][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d369719-5647-41ac-bbd0-166d4a01d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = cnn.EncoderCNN(embedding_dim=embedding_dim)\n",
    "\n",
    "rgb_image_patches = image_patches.unsqueeze(2)\n",
    "rgb_image_patches = torch.cat([rgb_image_patches,rgb_image_patches,rgb_image_patches], dim=2)\n",
    "rgb_image_patches.shape\n",
    "input_patches = model_cnn(rgb_image_patches.to(torch.float32).reshape(batch_size*n_patches_dim0*n_patches_dim1, 3, 224, 224)).reshape(batch_size, n_patches, embedding_dim).to(device)\n",
    "input_patches.shape, input_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de52b92-76f8-4775-8ef5-02508053a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input_images(input_seqs, coordinates):\n",
    "    images = torch.tensor(create_images_from_input_seqs(input_seqs, coordinates, batch_size, img_height, img_width, class_samples, vocab_in, max_length, img_dir))\n",
    "    image_patches = create_image_patches(image_tensor, patch_size=patch_size, flatten_patches=False)\n",
    "    rgb_image_patches = image_patches.unsqueeze(2)\n",
    "    rgb_image_patches = torch.cat([rgb_image_patches,rgb_image_patches,rgb_image_patches], dim=2)\n",
    "    input_patches = model_cnn(rgb_image_patches.to(torch.float32).reshape(batch_size*n_patches_dim0*n_patches_dim1, 3, 224, 224)).reshape(batch_size, n_patches, embedding_dim)\n",
    "    return input_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae052b8-9e34-45cc-b358-82a2844237dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = encode_input_images(input_seqs, coordinates)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae84cc-67fe-4487-b119-51c1e6563d7c",
   "metadata": {},
   "source": [
    "# The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7974696-6918-47ff-93b7-14cc4517dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint = False\n",
    "checkpoint_file = \"model_2023-01-15_09-17-53.pt\"\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "encoder = rnn.RecurrentEncoder(\n",
    "    cell_type=cell_type,\n",
    "    embedding_type=encoder_embedding_type,\n",
    "    vocab_size=len(vocab_in),\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    max_length=max_length,\n",
    "    num_layers=num_layers,\n",
    "    dropout=0.1,\n",
    "    bidirectional=bidirectional,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "decoder = rnn.RecurrentAttentionDecoder(\n",
    "    cell_type=cell_type,\n",
    "    embedding_type=decoder_embedding_type,\n",
    "    attention_type=attention_type,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=len(vocab_out),\n",
    "    max_length=max_length,\n",
    "    batch_size=batch_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=0.1,\n",
    "    bidirectional=bidirectional,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Initialize optimizer for encoder and decoder\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Load model weights from checkpoint\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    cell_type=checkpoint['cell_type']\n",
    "    attention_type=checkpoint['attention_type']\n",
    "    encoder_embedding_type=checkpoint['encoder_embedding_type']\n",
    "    decoder_embedding_typecheckpoint['decoder_embedding_type']\n",
    "    encoder.load_state_dict(checkpoint['encoder_model_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_model_state_dict'])\n",
    "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "    num_layers = checkpoint['num_layers']\n",
    "    embedding_dim = checkpoint['embedding_dim']\n",
    "    hidden_size = checkpoint['hidden_size']\n",
    "    bidirectional = checkpoint['bidirectional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d63a2d8-bced-44ec-b32b-19d5f3055885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the encoder hidden state and cell state with zeros\n",
    "hn = encoder.initHidden(input_seqs.shape[0], device=dataset.device)\n",
    "cn = encoder.initHidden(input_seqs.shape[0], device=dataset.device)\n",
    "hidden = (hn, cn) if cell_type == rnn.CellType.LSTM else hn\n",
    "\n",
    "_hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "\n",
    "# Iterate over the sequence words and run every word through the encoder\n",
    "for i in range(input_patches.shape[1]):\n",
    "    # Run the i-th word of the input sequence through the encoder.\n",
    "    # As a result we will get the prediction (output), the hidden state and the cell state.\n",
    "    # The hidden state and cell state will be used as inputs in the next round\n",
    "    print(f\"Run patch {i+1} of all {input_patches.shape[0]} images through the encoder\")\n",
    "    output, hidden = encoder(\n",
    "        input_patches[:, i, :],\n",
    "        coordinates[:, i],\n",
    "        hidden\n",
    "    )\n",
    "    encoder_outputs[:, i:i+1, :] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3c2a2-1b0a-404a-9d8c-0ae7fbc8485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape, hn.shape, cn.shape, encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29054ba-e5b3-405f-bc5b-149f8862da81",
   "metadata": {},
   "source": [
    "# The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda91121-2c2d-4531-b7ee-d67096698123",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "\n",
    "# Iterate over words of target sequence and run words through the decoder.\n",
    "# This will produce a prediction for the next word in the sequence\n",
    "for i in range(0, target_seqs.size(1)):\n",
    "    print(f\"Run word {i+1} through decoder\", hn[0].shape if cell_type == rnn.CellType.LSTM else hn.shape, encoder_hidden_states.shape)\n",
    "    output, hidden, attn = decoder(\n",
    "        x=target_seqs[:, i].unsqueeze(dim=1),\n",
    "        positions=positions[:, i:i+1],\n",
    "        annotations=encoder_outputs,\n",
    "        hidden=hidden\n",
    "    )\n",
    "    loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "\n",
    "print(\"LOSS\", loss.item() / max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c59e1-3885-4a12-8696-9fcf3da9cf9e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f9ad9-7ae1-4b49-99b8-490eacfd5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(10000):\n",
    "    # With a certain chance present the model the true predictions\n",
    "    # instead of its own predictions in the next iteration\n",
    "    use_teacher_forcing_prob = 0.5\n",
    "    use_teacher_forcing = random.random() < use_teacher_forcing_prob\n",
    "    \n",
    "    # Get a batch of trianing data\n",
    "    input_seqs, coordinates, target_seqs = dataset[0]\n",
    "    input_patches = encode_input_images(input_seqs, coordinates).to(input_seqs.device)\n",
    "\n",
    "    # Initialize the encoder hidden state and cell state with zeros\n",
    "    hn = encoder.initHidden(input_seqs.shape[0], device=dataset.device)\n",
    "    cn = encoder.initHidden(input_seqs.shape[0], device=dataset.device)\n",
    "    hidden = (hn, cn) if cell_type == rnn.CellType.LSTM else hn\n",
    "    \n",
    "    # Initialize encoder outputs tensor\n",
    "    last_n_states = 2 if bidirectional else 1\n",
    "    _hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "    encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "    encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "    \n",
    "    # Set gradients of all model parameters to zero\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "    \n",
    "    ####################\n",
    "    #     ENCODING     #\n",
    "    ####################\n",
    "\n",
    "    # Iterate over the sequence words and run every word through the encoder\n",
    "    for i in range(input_patches.shape[1]):\n",
    "        # Run the i-th word of the input sequence through the encoder.\n",
    "        # As a result we will get the prediction (output), the hidden state (hn).\n",
    "        # The hidden state and cell state will be used as inputs in the next round\n",
    "        output, hidden = encoder(\n",
    "            x=input_patches[:, i],\n",
    "            coordinates=coordinates[:, i],\n",
    "            hidden=hidden\n",
    "        )\n",
    "        # Save encoder outputs and states for current word\n",
    "        encoder_outputs[:, i:i+1, :] = output\n",
    "        encoder_hidden_states[:, i, :] = rnn.concat_hidden_states(hn)\n",
    "\n",
    "    ####################\n",
    "    #     DECODING     #\n",
    "    ####################\n",
    "    \n",
    "    accuracy = 0.0\n",
    "\n",
    "    # The first words that we be presented to the model is the '<start>' token\n",
    "    prediction = target_seqs[:, 0]\n",
    "    \n",
    "    # Iterate over words of target sequence and run words through the decoder.\n",
    "    # This will produce a prediction for the next word in the sequence\n",
    "    for i in range(1, target_seqs.size(1)):\n",
    "        # Run word i through decoder and get word i+1 and the new hidden state as outputs\n",
    "        if use_teacher_forcing:\n",
    "            output, hidden, _ = decoder(\n",
    "                x=target_seqs[:, i-1].unsqueeze(dim=1),\n",
    "                positions=positions[:, i-1:i],\n",
    "                annotations=encoder_outputs,\n",
    "                hidden=hidden\n",
    "            )\n",
    "            # Get the predicted classes of the model\n",
    "            topv, topi = output.topk(1)\n",
    "        else:\n",
    "            output, hidden, _ = decoder(\n",
    "                x=prediction.unsqueeze(dim=1),\n",
    "                positions=positions[:, i-1:i],\n",
    "                annotations=encoder_outputs,\n",
    "                hidden=hidden\n",
    "            )\n",
    "            # Get the predicted classes of the model\n",
    "            topv, topi = output.topk(1)\n",
    "            prediction = topi.squeeze()    \n",
    "        loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "        accuracy += float((topi.squeeze() == target_seqs[:, i]).sum() / (target_seqs.size(0)*(target_seqs.size(1)-1)))\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print_every = 10\n",
    "    if not epoch % print_every:\n",
    "        _accuracy = sum(accuracies[-print_every:]) / print_every\n",
    "        print(f\"LOSS after epoch {epoch}\", loss.item() / (target_seqs.size(1)), \"ACCURACY\", _accuracy)\n",
    "\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    accuracy = 0.0\n",
    "\n",
    "    # Update weights of encoder and decoder\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e3605-8f59-42ee-b260-5bb500518d00",
   "metadata": {},
   "source": [
    "#### Save model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67bec2-ccfd-4492-bd1a-e51fad94fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "model_data = {\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"cell_type\": cell_type,\n",
    "    \"encoder_embedding_type\": encoder_embedding_type,\n",
    "    \"decoder_embedding_type\": decoder_embedding_type,\n",
    "    \"attention_type\": attention_type,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length\n",
    "}\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f\"{cell_type}_{num_layers}layers_encemb-{encoder_embedding_type}_decemb-{decoder_embedding_type}_attn-{attention_type}\"\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'encoder_model_state_dict': encoder.state_dict(),\n",
    "    'decoder_model_state_dict': decoder.state_dict(),\n",
    "    'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "    'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"cell_type\": cell_type,\n",
    "    \"encoder_embedding_type\": encoder_embedding_type,\n",
    "    \"decoder_embedding_type\": decoder_embedding_type,\n",
    "    \"attention_type\": attention_type,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"bidirectional\": bidirectional,\n",
    "}, filename + \".pt\")\n",
    "\n",
    "\n",
    "with open(filename + '.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "    \n",
    "print(str(date_time), \"Saved model: \" + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56c813-13d3-45a2-b9e1-47fed03bcf3c",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "We run our input sequences through the model and get output seuences. Then we decode the output sequences with the Vocabulary class and get our final latex code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb8ff7-8d9a-4c28-8f95-264703cfe636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_seqs, coordinates, target_seqs):\n",
    "    vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "    vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "    predictions = torch.zeros(target_seqs.shape)\n",
    "    attention_matrix = torch.zeros((input_seqs.shape[0], input_seqs.shape[1], input_seqs.shape[1]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize the encoder hidden state and cell state with zeros\n",
    "        hn = encoder.initHidden(input_seqs.shape[0], device=dataset.device)\n",
    "        \n",
    "        # Initialize the encoder hidden state and cell state with \n",
    "        last_n_states = 2 if bidirectional else 1\n",
    "        _hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "        encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "\n",
    "        # Iterate over the sequence words and run every word through the encoder\n",
    "        for i in range(input_seqs.size(1)):\n",
    "            output, hn = encoder(\n",
    "                input_seqs[:, i].unsqueeze(dim=1),\n",
    "                coordinates[:, i],\n",
    "                hn\n",
    "            )\n",
    "            encoder_outputs[:, i:i+1, :] = output\n",
    "            encoder_hidden_states[:, i, :] = rnn.concat_hidden_states(hn)\n",
    "        \n",
    "        # Predict tokens of the target sequence by running the hidden state through\n",
    "        # the decoder\n",
    "        for i in range(0, target_seqs.size(1)):\n",
    "            output, hn, attention = decoder(\n",
    "                x=target_seqs[:, i-1].unsqueeze(dim=1),\n",
    "                coordinates=coordinates[:, i-1],\n",
    "                annotations=encoder_outputs,\n",
    "                hidden=hn\n",
    "            )\n",
    "            # Select the indices of the most likely tokens\n",
    "            predicted_char = torch.argmax(output, dim=1)\n",
    "            predictions[:, i] = torch.argmax(output, dim=1).squeeze()\n",
    "            attention_matrix[:, :, i:i+1] = attention\n",
    "        \n",
    "        return predictions, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4224ca-ab02-437b-a0ea-5704b1c01daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, attention_matrix = predict(input_seqs, coordinates, target_seqs)\n",
    "prediction.shape, attention_matrix.shape, input_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6896f-5818-4718-a584-0aa59e9226de",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, prediction.size(0)-1)\n",
    "seq_in = vocab_in.decode_sequence(input_seqs[idx].cpu().numpy())\n",
    "seq_out = vocab_out.decode_sequence(predictions[idx].cpu().numpy())\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.matshow(attention_matrix[idx], cmap='bone')\n",
    "ax.set_xticklabels([seq_out[j] for j in range(prediction.size(1))], rotation=45)\n",
    "ax.set_yticklabels([seq_in[j] for j in range(prediction.size(1))])\n",
    "#ax.tick_params(labelsize=15)\n",
    "ax.set(xlabel='Output Sequence', ylabel='Input Sequence')\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(prediction.size(1)))\n",
    "ax.yaxis.set_major_locator(plt.MaxNLocator(prediction.size(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476c3b2-d8ad-4506-8ace-814d9e9eefcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_swapped = g.random_swap(input_seqs[0], i=2).unsqueeze(dim=0)\n",
    "coords_swapped = g.random_swap(coordinates[0], i=2).unsqueeze(dim=0)\n",
    "prediction_swapped = predict(in_swapped, coords_swapped, target_seqs[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0408b19-3288-48ac-905b-88eb6ef4017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs[0:1] == in_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb4a03-69bd-45e1-960f-244da1c18417",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction == prediction_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf18d6-94da-42ce-8bad-86a9f2d16d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random sequence and its prediction from the model\n",
    "import random\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "predictions, attention_matrix = predict(input_seqs, coordinates, target_seqs)\n",
    "\n",
    "i = random.randint(0, predictions.size(0))\n",
    "print(\"MODEL INPUT\", vocab_in.decode_sequence(input_seqs[i].cpu().numpy()))\n",
    "print(\"MODEL OUTPUT\", vocab_out.decode_sequence(predictions[i].cpu().numpy()))\n",
    "print(\"TARGET OUTPUT\", vocab_out.decode_sequence(target_seqs[i][1:].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeae19b-77d4-4715-8068-9822af23009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = vocab_out.decode_sequence(predictions[i].cpu().numpy())\n",
    "prediction = list(filter(lambda x: x != '<end>', prediction))\n",
    "prediction = \"\".join(prediction)\n",
    "print(\"MODEL OUTPUT\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05457f9c-56b4-4aaa-ae20-c4e8b120ad62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
