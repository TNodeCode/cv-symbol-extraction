{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307e370c-40cb-4194-bf97-ad9e9ab745be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seqgen.seq_gen as g\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from seqgen.model import seq2seq_lstm\n",
    "from seqgen.vocabulary import *\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "197faa56-4d2f-4453-bbda-32c5b4115b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:83: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bccbe91c-6286-4398-ad71-ee251ae1af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "num_layers=1\n",
    "embedding_dim=16\n",
    "hidden_size=16\n",
    "batch_size=32\n",
    "max_length=10\n",
    "positional_encoding=True\n",
    "bidirectional=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a78d88d5-47ef-4b8f-9d41-e1d8fb70c635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilof\\AppData\\Local\\Temp\\ipykernel_19716\\1347484698.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
      "C:\\Users\\tilof\\AppData\\Local\\Temp\\ipykernel_19716\\1347484698.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coordinates = torch.tensor(features[:, :, 1:])\n"
     ]
    }
   ],
   "source": [
    "features, target_seqs = g.generate_synthetic_training_data(batch_size, max_length=max_length, device=device, continue_prob=0.999, swap_times=10)\n",
    "input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
    "coordinates = torch.tensor(features[:, :, 1:])\n",
    "positions_coords = seq2seq_lstm.get_coordinate_encoding(coordinates, max_length=max_length, d=embedding_dim, device=device)\n",
    "positions_targets = seq2seq_lstm.get_position_encoding(max_length, embedding_dim, device=device).repeat(batch_size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "437f41f6-057b-41e5-9d52-1744666d4aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 10, 5]),\n",
       " torch.Size([32, 10]),\n",
       " torch.Size([32, 10, 4]),\n",
       " torch.Size([32, 10]),\n",
       " torch.Size([32, 10, 16]),\n",
       " torch.Size([32, 10, 16]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, input_seqs.shape, coordinates.shape, target_seqs.shape, positions_coords.shape, positions_targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae84cc-67fe-4487-b119-51c1e6563d7c",
   "metadata": {},
   "source": [
    "# The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7974696-6918-47ff-93b7-14cc4517dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "load_from_checkpoint = False\n",
    "checkpoint_file = \"model_2023-01-15_09-17-53.pt\"\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "encoder = seq2seq_lstm.EncoderGRUPosEnc(vocab_size=len(vocab_in), embedding_dim=embedding_dim, num_layers=num_layers, max_length=max_length, hidden_size=hidden_size, bidirectional=bidirectional, pos_encoding=positional_encoding).to(features.device)\n",
    "decoder = seq2seq_lstm.DecoderGRUAttention(embedding_dim=embedding_dim, num_layers=num_layers, max_length=max_length, hidden_size=hidden_size, vocab_size=len(vocab_out), bidirectional=bidirectional, pos_encoding=positional_encoding).to(features.device)\n",
    "\n",
    "# Initialize optimizer for encoder and decoder\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Load model weights from checkpoint\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    encoder.load_state_dict(checkpoint['encoder_model_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_model_state_dict'])\n",
    "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "    num_layers = checkpoint['num_layers']\n",
    "    embedding_dim = checkpoint['embedding_dim']\n",
    "    hidden_size = checkpoint['hidden_size']\n",
    "    bidirectional = checkpoint['bidirectional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d63a2d8-bced-44ec-b32b-19d5f3055885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run word 1 of all 32 sequences through the encoder\n",
      "Run word 2 of all 32 sequences through the encoder\n",
      "Run word 3 of all 32 sequences through the encoder\n",
      "Run word 4 of all 32 sequences through the encoder\n",
      "Run word 5 of all 32 sequences through the encoder\n",
      "Run word 6 of all 32 sequences through the encoder\n",
      "Run word 7 of all 32 sequences through the encoder\n",
      "Run word 8 of all 32 sequences through the encoder\n",
      "Run word 9 of all 32 sequences through the encoder\n",
      "Run word 10 of all 32 sequences through the encoder\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encoder hidden state and cell state with zeros\n",
    "hn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "cn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "\n",
    "_hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "\n",
    "# Iterate over the sequence words and run every word through the encoder\n",
    "for i in range(input_seqs.shape[1]):\n",
    "    # Run the i-th word of the input sequence through the encoder.\n",
    "    # As a result we will get the prediction (output), the hidden state and the cell state.\n",
    "    # The hidden state and cell state will be used as inputs in the next round\n",
    "    print(f\"Run word {i+1} of all {input_seqs.shape[0]} sequences through the encoder\")\n",
    "    output, hn = encoder(\n",
    "        input_seqs[:, i].unsqueeze(dim=1),\n",
    "        coordinates[:, i],\n",
    "        positions_coords[:, i:i+1],\n",
    "        hn\n",
    "    )\n",
    "    encoder_outputs[:, i:i+1, :] = output\n",
    "    encoder_hidden_states[:, i, :] = seq2seq_lstm.concat_hidden_states(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbb3c2a2-1b0a-404a-9d8c-0ae7fbc8485c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 32]),\n",
       " torch.Size([2, 32, 16]),\n",
       " torch.Size([2, 32, 16]),\n",
       " torch.Size([32, 10, 32]),\n",
       " torch.Size([32, 10, 32]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, hn.shape, cn.shape, encoder_hidden_states.shape, encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29054ba-e5b3-405f-bc5b-149f8862da81",
   "metadata": {},
   "source": [
    "# The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda91121-2c2d-4531-b7ee-d67096698123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run word 1 through decoder torch.Size([2, 32, 16]) torch.Size([32, 10, 32])\n",
      "Run word 2 through decoder torch.Size([2, 32, 16]) torch.Size([32, 10, 32])\n",
      "Run word 3 through decoder torch.Size([2, 32, 16]) torch.Size([32, 10, 32])\n",
      "Run word 4 through decoder torch.Size([2, 32, 16]) torch.Size([32, 10, 32])\n",
      "Run word 5 through decoder torch.Size([2, 32, 16]) torch.Size([32, 10, 32])\n",
      "Run word 6 through decoder torch.Size([2, 32, 16]) torch.Size([32, 10, 32])\n",
      "Run word 7 through decoder torch.Size([2, 32, 16]) torch.Size([32, 10, 32])\n",
      "Run word 8 through decoder torch.Size([2, 32, 16]) torch.Size([32, 10, 32])\n",
      "Run word 9 through decoder torch.Size([2, 32, 16]) torch.Size([32, 10, 32])\n",
      "Run word 10 through decoder torch.Size([2, 32, 16]) torch.Size([32, 10, 32])\n",
      "LOSS 3.2254169464111326\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "\n",
    "# Iterate over words of target sequence and run words through the decoder.\n",
    "# This will produce a prediction for the next word in the sequence\n",
    "for i in range(0, target_seqs.size(1)):\n",
    "    print(f\"Run word {i+1} through decoder\", hn.shape, encoder_hidden_states.shape)\n",
    "    output, hn, attention = decoder(\n",
    "        x=target_seqs[:, i].unsqueeze(dim=1),\n",
    "        coordinates=coordinates[:, i],\n",
    "        annotations=encoder_outputs,\n",
    "        position=positions_targets[:, i:i+1],\n",
    "        hidden=hn\n",
    "    )\n",
    "    loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "\n",
    "print(\"LOSS\", loss.item() / max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c59e1-3885-4a12-8696-9fcf3da9cf9e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f9ad9-7ae1-4b49-99b8-490eacfd5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilof\\AppData\\Local\\Temp\\ipykernel_19716\\2309256053.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
      "C:\\Users\\tilof\\AppData\\Local\\Temp\\ipykernel_19716\\2309256053.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coordinates = torch.tensor(features[:, :, 1:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS after epoch 0 2.925399398803711 ACCURACY 0.00031250000232830645\n",
      "LOSS after epoch 100 1.6385673522949218 ACCURACY 0.285520836240612\n",
      "LOSS after epoch 200 1.2141572952270507 ACCURACY 0.5580902794282884\n",
      "LOSS after epoch 300 1.1330288887023925 ACCURACY 0.6374305572733283\n",
      "LOSS after epoch 400 1.089541244506836 ACCURACY 0.6720486124418676\n",
      "LOSS after epoch 500 1.0537545204162597 ACCURACY 0.6924305563420057\n",
      "LOSS after epoch 600 1.0178559303283692 ACCURACY 0.7072222233563662\n",
      "LOSS after epoch 700 0.5338333129882813 ACCURACY 0.7455555554106832\n",
      "LOSS after epoch 800 0.8128387451171875 ACCURACY 0.7469791673123837\n",
      "LOSS after epoch 900 0.4649209499359131 ACCURACY 0.7863888899981976\n",
      "LOSS after epoch 1000 0.8619023323059082 ACCURACY 0.7694444453157484\n",
      "LOSS after epoch 1100 0.4248810768127441 ACCURACY 0.8015972237288952\n",
      "LOSS after epoch 1200 0.7515764236450195 ACCURACY 0.794965279456228\n",
      "LOSS after epoch 1300 0.48764815330505373 ACCURACY 0.8137500003352761\n",
      "LOSS after epoch 1400 0.3805121660232544 ACCURACY 0.8195833333954216\n",
      "LOSS after epoch 1500 0.6982115268707275 ACCURACY 0.8087500014528632\n",
      "LOSS after epoch 1600 0.5886683464050293 ACCURACY 0.8271875009313225\n",
      "LOSS after epoch 1700 0.6184808731079101 ACCURACY 0.8194444463029504\n",
      "LOSS after epoch 1800 0.7624595642089844 ACCURACY 0.8214930570498109\n",
      "LOSS after epoch 1900 0.656169843673706 ACCURACY 0.8255208351835609\n",
      "LOSS after epoch 2000 0.794004487991333 ACCURACY 0.8476388902589679\n",
      "LOSS after epoch 2100 0.6157352924346924 ACCURACY 0.8273611118644476\n",
      "LOSS after epoch 2200 0.7033268451690674 ACCURACY 0.8340625011175871\n",
      "LOSS after epoch 2300 0.3250804662704468 ACCURACY 0.8325000010430813\n",
      "LOSS after epoch 2400 0.6115830421447754 ACCURACY 0.8340625012665988\n",
      "LOSS after epoch 2500 0.27771103382110596 ACCURACY 0.8379166677966714\n",
      "LOSS after epoch 2600 0.25624403953552244 ACCURACY 0.8473611120507121\n",
      "LOSS after epoch 2700 0.47002511024475097 ACCURACY 0.8449652782455087\n",
      "LOSS after epoch 2800 0.5005658149719239 ACCURACY 0.8512500020489097\n",
      "LOSS after epoch 2900 0.5370802879333496 ACCURACY 0.8546527784690261\n",
      "LOSS after epoch 3000 0.5731118202209473 ACCURACY 0.8540625011920929\n",
      "LOSS after epoch 3100 0.2770202875137329 ACCURACY 0.8653819443285465\n",
      "LOSS after epoch 3200 0.45869622230529783 ACCURACY 0.8588194464147091\n",
      "LOSS after epoch 3300 0.23474278450012206 ACCURACY 0.8692013899236918\n",
      "LOSS after epoch 3400 0.2526297330856323 ACCURACY 0.8761458339169621\n",
      "LOSS after epoch 3500 0.2975959539413452 ACCURACY 0.869756944514811\n",
      "LOSS after epoch 3600 0.2616861343383789 ACCURACY 0.8658333348855376\n",
      "LOSS after epoch 3700 0.21855056285858154 ACCURACY 0.8745833348855376\n",
      "LOSS after epoch 3800 0.28822214603424073 ACCURACY 0.8729861114174128\n",
      "LOSS after epoch 3900 0.27431654930114746 ACCURACY 0.8815972229465843\n",
      "LOSS after epoch 4000 0.24467194080352783 ACCURACY 0.8755555565655232\n",
      "LOSS after epoch 4100 0.535215425491333 ACCURACY 0.8879861116036772\n",
      "LOSS after epoch 4200 0.20570616722106932 ACCURACY 0.8867013898491859\n",
      "LOSS after epoch 4300 0.21821584701538085 ACCURACY 0.8770138905569911\n",
      "LOSS after epoch 4400 0.28449239730834963 ACCURACY 0.8769097243249416\n",
      "LOSS after epoch 4500 0.4255062580108643 ACCURACY 0.875208334363997\n",
      "LOSS after epoch 4600 0.23796401023864747 ACCURACY 0.8868055563420058\n",
      "LOSS after epoch 4700 0.49638891220092773 ACCURACY 0.8927083345502616\n",
      "LOSS after epoch 4800 0.39427173137664795 ACCURACY 0.8882638903707266\n",
      "LOSS after epoch 4900 0.19096643924713136 ACCURACY 0.8738888905569911\n",
      "LOSS after epoch 5000 0.41574759483337403 ACCURACY 0.8849652792513371\n",
      "LOSS after epoch 5100 0.3910073757171631 ACCURACY 0.8886458348110318\n",
      "LOSS after epoch 5200 0.5884511470794678 ACCURACY 0.8849652787670493\n",
      "LOSS after epoch 5300 0.30336382389068606 ACCURACY 0.8879513892531395\n",
      "LOSS after epoch 5400 0.20322961807250978 ACCURACY 0.887118055485189\n",
      "LOSS after epoch 5500 0.7205067157745362 ACCURACY 0.8811111114546657\n",
      "LOSS after epoch 5600 0.2243508815765381 ACCURACY 0.8839583338052034\n",
      "LOSS after epoch 5700 0.3883713483810425 ACCURACY 0.8919097231328488\n",
      "LOSS after epoch 5800 0.5060606002807617 ACCURACY 0.8832291674241424\n",
      "LOSS after epoch 5900 0.1841457962989807 ACCURACY 0.8875347227975726\n",
      "LOSS after epoch 6000 0.2083608865737915 ACCURACY 0.8897222233191132\n",
      "LOSS after epoch 6100 0.19650323390960694 ACCURACY 0.8999305564165115\n",
      "LOSS after epoch 6200 0.4032161235809326 ACCURACY 0.8874305570870638\n",
      "LOSS after epoch 6300 0.3545694828033447 ACCURACY 0.9059722234308719\n",
      "LOSS after epoch 6400 0.4094966411590576 ACCURACY 0.8834027783572673\n",
      "LOSS after epoch 6500 0.41285390853881837 ACCURACY 0.8971527789160609\n",
      "LOSS after epoch 6600 0.23686983585357665 ACCURACY 0.8912500010430813\n",
      "LOSS after epoch 6700 0.18679256439208985 ACCURACY 0.8919791679829359\n",
      "LOSS after epoch 6800 0.39094064235687254 ACCURACY 0.895277778916061\n",
      "LOSS after epoch 6900 0.2418466329574585 ACCURACY 0.8958333341777325\n",
      "LOSS after epoch 7000 0.21076414585113526 ACCURACY 0.8929861123859882\n",
      "LOSS after epoch 7100 0.2620664358139038 ACCURACY 0.8934722230955958\n",
      "LOSS after epoch 7200 0.6631472110748291 ACCURACY 0.8920138895511627\n",
      "LOSS after epoch 7300 0.16180682182312012 ACCURACY 0.8977083336934447\n",
      "LOSS after epoch 7400 0.2619616985321045 ACCURACY 0.9033680560812354\n",
      "LOSS after epoch 7500 0.22842905521392823 ACCURACY 0.8884027783572673\n",
      "LOSS after epoch 7600 0.1787220597267151 ACCURACY 0.9028125019744039\n",
      "LOSS after epoch 7700 0.2368699789047241 ACCURACY 0.895277778916061\n",
      "LOSS after epoch 7800 0.5368048191070557 ACCURACY 0.8948611125349999\n",
      "LOSS after epoch 7900 0.13594762086868287 ACCURACY 0.8927430568635464\n",
      "LOSS after epoch 8000 0.15653084516525267 ACCURACY 0.8882638901844621\n",
      "LOSS after epoch 8100 0.1940504789352417 ACCURACY 0.9020138907805085\n",
      "LOSS after epoch 8200 0.17746661901473998 ACCURACY 0.9056944450736046\n",
      "LOSS after epoch 8300 0.2630284070968628 ACCURACY 0.9021180575340986\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(50000):\n",
    "    # With a certain chance present the model the true predictions\n",
    "    # instead of its own predictions in the next iteration\n",
    "    use_teacher_forcing_prob = 0.5\n",
    "    use_teacher_forcing = random.random() < use_teacher_forcing_prob\n",
    "    \n",
    "    # Get a batch of trianing data\n",
    "    features, target_seqs = g.generate_synthetic_training_data(batch_size, max_length=max_length, continue_prob=0.999, device=device, swap_times=max_length)\n",
    "    features = features.to(device)\n",
    "    target_seqs = target_seqs.to(device)\n",
    "    input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
    "    coordinates = torch.tensor(features[:, :, 1:])\n",
    "    positions_coords = seq2seq_lstm.get_coordinate_encoding(coordinates, max_length=max_length, d=embedding_dim, device=device)\n",
    "    positions_targets = seq2seq_lstm.get_position_encoding(max_length, embedding_dim, device=device).repeat(batch_size, 1, 1)\n",
    "\n",
    "    # Initialize the encoder hidden state and cell state with zeros\n",
    "    hn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "    \n",
    "    # Initialize encoder outputs tensor\n",
    "    last_n_states = 2 if bidirectional else 1\n",
    "    _hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "    encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "    encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "    \n",
    "    # Set gradients of all model parameters to zero\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "    \n",
    "    ####################\n",
    "    #     ENCODING     #\n",
    "    ####################\n",
    "\n",
    "    # Iterate over the sequence words and run every word through the encoder\n",
    "    for i in range(input_seqs.shape[1]):\n",
    "        # Run the i-th word of the input sequence through the encoder.\n",
    "        # As a result we will get the prediction (output), the hidden state (hn).\n",
    "        # The hidden state and cell state will be used as inputs in the next round\n",
    "        output, hn = encoder(\n",
    "            input_seqs[:, i].unsqueeze(dim=1),\n",
    "            coordinates[:, i],\n",
    "            positions_coords[:, i:i+1],\n",
    "            hn\n",
    "        )\n",
    "        # Save encoder outputs and states for current word\n",
    "        encoder_outputs[:, i:i+1, :] = output\n",
    "        encoder_hidden_states[:, i, :] = seq2seq_lstm.concat_hidden_states(hn)\n",
    "\n",
    "    ####################\n",
    "    #     DECODING     #\n",
    "    ####################\n",
    "    \n",
    "    accuracy = 0.0\n",
    "\n",
    "    # The first words that we be presented to the model is the '<start>' token\n",
    "    prediction = target_seqs[:, 0]\n",
    "    \n",
    "    # Iterate over words of target sequence and run words through the decoder.\n",
    "    # This will produce a prediction for the next word in the sequence\n",
    "    for i in range(1, target_seqs.size(1)):\n",
    "        # Run word i through decoder and get word i+1 and the new hidden state as outputs\n",
    "        if use_teacher_forcing:\n",
    "            output, hn, attention = decoder(\n",
    "                x=target_seqs[:, i-1].unsqueeze(dim=1),\n",
    "                coordinates=coordinates[:, i-1],\n",
    "                annotations=encoder_outputs,\n",
    "                position=positions_targets[:, i:i+1],\n",
    "                hidden=hn\n",
    "            )\n",
    "            # Get the predicted classes of the model\n",
    "            topv, topi = output.topk(1)\n",
    "        else:\n",
    "            output, hn, attention = decoder(\n",
    "                x=prediction.unsqueeze(dim=1),\n",
    "                coordinates=coordinates[:, i-1],\n",
    "                annotations=encoder_outputs,\n",
    "                position=positions_targets[:, i:i+1],\n",
    "                hidden=hn\n",
    "            )\n",
    "            # Get the predicted classes of the model\n",
    "            topv, topi = output.topk(1)\n",
    "            prediction = topi.squeeze()    \n",
    "        loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "        accuracy += float((topi.squeeze() == target_seqs[:, i]).sum() / (target_seqs.size(0)*(target_seqs.size(1)-1)))\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print_every = 100\n",
    "    if not epoch % print_every:\n",
    "        _accuracy = sum(accuracies[-print_every:]) / print_every\n",
    "        print(f\"LOSS after epoch {epoch}\", loss.item() / (target_seqs.size(1)), \"ACCURACY\", _accuracy)\n",
    "\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    accuracy = 0.0\n",
    "\n",
    "    # Update weights of encoder and decoder\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e3605-8f59-42ee-b260-5bb500518d00",
   "metadata": {},
   "source": [
    "#### Save model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67bec2-ccfd-4492-bd1a-e51fad94fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "model_data = {\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length\n",
    "}\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'encoder_model_state_dict': encoder.state_dict(),\n",
    "    'decoder_model_state_dict': decoder.state_dict(),\n",
    "    'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "    'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"bidirectional\": bidirectional,\n",
    "}, \"model_\" + date_time + \".pt\")\n",
    "\n",
    "\n",
    "with open(\"training_\" + date_time + '.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56c813-13d3-45a2-b9e1-47fed03bcf3c",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "We run our input sequences through the model and get output seuences. Then we decode the output sequences with the Vocabulary class and get our final latex code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb8ff7-8d9a-4c28-8f95-264703cfe636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_seqs, coordinates, target_seqs):\n",
    "    vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "    vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "    predictions = torch.zeros(target_seqs.shape)\n",
    "    attention_matrix = torch.zeros((input_seqs.shape[0], input_seqs.shape[1], input_seqs.shape[1]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize the encoder hidden state and cell state with zeros\n",
    "        hn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "        \n",
    "        # Initialize the encoder hidden state and cell state with \n",
    "        last_n_states = 2 if bidirectional else 1\n",
    "        _hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "        encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "\n",
    "        # Iterate over the sequence words and run every word through the encoder\n",
    "        for i in range(input_seqs.size(1)):\n",
    "            output, hn = encoder(\n",
    "                input_seqs[:, i].unsqueeze(dim=1),\n",
    "                coordinates[:, i],\n",
    "                positions_coords[:, i:i+1],\n",
    "                hn\n",
    "            )\n",
    "            encoder_outputs[:, i:i+1, :] = output\n",
    "            encoder_hidden_states[:, i, :] = seq2seq_lstm.concat_hidden_states(hn)\n",
    "        \n",
    "        # Predict tokens of the target sequence by running the hidden state through\n",
    "        # the decoder\n",
    "        for i in range(0, target_seqs.size(1)):\n",
    "            output, hn, attention = decoder(\n",
    "                x=target_seqs[:, i-1].unsqueeze(dim=1),\n",
    "                coordinates=coordinates[:, i-1],\n",
    "                annotations=encoder_outputs,\n",
    "                position=positions_targets[:, i:i+1],\n",
    "                hidden=hn\n",
    "            )\n",
    "            # Select the indices of the most likely tokens\n",
    "            predicted_char = torch.argmax(output, dim=1)\n",
    "            predictions[:, i] = torch.argmax(output, dim=1).squeeze()\n",
    "            attention_matrix[:, :, i:i+1] = attention\n",
    "        \n",
    "        return predictions, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4224ca-ab02-437b-a0ea-5704b1c01daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, attention_matrix = predict(input_seqs, coordinates, target_seqs)\n",
    "prediction.shape, attention_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6896f-5818-4718-a584-0aa59e9226de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(attention_matrix[random.randint(0, prediction.size(0)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476c3b2-d8ad-4506-8ace-814d9e9eefcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_swapped = g.random_swap(input_seqs[0], i=2).unsqueeze(dim=0)\n",
    "coords_swapped = g.random_swap(coordinates[0], i=2).unsqueeze(dim=0)\n",
    "prediction_swapped = predict(in_swapped, coords_swapped, target_seqs[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0408b19-3288-48ac-905b-88eb6ef4017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs[0:1] == in_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb4a03-69bd-45e1-960f-244da1c18417",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction == prediction_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf18d6-94da-42ce-8bad-86a9f2d16d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random sequence and its prediction from the model\n",
    "import random\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "predictions, attention_matrix = predict(input_seqs, coordinates, target_seqs)\n",
    "\n",
    "i = random.randint(0, predictions.size(0))\n",
    "print(\"MODEL INPUT\", vocab_in.decode_sequence(input_seqs[i].cpu().numpy()))\n",
    "print(\"MODEL OUTPUT\", vocab_out.decode_sequence(predictions[i].cpu().numpy()))\n",
    "print(\"TARGET OUTPUT\", vocab_out.decode_sequence(target_seqs[i][1:].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeae19b-77d4-4715-8068-9822af23009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = vocab_out.decode_sequence(predictions[i].cpu().numpy())\n",
    "prediction = list(filter(lambda x: x != '<end>', prediction))\n",
    "prediction = \"\".join(prediction)\n",
    "print(\"MODEL OUTPUT\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b7aff-cecf-476a-bd3c-ae737ff8aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(attention_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05457f9c-56b4-4aaa-ae20-c4e8b120ad62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
