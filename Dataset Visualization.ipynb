{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1926c2db-80e0-431a-a125-cbd31b3af4e8",
   "metadata": {},
   "source": [
    "# Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261b1b7-3709-447c-83ac-103505c0affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "from seqgen.vocabulary import *\n",
    "from seqgen.datasets.sequences import *\n",
    "from seqgen.preprocess import *\n",
    "from seqgen.seq_gen import add_noise_to_coordinates\n",
    "from seqgen.symbol_replacement import replace_symbols\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79e0f3-6501-47a7-afe0-e4ab6672052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "max_length=25\n",
    "img_width=800\n",
    "img_height=200\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "basedir = \"dataset-generation/archive/latex_images\"\n",
    "dataset = SyntheticSequenceDataset(vocab_in, vocab_out, max_length, batch_size, continue_prob=0.95, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4173085-2c34-4347-8a35-fa19ca2f5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"class_samples.pkl\"):\n",
    "    with open(\"class_samples.pkl\", \"rb\") as f:\n",
    "        class_samples = pickle.load(f)\n",
    "else:\n",
    "    classes = os.listdir(basedir)\n",
    "    class_samples = {}\n",
    "    for c in classes:\n",
    "        class_samples.update({c: []})\n",
    "        for f in os.listdir(f\"{basedir}/{c}\"):\n",
    "            if os.path.isfile(f\"{basedir}/{c}/{f}\"):\n",
    "                class_samples[c].append(f)\n",
    "\n",
    "    with open(\"class_samples.pkl\", \"wb\") as f:\n",
    "        pickle.dump(class_samples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9851c4-4897-4c21-b0cb-b3247821e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs, coordinates, target_seqs = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d985d-2f82-41b6-8607-c948d41c18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates[:, :, [0,2]] *= img_width\n",
    "coordinates[:, :, [1,3]] *= img_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec73959-82b2-4379-93e7-610c03fc6a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_symbols_to_classes(symbol):\n",
    "    #mappings = {'a':'A', 'b':'B', 'c':'C', 'COMMA':',', 'd':'D', 'e':'E', 'f':'F', 'g':'G', 'h':'H', 'i':'I', 'j':'J', 'k':'K', 'l':'L', 'm':'M', 'n':'N', 'o':'O', 'p':'P', 'phi':'phi_lower', 'prime':',', 'q':'Q', 'r':'R', 's':'S', 't':'T', 'sum':'sigma_upper', 'U':'u', 'V':'v', 'W':'w', 'x':'X', 'Y':'y', 'Z':'z', '|': 'vert', '/':'vert'}\n",
    "    mappings = {'COMMA':',', 'prime':',', 'sum':'sigma_upper', '|': 'vert', '/':'div'}\n",
    "    if symbol in mappings.keys():\n",
    "        return mappings[symbol]\n",
    "    elif symbol in string.ascii_lowercase:\n",
    "        return symbol + \"_low\"\n",
    "    elif symbol in string.ascii_uppercase:\n",
    "        return symbol.lower() + \"_up\"\n",
    "    return symbol\n",
    "\n",
    "def load_image(symbol, class_samples, erode=False):\n",
    "    n_samples = len(class_samples[symbol])\n",
    "    idx = random.randint(0, n_samples-1)\n",
    "    # Read image.\n",
    "    img = cv2.imread(f'{basedir}/{symbol}/{class_samples[symbol][idx]}', cv2.IMREAD_GRAYSCALE)\n",
    "    if erode:\n",
    "        img = cv2.erode(img, np.ones((5, 5), np.uint8), iterations=1)\n",
    "    return img\n",
    "\n",
    "def create_img_array_from_coordinates(coordinates, img_height, img_width):\n",
    "    img_array = torch.zeros((img_height, img_width))\n",
    "    for i in range(max_length):\n",
    "        x0, y0, x1, y1 = coordinates[i].to(torch.int64)\n",
    "        img_array[int(y0):int(y1), int(x0):int(x0)+1] = 1.0\n",
    "        img_array[int(y0):int(y1), int(x1):int(x1)+1] = 1.0\n",
    "        img_array[int(y0):int(y0)+1, int(x0):int(x1)] = 1.0\n",
    "        img_array[int(y1):int(y1)+1, int(x0):int(x1)] = 1.0\n",
    "    return img_array\n",
    "\n",
    "def create_cv2img_array_from_coordinates(input_seqs, coordinates, img_height, img_width, vocab, smallest_index=3):\n",
    "    img_array = np.ones((img_height, img_width))\n",
    "    for i in range(max_length):\n",
    "        if input_seqs[i] < smallest_index:\n",
    "            continue\n",
    "        symbol = vocab_in.idx2word[int(input_seqs[i])]\n",
    "        img = load_image(map_symbols_to_classes(symbol.replace('\\\\', '')), class_samples)\n",
    "        if img is None:\n",
    "            raise Exception(f\"Image could not be generated\")\n",
    "        img = img / 255\n",
    "        x0, y0, x1, y1 = coordinates[i].to(torch.int64)\n",
    "        w, h = int(x1 - x0), int(y1 - y0)\n",
    "        if (w > 0 and h > 0):\n",
    "            img = cv2.resize(img, (w, h), interpolation = cv2.INTER_LINEAR)\n",
    "            #img = cv2.erode(img, np.ones((2, 2), np.uint8), iterations=3)\n",
    "            img_array[int(y0):int(y0)+h, int(x0):int(x0)+w] = img\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73db257-e6cc-4667-a92a-e13b4eb397ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(batch_size*2, figsize=(50, 80))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    ax[2*i].imshow(create_img_array_from_coordinates(coordinates[i], img_height, img_width))\n",
    "    ax[2*i+1].imshow(create_cv2img_array_from_coordinates(input_seqs[i], coordinates[i], img_height, img_width, vocab_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9838c65-1a29-4fa5-8ae8-e4fbce8455db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqgen.datasets.realdata import RealSequencesDataset\n",
    "dataset = RealSequencesDataset(filename=\"data/val/label.txt\", vocab_in=vocab_in, vocab_out=vocab_out, max_length=50, batch_size=10)\n",
    "\n",
    "input_seqs, coordinates, target_seqs = dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed4ac7-43a8-453f-9af9-6bfca220b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2*batch_size, figsize=(50, 80))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    coords = np.array(coordinates[i])\n",
    "    coords = np.array(normalize_coordinates(np.array([coords]), contains_class=False)).squeeze()\n",
    "    coords[:, [0,2]] *= img_width\n",
    "    coords[:, [1,3]] *= img_height\n",
    "    coords = torch.tensor(coords)\n",
    "    ax[2*i].imshow(create_img_array_from_coordinates(coords, img_height, img_width).numpy())\n",
    "    ax[2*i+1].imshow(create_cv2img_array_from_coordinates(input_seqs[i], coords, img_height, img_width, vocab_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c698ae2-7570-403d-8fda-2a866594a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2*batch_size, figsize=(50, 80))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    coords = add_noise_to_coordinates(coordinates[i])\n",
    "    coords = np.array(normalize_coordinates(np.array(coords), contains_class=False)).squeeze()\n",
    "    coords[:, [0,2]] *= img_width\n",
    "    coords[:, [1,3]] *= img_height\n",
    "    coords = torch.tensor(coords)\n",
    "    ax[2*i].imshow(create_img_array_from_coordinates(coords, img_height, img_width).numpy())\n",
    "    ax[2*i+1].imshow(create_cv2img_array_from_coordinates(input_seqs[i], coords, img_height, img_width, vocab_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1ad73-5b11-4633-969b-4561ba5121c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2*batch_size, figsize=(50, 80))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    coords = add_noise_to_coordinates(coordinates[i])\n",
    "    coords = np.array(normalize_coordinates(np.array(coords), contains_class=False)).squeeze()\n",
    "    coords[:, [0,2]] *= img_width\n",
    "    coords[:, [1,3]] *= img_height\n",
    "    coords = torch.tensor(coords)\n",
    "    in_seq, out_seq = replace_symbols(input_seqs[i], target_seqs[i], vocab_in, vocab_out)\n",
    "    ax[2*i].imshow(create_img_array_from_coordinates(coords, img_height, img_width).numpy())\n",
    "    ax[2*i+1].imshow(create_cv2img_array_from_coordinates(in_seq, coords, img_height, img_width, vocab_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43793a0f-34a2-42ad-b2ec-330e317ef597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
