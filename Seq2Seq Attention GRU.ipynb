{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307e370c-40cb-4194-bf97-ad9e9ab745be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seqgen.seq_gen as g\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "197faa56-4d2f-4453-bbda-32c5b4115b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccbe91c-6286-4398-ad71-ee251ae1af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "num_layers=2\n",
    "embedding_dim=16\n",
    "hidden_size=16\n",
    "batch_size=64\n",
    "max_length=16\n",
    "bidirectional=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78d88d5-47ef-4b8f-9d41-e1d8fb70c635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilof\\AppData\\Local\\Temp\\ipykernel_12420\\3989188473.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
      "C:\\Users\\tilof\\AppData\\Local\\Temp\\ipykernel_12420\\3989188473.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coordinates = torch.tensor(features[:, :, 1:])\n"
     ]
    }
   ],
   "source": [
    "features, target_seqs = g.generate_synthetic_training_data(batch_size, max_length=max_length, device=device, continue_prob=0.997, swap_times=0)\n",
    "input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
    "coordinates = torch.tensor(features[:, :, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437f41f6-057b-41e5-9d52-1744666d4aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 16, 5]),\n",
       " torch.Size([64, 16]),\n",
       " torch.Size([64, 16, 4]),\n",
       " torch.Size([64, 16]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, input_seqs.shape, coordinates.shape, target_seqs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae84cc-67fe-4487-b119-51c1e6563d7c",
   "metadata": {},
   "source": [
    "# The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c25b8f-93b5-4493-81e4-fb6c1f3a67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqgen.model import seq2seq_lstm\n",
    "from seqgen.vocabulary import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7974696-6918-47ff-93b7-14cc4517dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint = False\n",
    "checkpoint_file = \"model_len25_biy_layers3.pt\"\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "encoder = seq2seq_lstm.EncoderGRU(vocab_size=len(vocab_in), embedding_dim=embedding_dim, num_layers=num_layers, max_length=max_length, hidden_size=hidden_size, bidirectional=bidirectional, pos_encoding=False).to(features.device)\n",
    "decoder = seq2seq_lstm.DecoderGRUAttention2(embedding_dim=embedding_dim, num_layers=num_layers, max_length=max_length, hidden_size=hidden_size, vocab_size=len(vocab_out), bidirectional=bidirectional, pos_encoding=False).to(features.device)\n",
    "\n",
    "# Initialize optimizer for encoder and decoder\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "#positions = seq2seq_lstm.get_position_encoding(max_length, embedding_dim, device=device)\n",
    "positions = seq2seq_lstm.get_coordinate_encoding(coordinates, d=embedding_dim, device=device)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Load model weights from checkpoint\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    encoder.load_state_dict(checkpoint['encoder_model_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_model_state_dict'])\n",
    "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "    num_layers = checkpoint['num_layers']\n",
    "    embedding_dim = checkpoint['embedding_dim']\n",
    "    hidden_size = checkpoint['hidden_size']\n",
    "    bidirectional = checkpoint['bidirectional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d63a2d8-bced-44ec-b32b-19d5f3055885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run word 1 of all 64 sequences through the encoder\n",
      "Run word 2 of all 64 sequences through the encoder\n",
      "Run word 3 of all 64 sequences through the encoder\n",
      "Run word 4 of all 64 sequences through the encoder\n",
      "Run word 5 of all 64 sequences through the encoder\n",
      "Run word 6 of all 64 sequences through the encoder\n",
      "Run word 7 of all 64 sequences through the encoder\n",
      "Run word 8 of all 64 sequences through the encoder\n",
      "Run word 9 of all 64 sequences through the encoder\n",
      "Run word 10 of all 64 sequences through the encoder\n",
      "Run word 11 of all 64 sequences through the encoder\n",
      "Run word 12 of all 64 sequences through the encoder\n",
      "Run word 13 of all 64 sequences through the encoder\n",
      "Run word 14 of all 64 sequences through the encoder\n",
      "Run word 15 of all 64 sequences through the encoder\n",
      "Run word 16 of all 64 sequences through the encoder\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encoder hidden state and cell state with zeros\n",
    "hn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "cn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "\n",
    "_hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "\n",
    "# Iterate over the sequence words and run every word through the encoder\n",
    "for i in range(input_seqs.shape[1]):\n",
    "    # Run the i-th word of the input sequence through the encoder.\n",
    "    # As a result we will get the prediction (output), the hidden state and the cell state.\n",
    "    # The hidden state and cell state will be used as inputs in the next round\n",
    "    print(f\"Run word {i+1} of all {input_seqs.shape[0]} sequences through the encoder\")\n",
    "    output, hn = encoder(input_seqs[:, i].unsqueeze(dim=1), coordinates[:, i], positions[:, i:i+1], hn)\n",
    "    encoder_outputs[:, i:i+1, :] = output\n",
    "    encoder_hidden_states[:, i, :] = seq2seq_lstm.concat_hidden_states(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbb3c2a2-1b0a-404a-9d8c-0ae7fbc8485c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 16]),\n",
       " torch.Size([2, 64, 16]),\n",
       " torch.Size([2, 64, 16]),\n",
       " torch.Size([64, 16, 32]),\n",
       " torch.Size([64, 16, 16]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, hn.shape, cn.shape, encoder_hidden_states.shape, encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29054ba-e5b3-405f-bc5b-149f8862da81",
   "metadata": {},
   "source": [
    "# The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eda91121-2c2d-4531-b7ee-d67096698123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run word 1 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 2 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 3 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 4 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 5 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 6 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 7 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 8 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 9 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 10 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 11 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 12 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 13 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 14 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 15 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "Run word 16 through decoder torch.Size([2, 64, 16]) torch.Size([64, 16, 32])\n",
      "LOSS 3.2781662940979004\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "\n",
    "# Iterate over words of target sequence and run words through the decoder.\n",
    "# This will produce a prediction for the next word in the sequence\n",
    "for i in range(0, target_seqs.size(1)):\n",
    "    print(f\"Run word {i+1} through decoder\", hn.shape, encoder_hidden_states.shape)\n",
    "    output, hn, attention = decoder(\n",
    "        x=target_seqs[:, i].unsqueeze(dim=1),\n",
    "        coordinates=coordinates[:, i],\n",
    "        annotations=encoder_hidden_states,\n",
    "        position=positions[:, i:i+1],\n",
    "        hidden=hn\n",
    "    )\n",
    "    loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "\n",
    "print(\"LOSS\", loss.item() / max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c59e1-3885-4a12-8696-9fcf3da9cf9e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f9ad9-7ae1-4b49-99b8-490eacfd5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilof\\AppData\\Local\\Temp\\ipykernel_12420\\2510380868.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
      "C:\\Users\\tilof\\AppData\\Local\\Temp\\ipykernel_12420\\2510380868.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coordinates = torch.tensor(features[:, :, 1:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS after epoch 0 3.065668821334839 ACCURACY 4.8828125e-05\n",
      "LOSS after epoch 100 2.492243528366089 ACCURACY 0.097607421875\n",
      "LOSS after epoch 200 2.4589684009552 ACCURACY 0.119306640625\n",
      "LOSS after epoch 300 2.3016295433044434 ACCURACY 0.14443359375\n",
      "LOSS after epoch 400 2.233405351638794 ACCURACY 0.170302734375\n",
      "LOSS after epoch 500 2.1682372093200684 ACCURACY 0.192138671875\n",
      "LOSS after epoch 600 2.321911573410034 ACCURACY 0.179072265625\n",
      "LOSS after epoch 700 2.198976993560791 ACCURACY 0.18244140625\n",
      "LOSS after epoch 800 2.1662652492523193 ACCURACY 0.20322265625\n",
      "LOSS after epoch 900 2.143745183944702 ACCURACY 0.215859375\n",
      "LOSS after epoch 1000 2.0467982292175293 ACCURACY 0.227509765625\n",
      "LOSS after epoch 1100 2.044188976287842 ACCURACY 0.238330078125\n",
      "LOSS after epoch 1200 1.9282972812652588 ACCURACY 0.256025390625\n",
      "LOSS after epoch 1300 1.9855766296386719 ACCURACY 0.262353515625\n",
      "LOSS after epoch 1400 1.9942045211791992 ACCURACY 0.248759765625\n",
      "LOSS after epoch 1500 1.9476869106292725 ACCURACY 0.246640625\n",
      "LOSS after epoch 1600 1.9970340728759766 ACCURACY 0.25439453125\n",
      "LOSS after epoch 1700 1.9257824420928955 ACCURACY 0.258330078125\n",
      "LOSS after epoch 1800 1.8363815546035767 ACCURACY 0.267109375\n",
      "LOSS after epoch 1900 2.158437728881836 ACCURACY 0.271259765625\n",
      "LOSS after epoch 2000 1.8509576320648193 ACCURACY 0.274765625\n",
      "LOSS after epoch 2100 1.8521353006362915 ACCURACY 0.280263671875\n",
      "LOSS after epoch 2200 1.810558557510376 ACCURACY 0.28353515625\n",
      "LOSS after epoch 2300 1.849995493888855 ACCURACY 0.277490234375\n",
      "LOSS after epoch 2400 1.8561103343963623 ACCURACY 0.2869140625\n",
      "LOSS after epoch 2500 1.8106255531311035 ACCURACY 0.286884765625\n",
      "LOSS after epoch 2600 1.8316326141357422 ACCURACY 0.291865234375\n",
      "LOSS after epoch 2700 1.7717775106430054 ACCURACY 0.29435546875\n",
      "LOSS after epoch 2800 1.8998167514801025 ACCURACY 0.265078125\n",
      "LOSS after epoch 2900 1.8088676929473877 ACCURACY 0.279609375\n",
      "LOSS after epoch 3000 1.8343732357025146 ACCURACY 0.28384765625\n",
      "LOSS after epoch 3100 1.8051340579986572 ACCURACY 0.290234375\n",
      "LOSS after epoch 3200 1.7568424940109253 ACCURACY 0.295810546875\n",
      "LOSS after epoch 3300 1.7816176414489746 ACCURACY 0.29916015625\n",
      "LOSS after epoch 3400 1.763124942779541 ACCURACY 0.301328125\n",
      "LOSS after epoch 3500 2.1070241928100586 ACCURACY 0.29470703125\n",
      "LOSS after epoch 3600 1.8480068445205688 ACCURACY 0.27103515625\n",
      "LOSS after epoch 3700 1.8201318979263306 ACCURACY 0.285439453125\n",
      "LOSS after epoch 3800 1.7921868562698364 ACCURACY 0.28685546875\n",
      "LOSS after epoch 3900 1.8046720027923584 ACCURACY 0.294814453125\n",
      "LOSS after epoch 4000 1.7755368947982788 ACCURACY 0.2987109375\n",
      "LOSS after epoch 4100 1.7503653764724731 ACCURACY 0.301220703125\n",
      "LOSS after epoch 4200 1.7697482109069824 ACCURACY 0.30224609375\n",
      "LOSS after epoch 4300 1.73072350025177 ACCURACY 0.30083984375\n",
      "LOSS after epoch 4400 1.7250627279281616 ACCURACY 0.3041796875\n",
      "LOSS after epoch 4500 1.7093552350997925 ACCURACY 0.3064453125\n",
      "LOSS after epoch 4600 1.704211950302124 ACCURACY 0.307744140625\n",
      "LOSS after epoch 4700 1.66659677028656 ACCURACY 0.309296875\n",
      "LOSS after epoch 4800 1.708532452583313 ACCURACY 0.3143359375\n",
      "LOSS after epoch 4900 1.7093802690505981 ACCURACY 0.313408203125\n",
      "LOSS after epoch 5000 1.6909289360046387 ACCURACY 0.31525390625\n",
      "LOSS after epoch 5100 1.6560558080673218 ACCURACY 0.308837890625\n",
      "LOSS after epoch 5200 1.6256650686264038 ACCURACY 0.31857421875\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(50000):\n",
    "    # With a certain chance present the model the true predictions\n",
    "    # instead of its own predictions in the next iteration\n",
    "    use_teacher_forcing_prob = 0.5\n",
    "    use_teacher_forcing = random.random() < use_teacher_forcing_prob\n",
    "    \n",
    "    # Get a batch of trianing data\n",
    "    features, target_seqs = g.generate_synthetic_training_data(batch_size, max_length=max_length, continue_prob=0.999, device=device, swap_times=0)\n",
    "    features = features.to(device)\n",
    "    target_seqs = target_seqs.to(device)\n",
    "    input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
    "    coordinates = torch.tensor(features[:, :, 1:])\n",
    "\n",
    "    # Initialize the encoder hidden state and cell state with zeros\n",
    "    hn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "    \n",
    "    # Initialize encoder outputs tensor\n",
    "    last_n_states = 2 if bidirectional else 1\n",
    "    _hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "    encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "    encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "    \n",
    "    # Set gradients of all model parameters to zero\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "    \n",
    "    ####################\n",
    "    #     ENCODING     #\n",
    "    ####################\n",
    "\n",
    "    # Iterate over the sequence words and run every word through the encoder\n",
    "    for i in range(input_seqs.shape[1]):\n",
    "        # Run the i-th word of the input sequence through the encoder.\n",
    "        # As a result we will get the prediction (output), the hidden state (hn) and the cell state (cn).\n",
    "        # The hidden state and cell state will be used as inputs in the next round\n",
    "        output, hn = encoder(\n",
    "            input_seqs[:, i].unsqueeze(dim=1),\n",
    "            coordinates[:, i],\n",
    "            positions[:, i:i+1],\n",
    "            hn\n",
    "        )\n",
    "        # Save encoder outputs and states for current word\n",
    "        encoder_outputs[:, i:i+1, :] = output\n",
    "        encoder_hidden_states[:, i, :] = seq2seq_lstm.concat_hidden_states(hn)\n",
    "\n",
    "    ####################\n",
    "    #     DECODING     #\n",
    "    ####################\n",
    "    \n",
    "    accuracy = 0.0\n",
    "\n",
    "    # The first words that we be presented to the model is the '<start>' token\n",
    "    prediction = target_seqs[:, 0]\n",
    "    \n",
    "    # Iterate over words of target sequence and run words through the decoder.\n",
    "    # This will produce a prediction for the next word in the sequence\n",
    "    for i in range(1, target_seqs.size(1)):\n",
    "        # Run word i through decoder and get word i+1 and the new hidden state as outputs\n",
    "        if use_teacher_forcing:\n",
    "            output, hn, attention = decoder(\n",
    "                x=prediction.unsqueeze(dim=1),\n",
    "                coordinates=coordinates[:, i-1],\n",
    "                annotations=encoder_hidden_states,\n",
    "                position=positions[:, i:i+1],\n",
    "                hidden=hn\n",
    "            )\n",
    "        else:\n",
    "            output, hn, attention = decoder(\n",
    "                x=prediction.unsqueeze(dim=1),\n",
    "                coordinates=coordinates[:, i-1],\n",
    "                annotations=encoder_hidden_states,\n",
    "                position=positions[:, i:i+1],\n",
    "                hidden=hn\n",
    "            )\n",
    "\n",
    "        # Get the predicted classes of the model\n",
    "        #print(\"OUTPUT\", output.shape, output.squeeze().shape)\n",
    "        topv, topi = output.topk(1)\n",
    "        #print(\"TOP\", topi.shape, topv.shape)\n",
    "        prediction = topi.squeeze()    \n",
    "        loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "        accuracy += float((prediction == target_seqs[:, i]).sum() / (target_seqs.size(0)*target_seqs.size(1)))\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print_every = 100\n",
    "    if not epoch % print_every:\n",
    "        _accuracy = sum(accuracies[-print_every:]) / print_every\n",
    "        print(f\"LOSS after epoch {epoch}\", loss.item() / (target_seqs.size(1)), \"ACCURACY\", _accuracy)\n",
    "\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    accuracy = 0.0\n",
    "\n",
    "    # Update weights of encoder and decoder\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e3605-8f59-42ee-b260-5bb500518d00",
   "metadata": {},
   "source": [
    "#### Save model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67bec2-ccfd-4492-bd1a-e51fad94fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "model_data = {\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length\n",
    "}\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'encoder_model_state_dict': encoder.state_dict(),\n",
    "    'decoder_model_state_dict': decoder.state_dict(),\n",
    "    'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "    'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"bidirectional\": bidirectional,\n",
    "}, \"model_\" + date_time + \".pt\")\n",
    "\n",
    "\n",
    "with open(\"training_\" + date_time + '.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56c813-13d3-45a2-b9e1-47fed03bcf3c",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "We run our input sequences through the model and get output seuences. Then we decode the output sequences with the Vocabulary class and get our final latex code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb8ff7-8d9a-4c28-8f95-264703cfe636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_seqs, coordinates, target_seqs):\n",
    "    vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "    vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "    predictions = torch.zeros(target_seqs.shape)\n",
    "    attention_matrix = torch.zeros((input_seqs.shape[0], input_seqs.shape[1], input_seqs.shape[1]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize the encoder hidden state and cell state with zeros\n",
    "        hn = encoder.initHidden(input_seqs.shape[0], device=features.device)        \n",
    "        _hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        last_n_states = 2 if bidirectional else 1\n",
    "        encoder_hidden_states = torch.zeros((input_seqs.shape[0], max_length, _hidden_size)).to(device)\n",
    "        encoder_outputs = torch.zeros((input_seqs.shape[0], max_length, _hidden_size)).to(device)\n",
    "\n",
    "        # Iterate over the sequence words and run every word through the encoder\n",
    "        for i in range(input_seqs.size(1)):\n",
    "            output, hn = encoder(\n",
    "                input_seqs[:, i].unsqueeze(dim=1),\n",
    "                coordinates[:, i],\n",
    "                positions[:, i:i+1],\n",
    "                hn\n",
    "            )\n",
    "            encoder_outputs[:, i:i+1, :] = output\n",
    "            encoder_hidden_states[:, i:i+1, :] = seq2seq_lstm.concat_hidden_states(hn[-last_n_states:]).unsqueeze(dim=1)\n",
    "\n",
    "        # Predict tokens of the target sequence by running the hidden state through\n",
    "        # the decoder\n",
    "        for i in range(0, target_seqs.size(1)):\n",
    "            output, hn, attention = decoder(\n",
    "                x=target_seqs[:, i].unsqueeze(dim=1),\n",
    "                coordinates=coordinates[:, i],\n",
    "                annotations=encoder_hidden_states,\n",
    "                position=positions[:, i:i+1],\n",
    "                hidden=hn\n",
    "            )\n",
    "            # Select the indices of the most likely tokens\n",
    "            predicted_char = torch.argmax(output, dim=1)\n",
    "            predictions[:, i] = torch.argmax(output, dim=1).squeeze()\n",
    "            attention_matrix[:, :, i:i+1] = attention\n",
    "        \n",
    "        return predictions, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4224ca-ab02-437b-a0ea-5704b1c01daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, attention_matrix = predict(input_seqs, coordinates, target_seqs)\n",
    "prediction.shape, attention_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6896f-5818-4718-a584-0aa59e9226de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(attention_matrix[random.randint(0, prediction.size(0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476c3b2-d8ad-4506-8ace-814d9e9eefcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_swapped = g.random_swap(input_seqs[0], i=2).unsqueeze(dim=0)\n",
    "coords_swapped = g.random_swap(coordinates[0], i=2).unsqueeze(dim=0)\n",
    "prediction_swapped = predict(in_swapped, coords_swapped, target_seqs[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0408b19-3288-48ac-905b-88eb6ef4017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs[0:1] == in_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb4a03-69bd-45e1-960f-244da1c18417",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction == prediction_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf18d6-94da-42ce-8bad-86a9f2d16d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random sequence and its prediction from the model\n",
    "import random\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "predictions, attention_matrix = predict(input_seqs, coordinates, target_seqs)\n",
    "\n",
    "i = random.randint(0, predictions.size(0))\n",
    "print(\"MODEL INPUT\", vocab_in.decode_sequence(input_seqs[i].cpu().numpy()))\n",
    "print(\"MODEL OUTPUT\", vocab_out.decode_sequence(predictions[i].cpu().numpy()))\n",
    "print(\"TARGET OUTPUT\", vocab_out.decode_sequence(target_seqs[i][1:].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeae19b-77d4-4715-8068-9822af23009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = vocab_out.decode_sequence(predictions[i].cpu().numpy())\n",
    "prediction = list(filter(lambda x: x != '<end>', prediction))\n",
    "prediction = \"\".join(prediction)\n",
    "print(\"MODEL OUTPUT\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b7aff-cecf-476a-bd3c-ae737ff8aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(attention_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05457f9c-56b4-4aaa-ae20-c4e8b120ad62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
