{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307e370c-40cb-4194-bf97-ad9e9ab745be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seqgen.seq_gen as g\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "197faa56-4d2f-4453-bbda-32c5b4115b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccbe91c-6286-4398-ad71-ee251ae1af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "num_layers=1\n",
    "embedding_dim = 64\n",
    "hidden_size=64\n",
    "batch_size=32\n",
    "max_length=20\n",
    "bidirectional=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78d88d5-47ef-4b8f-9d41-e1d8fb70c635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilof\\AppData\\Local\\Temp\\ipykernel_14556\\3989188473.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
      "C:\\Users\\tilof\\AppData\\Local\\Temp\\ipykernel_14556\\3989188473.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coordinates = torch.tensor(features[:, :, 1:])\n"
     ]
    }
   ],
   "source": [
    "features, target_seqs = g.generate_synthetic_training_data(batch_size, max_length=max_length, device=device, continue_prob=0.997, swap_times=0)\n",
    "input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
    "coordinates = torch.tensor(features[:, :, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437f41f6-057b-41e5-9d52-1744666d4aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 20, 5]),\n",
       " torch.Size([32, 20]),\n",
       " torch.Size([32, 20, 4]),\n",
       " torch.Size([32, 20]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, input_seqs.shape, coordinates.shape, target_seqs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae84cc-67fe-4487-b119-51c1e6563d7c",
   "metadata": {},
   "source": [
    "# The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c25b8f-93b5-4493-81e4-fb6c1f3a67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqgen.model import seq2seq_lstm\n",
    "from seqgen.vocabulary import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7974696-6918-47ff-93b7-14cc4517dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint = False\n",
    "checkpoint_file = \"model_len25_biy_layers3.pt\"\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "encoder = seq2seq_lstm.EncoderRNN(vocab_size=len(vocab_in), embedding_dim=embedding_dim, num_layers=num_layers, max_length=max_length, hidden_size=hidden_size, bidirectional=bidirectional, pos_encoding=False).to(features.device)\n",
    "decoder = seq2seq_lstm.DecoderLSTMAttention(embedding_dim=embedding_dim, num_layers=num_layers, max_length=max_length, hidden_size=hidden_size, vocab_size=len(vocab_out), bidirectional=bidirectional, pos_encoding=False).to(features.device)\n",
    "\n",
    "# Initialize optimizer for encoder and decoder\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "#positions = seq2seq_lstm.get_position_encoding(max_length, embedding_dim, device=device)\n",
    "positions = seq2seq_lstm.get_coordinate_encoding(coordinates, d=embedding_dim, device=device)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Load model weights from checkpoint\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    encoder.load_state_dict(checkpoint['encoder_model_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_model_state_dict'])\n",
    "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "    num_layers = checkpoint['num_layers']\n",
    "    embedding_dim = checkpoint['embedding_dim']\n",
    "    hidden_size = checkpoint['hidden_size']\n",
    "    bidirectional = checkpoint['bidirectional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d63a2d8-bced-44ec-b32b-19d5f3055885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run word 1 of all 32 sequences through the encoder\n",
      "Run word 2 of all 32 sequences through the encoder\n",
      "Run word 3 of all 32 sequences through the encoder\n",
      "Run word 4 of all 32 sequences through the encoder\n",
      "Run word 5 of all 32 sequences through the encoder\n",
      "Run word 6 of all 32 sequences through the encoder\n",
      "Run word 7 of all 32 sequences through the encoder\n",
      "Run word 8 of all 32 sequences through the encoder\n",
      "Run word 9 of all 32 sequences through the encoder\n",
      "Run word 10 of all 32 sequences through the encoder\n",
      "Run word 11 of all 32 sequences through the encoder\n",
      "Run word 12 of all 32 sequences through the encoder\n",
      "Run word 13 of all 32 sequences through the encoder\n",
      "Run word 14 of all 32 sequences through the encoder\n",
      "Run word 15 of all 32 sequences through the encoder\n",
      "Run word 16 of all 32 sequences through the encoder\n",
      "Run word 17 of all 32 sequences through the encoder\n",
      "Run word 18 of all 32 sequences through the encoder\n",
      "Run word 19 of all 32 sequences through the encoder\n",
      "Run word 20 of all 32 sequences through the encoder\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encoder hidden state and cell state with zeros\n",
    "hn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "cn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "\n",
    "_hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "\n",
    "# Iterate over the sequence words and run every word through the encoder\n",
    "for i in range(input_seqs.shape[1]):\n",
    "    # Run the i-th word of the input sequence through the encoder.\n",
    "    # As a result we will get the prediction (output), the hidden state and the cell state.\n",
    "    # The hidden state and cell state will be used as inputs in the next round\n",
    "    print(f\"Run word {i+1} of all {input_seqs.shape[0]} sequences through the encoder\")\n",
    "    output, (hn, cn) = encoder(input_seqs[:, i].unsqueeze(dim=1), coordinates[:, i], positions[:, i:i+1], (hn, cn))\n",
    "    encoder_outputs[:, i:i+1, :] = output\n",
    "    encoder_hidden_states[:, i, :] = seq2seq_lstm.concat_hidden_states(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbb3c2a2-1b0a-404a-9d8c-0ae7fbc8485c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 128]),\n",
       " torch.Size([2, 32, 64]),\n",
       " torch.Size([2, 32, 64]),\n",
       " torch.Size([32, 20, 128]),\n",
       " torch.Size([32, 20, 128]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, hn.shape, cn.shape, encoder_hidden_states.shape, encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29054ba-e5b3-405f-bc5b-149f8862da81",
   "metadata": {},
   "source": [
    "# The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda91121-2c2d-4531-b7ee-d67096698123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run word 1 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 2 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 3 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 4 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 5 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 6 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run word 7 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 8 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 9 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 10 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 11 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 12 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 13 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 14 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 15 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 16 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 17 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 18 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 19 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "Run word 20 through decoder torch.Size([2, 32, 64]) torch.Size([32, 20, 128])\n",
      "LOSS 3.256869888305664\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "\n",
    "# Iterate over words of target sequence and run words through the decoder.\n",
    "# This will produce a prediction for the next word in the sequence\n",
    "for i in range(0, target_seqs.size(1)):\n",
    "    print(f\"Run word {i+1} through decoder\", hn.shape, encoder_hidden_states.shape)\n",
    "    output, (hn, cn), attention = decoder(\n",
    "        x=target_seqs[:, i].unsqueeze(dim=1),\n",
    "        coordinates=coordinates[:, i],\n",
    "        annotations=encoder_hidden_states,\n",
    "        position=positions[:, i:i+1],\n",
    "        hidden=(hn, cn)\n",
    "    )\n",
    "    loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "\n",
    "print(\"LOSS\", loss.item() / max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c59e1-3885-4a12-8696-9fcf3da9cf9e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f9ad9-7ae1-4b49-99b8-490eacfd5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilof\\AppData\\Local\\Temp\\ipykernel_14556\\1797800676.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
      "C:\\Users\\tilof\\AppData\\Local\\Temp\\ipykernel_14556\\1797800676.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coordinates = torch.tensor(features[:, :, 1:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS after epoch 0 1.3657781600952148 ACCURACY 0.0\n",
      "LOSS after epoch 100 1.5163612365722656 ACCURACY 0.2063437541620806\n",
      "LOSS after epoch 200 1.357155704498291 ACCURACY 0.1831875035748817\n",
      "LOSS after epoch 300 1.4871570587158203 ACCURACY 0.21540625410154463\n",
      "LOSS after epoch 400 1.468447780609131 ACCURACY 0.19165625381981954\n",
      "LOSS after epoch 500 1.4287606239318849 ACCURACY 0.2448281293886248\n",
      "LOSS after epoch 600 1.5602463722229003 ACCURACY 0.22242187896277754\n",
      "LOSS after epoch 700 1.4016406059265136 ACCURACY 0.21118750390363858\n",
      "LOSS after epoch 800 1.3715116500854492 ACCURACY 0.22389062916859984\n",
      "LOSS after epoch 900 1.568272876739502 ACCURACY 0.23265625465661288\n",
      "LOSS after epoch 1000 1.3579626083374023 ACCURACY 0.19409375369665213\n",
      "LOSS after epoch 1100 1.5044645309448241 ACCURACY 0.23442187945591286\n",
      "LOSS after epoch 1200 1.4579524040222167 ACCURACY 0.2175468794023618\n",
      "LOSS after epoch 1300 1.4644655227661132 ACCURACY 0.19443750351667405\n",
      "LOSS after epoch 1400 1.4049686431884765 ACCURACY 0.23676562949549407\n",
      "LOSS after epoch 1500 1.5128771781921386 ACCURACY 0.18559375349897891\n",
      "LOSS after epoch 1600 1.3515018463134765 ACCURACY 0.20198437898419797\n",
      "LOSS after epoch 1700 1.4242352485656737 ACCURACY 0.19504687843611465\n",
      "LOSS after epoch 1800 1.308109664916992 ACCURACY 0.24050000451505185\n",
      "LOSS after epoch 1900 1.4534198760986328 ACCURACY 0.20420312878210098\n",
      "LOSS after epoch 2000 1.439930248260498 ACCURACY 0.1981718787434511\n",
      "LOSS after epoch 2100 1.5009114265441894 ACCURACY 0.19529687882401048\n",
      "LOSS after epoch 2200 1.4242798805236816 ACCURACY 0.18159375346731393\n",
      "LOSS after epoch 2300 1.4146822929382323 ACCURACY 0.20642187886405736\n",
      "LOSS after epoch 2400 1.377598190307617 ACCURACY 0.22446875437162817\n",
      "LOSS after epoch 2500 1.3029304504394532 ACCURACY 0.22306250447407364\n",
      "LOSS after epoch 2600 1.521925163269043 ACCURACY 0.2460000048810616\n",
      "LOSS after epoch 2700 1.4889838218688964 ACCURACY 0.22378125436138363\n",
      "LOSS after epoch 2800 1.4433765411376953 ACCURACY 0.2325000045960769\n",
      "LOSS after epoch 2900 1.372525978088379 ACCURACY 0.23192187919281423\n",
      "LOSS after epoch 3000 1.3692742347717286 ACCURACY 0.2530312545504421\n",
      "LOSS after epoch 3100 1.428398036956787 ACCURACY 0.207656254125759\n",
      "LOSS after epoch 3200 1.4661320686340331 ACCURACY 0.22618750434834511\n",
      "LOSS after epoch 3300 1.2715183258056642 ACCURACY 0.22409375410992652\n",
      "LOSS after epoch 3400 1.4653807640075684 ACCURACY 0.2144531294563785\n",
      "LOSS after epoch 3500 1.3457314491271972 ACCURACY 0.20865625359117984\n",
      "LOSS after epoch 3600 1.2293078422546386 ACCURACY 0.21303125438280404\n",
      "LOSS after epoch 3700 1.410214614868164 ACCURACY 0.2037968788528815\n",
      "LOSS after epoch 3800 1.3285651206970215 ACCURACY 0.22395312952343374\n",
      "LOSS after epoch 3900 1.3725593566894532 ACCURACY 0.23453125457745044\n",
      "LOSS after epoch 4000 1.3490456581115722 ACCURACY 0.25562500457279386\n",
      "LOSS after epoch 4100 1.4572998046875 ACCURACY 0.2372343793557957\n",
      "LOSS after epoch 4200 1.5900218963623047 ACCURACY 0.21340625395299867\n",
      "LOSS after epoch 4300 1.4382697105407716 ACCURACY 0.23629687955137343\n",
      "LOSS after epoch 4400 1.4341848373413086 ACCURACY 0.22893750459188594\n",
      "LOSS after epoch 4500 1.4464837074279786 ACCURACY 0.20123437888920306\n",
      "LOSS after epoch 4600 1.3686300277709962 ACCURACY 0.19834375383798034\n",
      "LOSS after epoch 4700 1.457217311859131 ACCURACY 0.23367187960073352\n",
      "LOSS after epoch 4800 1.4766910552978516 ACCURACY 0.20364062873646616\n",
      "LOSS after epoch 4900 1.4779696464538574 ACCURACY 0.18175000339746475\n",
      "LOSS after epoch 5000 1.456449317932129 ACCURACY 0.2341093794698827\n",
      "LOSS after epoch 5100 1.5284372329711915 ACCURACY 0.2217656290670857\n",
      "LOSS after epoch 5200 1.2699650764465331 ACCURACY 0.20859375389758497\n",
      "LOSS after epoch 5300 1.4003082275390626 ACCURACY 0.2201718793483451\n",
      "LOSS after epoch 5400 1.4934762954711913 ACCURACY 0.2152343789488077\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(100000):\n",
    "    # With a certain chance present the model the true predictions\n",
    "    # instead of its own predictions in the next iteration\n",
    "    use_teacher_forcing_prob = 0.5\n",
    "    use_teacher_forcing = random.random() < use_teacher_forcing_prob\n",
    "    \n",
    "    # Get a batch of trianing data\n",
    "    features, target_seqs = g.generate_synthetic_training_data(batch_size, max_length=max_length, continue_prob=0.99, device=device, swap_times=0)\n",
    "    features = features.to(device)\n",
    "    target_seqs = target_seqs.to(device)\n",
    "    input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
    "    coordinates = torch.tensor(features[:, :, 1:])\n",
    "\n",
    "    # Initialize the encoder hidden state and cell state with zeros\n",
    "    hn_enc = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "    cn_enc = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "    \n",
    "    # Initialize encoder outputs tensor\n",
    "    last_n_states = 2 if bidirectional else 1\n",
    "    _hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "    encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "    encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "    \n",
    "    # Set gradients of all model parameters to zero\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "    \n",
    "    ####################\n",
    "    #     ENCODING     #\n",
    "    ####################\n",
    "\n",
    "    # Iterate over the sequence words and run every word through the encoder\n",
    "    for i in range(input_seqs.shape[1]):\n",
    "        # Run the i-th word of the input sequence through the encoder.\n",
    "        # As a result we will get the prediction (output), the hidden state (hn) and the cell state (cn).\n",
    "        # The hidden state and cell state will be used as inputs in the next round\n",
    "        output, (hn_enc, cn_enc) = encoder(\n",
    "            input_seqs[:, i].unsqueeze(dim=1),\n",
    "            coordinates[:, i],\n",
    "            positions[:, i:i+1],\n",
    "            (hn_enc, cn_enc)\n",
    "        )\n",
    "        # Save encoder outputs and states for current word\n",
    "        encoder_outputs[:, i:i+1, :] = output\n",
    "        encoder_hidden_states[:, i, :] = seq2seq_lstm.concat_hidden_states(hn)\n",
    "\n",
    "    ####################\n",
    "    #     DECODING     #\n",
    "    ####################\n",
    "    \n",
    "    accuracy = 0.0\n",
    "\n",
    "    # The first words that we be presented to the model is the '<start>' token\n",
    "    prediction = target_seqs[:, 0]\n",
    "    \n",
    "    # The initial hidden state of the decoder is the final hidden state of the encoder\n",
    "    hn_dec, cn_dec = hn_enc, cn_enc\n",
    "    \n",
    "    # Iterate over words of target sequence and run words through the decoder.\n",
    "    # This will produce a prediction for the next word in the sequence\n",
    "    for i in range(1, target_seqs.size(1)):\n",
    "        # Run word i through decoder and get word i+1 and the new hidden state as outputs\n",
    "        if use_teacher_forcing:\n",
    "            output, (hn_dec, cn_dec), attention = decoder(\n",
    "                x=prediction.unsqueeze(dim=1),\n",
    "                coordinates=coordinates[:, i-1],\n",
    "                annotations=encoder_hidden_states,\n",
    "                position=positions[:, i:i+1],\n",
    "                hidden=(hn_dec, cn_dec)\n",
    "            )\n",
    "        else:\n",
    "            output, (hn_dec, cn_dec), attention = decoder(\n",
    "                x=prediction.unsqueeze(dim=1),\n",
    "                coordinates=coordinates[:, i-1],\n",
    "                annotations=encoder_hidden_states,\n",
    "                position=positions[:, i:i+1],\n",
    "                hidden=(hn_dec, cn_dec)\n",
    "            )\n",
    "\n",
    "            # Get the predicted classes of the model\n",
    "            topv, topi = output.topk(1)\n",
    "            prediction = topi.squeeze()    \n",
    "        loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "        accuracy += float((prediction == target_seqs[:, i]).sum() / (target_seqs.size(0)*target_seqs.size(1)))\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print_every = 100\n",
    "    if not epoch % print_every:\n",
    "        _accuracy = sum(accuracies[-print_every:]) / print_every\n",
    "        print(f\"LOSS after epoch {epoch}\", loss.item() / (target_seqs.size(1)), \"ACCURACY\", _accuracy)\n",
    "\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    accuracy = 0.0\n",
    "\n",
    "    # Update weights of encoder and decoder\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e3605-8f59-42ee-b260-5bb500518d00",
   "metadata": {},
   "source": [
    "#### Save model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67bec2-ccfd-4492-bd1a-e51fad94fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "model_data = {\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length\n",
    "}\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'encoder_model_state_dict': encoder.state_dict(),\n",
    "    'decoder_model_state_dict': decoder.state_dict(),\n",
    "    'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "    'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"bidirectional\": bidirectional,\n",
    "}, \"model_\" + date_time + \".pt\")\n",
    "\n",
    "\n",
    "with open(\"training_\" + date_time + '.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56c813-13d3-45a2-b9e1-47fed03bcf3c",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "We run our input sequences through the model and get output seuences. Then we decode the output sequences with the Vocabulary class and get our final latex code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0fb8ff7-8d9a-4c28-8f95-264703cfe636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_seqs, coordinates, target_seqs):\n",
    "    vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "    vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "    predictions = torch.zeros(target_seqs.shape)\n",
    "    attention_matrix = torch.zeros((input_seqs.shape[0], input_seqs.shape[1], input_seqs.shape[1]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize the encoder hidden state and cell state with zeros\n",
    "        hn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "        cn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "        \n",
    "        _hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        last_n_states = 2 if bidirectional else 1\n",
    "        encoder_hidden_states = torch.zeros((input_seqs.shape[0], max_length, _hidden_size)).to(device)\n",
    "        encoder_outputs = torch.zeros((input_seqs.shape[0], max_length, _hidden_size)).to(device)\n",
    "\n",
    "        # Iterate over the sequence words and run every word through the encoder\n",
    "        for i in range(input_seqs.size(1)):\n",
    "            output, (hn, cn) = encoder(\n",
    "                input_seqs[:, i].unsqueeze(dim=1),\n",
    "                coordinates[:, i],\n",
    "                positions[:, i:i+1],\n",
    "                (hn, cn)\n",
    "            )\n",
    "            encoder_outputs[:, i:i+1, :] = output\n",
    "            encoder_hidden_states[:, i:i+1, :] = seq2seq_lstm.concat_hidden_states(hn[-last_n_states:]).unsqueeze(dim=1)\n",
    "\n",
    "        # Predict tokens of the target sequence by running the hidden state through\n",
    "        # the decoder\n",
    "        for i in range(0, target_seqs.size(1)):\n",
    "            output, (hn, cn), attention = decoder(\n",
    "                x=target_seqs[:, i].unsqueeze(dim=1),\n",
    "                coordinates=coordinates[:, i],\n",
    "                annotations=encoder_hidden_states,\n",
    "                position=positions[:, i:i+1],\n",
    "                hidden=(hn, cn)\n",
    "            )\n",
    "            # Select the indices of the most likely tokens\n",
    "            predicted_char = torch.argmax(output, dim=2)\n",
    "            predictions[:, i] = torch.argmax(output, dim=2).squeeze()\n",
    "            attention_matrix[:, :, i:i+1] = attention\n",
    "        \n",
    "        return predictions, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b4224ca-ab02-437b-a0ea-5704b1c01daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 20]), torch.Size([2, 20, 20]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction, attention_matrix = predict(input_seqs[0:2], coordinates[0:2], target_seqs[0:2])\n",
    "prediction.shape, attention_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1d6896f-5818-4718-a584-0aa59e9226de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1dc7984a5c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(attention_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476c3b2-d8ad-4506-8ace-814d9e9eefcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_swapped = g.random_swap(input_seqs[0], i=2).unsqueeze(dim=0)\n",
    "coords_swapped = g.random_swap(coordinates[0], i=2).unsqueeze(dim=0)\n",
    "prediction_swapped = predict(in_swapped, coords_swapped, target_seqs[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0408b19-3288-48ac-905b-88eb6ef4017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs[0:1] == in_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb4a03-69bd-45e1-960f-244da1c18417",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction == prediction_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf18d6-94da-42ce-8bad-86a9f2d16d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random sequence and its prediction from the model\n",
    "import random\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "predictions, attention_matrix = predict(input_seqs, coordinates, target_seqs)\n",
    "\n",
    "i = random.randint(0, predictions.size(0))\n",
    "print(\"MODEL INPUT\", vocab_in.decode_sequence(input_seqs[i].cpu().numpy()))\n",
    "print(\"MODEL OUTPUT\", vocab_out.decode_sequence(predictions[i].cpu().numpy()))\n",
    "print(\"TARGET OUTPUT\", vocab_out.decode_sequence(target_seqs[i][1:].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeae19b-77d4-4715-8068-9822af23009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = vocab_out.decode_sequence(predictions[i].cpu().numpy())\n",
    "prediction = list(filter(lambda x: x != '<end>', prediction))\n",
    "prediction = \"\".join(prediction)\n",
    "print(\"MODEL OUTPUT\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b7aff-cecf-476a-bd3c-ae737ff8aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(attention_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05457f9c-56b4-4aaa-ae20-c4e8b120ad62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
