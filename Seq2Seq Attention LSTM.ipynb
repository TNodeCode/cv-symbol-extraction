{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307e370c-40cb-4194-bf97-ad9e9ab745be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seqgen.seq_gen as g\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "197faa56-4d2f-4453-bbda-32c5b4115b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccbe91c-6286-4398-ad71-ee251ae1af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "num_layers=1\n",
    "embedding_dim = 16\n",
    "hidden_size=16\n",
    "batch_size=25\n",
    "max_length=10\n",
    "bidirectional=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78d88d5-47ef-4b8f-9d41-e1d8fb70c635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilof\\AppData\\Local\\Temp\\ipykernel_7768\\3989188473.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
      "C:\\Users\\tilof\\AppData\\Local\\Temp\\ipykernel_7768\\3989188473.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coordinates = torch.tensor(features[:, :, 1:])\n"
     ]
    }
   ],
   "source": [
    "features, target_seqs = g.generate_synthetic_training_data(batch_size, max_length=max_length, device=device, continue_prob=0.997, swap_times=0)\n",
    "input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
    "coordinates = torch.tensor(features[:, :, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437f41f6-057b-41e5-9d52-1744666d4aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25, 10, 5]),\n",
       " torch.Size([25, 10]),\n",
       " torch.Size([25, 10, 4]),\n",
       " torch.Size([25, 10]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, input_seqs.shape, coordinates.shape, target_seqs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae84cc-67fe-4487-b119-51c1e6563d7c",
   "metadata": {},
   "source": [
    "# The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c25b8f-93b5-4493-81e4-fb6c1f3a67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqgen.model import seq2seq_lstm\n",
    "from seqgen.vocabulary import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7974696-6918-47ff-93b7-14cc4517dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "load_from_checkpoint = False\n",
    "checkpoint_file = \"model_len25_biy_layers3.pt\"\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "encoder = seq2seq_lstm.EncoderRNN(vocab_size=len(vocab_in), embedding_dim=embedding_dim, num_layers=num_layers, max_length=max_length, hidden_size=hidden_size, bidirectional=bidirectional, pos_encoding=False).to(features.device)\n",
    "decoder = seq2seq_lstm.DecoderRNN2(embedding_dim=embedding_dim, num_layers=num_layers, max_length=max_length, hidden_size=hidden_size, vocab_size=len(vocab_out), bidirectional=bidirectional, pos_encoding=False).to(features.device)\n",
    "\n",
    "# Initialize optimizer for encoder and decoder\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "#positions = seq2seq_lstm.get_position_encoding(max_length, embedding_dim, device=device)\n",
    "positions = seq2seq_lstm.get_coordinate_encoding(coordinates, d=embedding_dim, device=device)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Load model weights from checkpoint\n",
    "if load_from_checkpoint:\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    encoder.load_state_dict(checkpoint['encoder_model_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_model_state_dict'])\n",
    "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "    num_layers = checkpoint['num_layers']\n",
    "    embedding_dim = checkpoint['embedding_dim']\n",
    "    hidden_size = checkpoint['hidden_size']\n",
    "    bidirectional = checkpoint['bidirectional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d63a2d8-bced-44ec-b32b-19d5f3055885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run word 1 of all 25 sequences through the encoder\n",
      "Run word 2 of all 25 sequences through the encoder\n",
      "Run word 3 of all 25 sequences through the encoder\n",
      "Run word 4 of all 25 sequences through the encoder\n",
      "Run word 5 of all 25 sequences through the encoder\n",
      "Run word 6 of all 25 sequences through the encoder\n",
      "Run word 7 of all 25 sequences through the encoder\n",
      "Run word 8 of all 25 sequences through the encoder\n",
      "Run word 9 of all 25 sequences through the encoder\n",
      "Run word 10 of all 25 sequences through the encoder\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encoder hidden state and cell state with zeros\n",
    "hn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "cn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "\n",
    "_hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "\n",
    "# Iterate over the sequence words and run every word through the encoder\n",
    "for i in range(input_seqs.shape[1]):\n",
    "    # Run the i-th word of the input sequence through the encoder.\n",
    "    # As a result we will get the prediction (output), the hidden state and the cell state.\n",
    "    # The hidden state and cell state will be used as inputs in the next round\n",
    "    print(f\"Run word {i+1} of all {input_seqs.shape[0]} sequences through the encoder\")\n",
    "    output, (hn, cn) = encoder(input_seqs[:, i].unsqueeze(dim=1), coordinates[:, i], positions[:, i:i+1], (hn, cn))\n",
    "    encoder_outputs[:, i:i+1, :] = output\n",
    "    encoder_hidden_states[:, i, :] = seq2seq_lstm.concat_hidden_states(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbb3c2a2-1b0a-404a-9d8c-0ae7fbc8485c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25, 1, 32]),\n",
       " torch.Size([2, 25, 16]),\n",
       " torch.Size([2, 25, 16]),\n",
       " torch.Size([25, 10, 32]),\n",
       " torch.Size([25, 10, 32]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, hn.shape, cn.shape, encoder_hidden_states.shape, encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29054ba-e5b3-405f-bc5b-149f8862da81",
   "metadata": {},
   "source": [
    "# The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eda91121-2c2d-4531-b7ee-d67096698123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run word 1 through decoder torch.Size([2, 25, 16]) torch.Size([25, 10, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run word 2 through decoder torch.Size([2, 25, 16]) torch.Size([25, 10, 32])\n",
      "Run word 3 through decoder torch.Size([2, 25, 16]) torch.Size([25, 10, 32])\n",
      "Run word 4 through decoder torch.Size([2, 25, 16]) torch.Size([25, 10, 32])\n",
      "Run word 5 through decoder torch.Size([2, 25, 16]) torch.Size([25, 10, 32])\n",
      "Run word 6 through decoder torch.Size([2, 25, 16]) torch.Size([25, 10, 32])\n",
      "Run word 7 through decoder torch.Size([2, 25, 16]) torch.Size([25, 10, 32])\n",
      "Run word 8 through decoder torch.Size([2, 25, 16]) torch.Size([25, 10, 32])\n",
      "Run word 9 through decoder torch.Size([2, 25, 16]) torch.Size([25, 10, 32])\n",
      "Run word 10 through decoder torch.Size([2, 25, 16]) torch.Size([25, 10, 32])\n",
      "LOSS 3.2226314544677734\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "\n",
    "# Iterate over words of target sequence and run words through the decoder.\n",
    "# This will produce a prediction for the next word in the sequence\n",
    "for i in range(0, target_seqs.size(1)):\n",
    "    print(f\"Run word {i+1} through decoder\", hn.shape, encoder_hidden_states.shape)\n",
    "    output, (hn, cn), attention = decoder(\n",
    "        x=target_seqs[:, i].unsqueeze(dim=1),\n",
    "        coordinates=coordinates[:, i],\n",
    "        annotations=encoder_hidden_states,\n",
    "        position=positions[:, i:i+1],\n",
    "        hidden=(hn, cn)\n",
    "    )\n",
    "    loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "\n",
    "print(\"LOSS\", loss.item() / max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02683624-ab59-49b1-b645-105231c21692",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e4bb73f-6640-4821-a46a-ceda36ef2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#context_vector, attention = attn(hn, encoder_hidden_states, logging=True)\n",
    "#context_vector.shape, attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c59e1-3885-4a12-8696-9fcf3da9cf9e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f9ad9-7ae1-4b49-99b8-490eacfd5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(100000):\n",
    "    # With a certain chance present the model the true predictions\n",
    "    # instead of its own predictions in the next iteration\n",
    "    use_teacher_forcing_prob = 0.5\n",
    "    use_teacher_forcing = random.random() < use_teacher_forcing_prob\n",
    "    \n",
    "    # Get a batch of trianing data\n",
    "    features, target_seqs = g.generate_synthetic_training_data(batch_size, max_length=max_length, continue_prob=0.99, device=device, swap_times=0)\n",
    "    features = features.to(device)\n",
    "    target_seqs = target_seqs.to(device)\n",
    "    input_seqs = torch.tensor(features[:, :, 0]).to(torch.int64)\n",
    "    coordinates = torch.tensor(features[:, :, 1:])\n",
    "\n",
    "    # Initialize the encoder hidden state and cell state with zeros\n",
    "    hn_enc = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "    cn_enc = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "    \n",
    "    # Initialize encoder outputs tensor\n",
    "    last_n_states = 2 if bidirectional else 1\n",
    "    _hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "    encoder_hidden_states = torch.zeros((batch_size, max_length, _hidden_size*num_layers)).to(device)\n",
    "    encoder_outputs = torch.zeros((batch_size, max_length, _hidden_size)).to(device)\n",
    "    \n",
    "    # Set gradients of all model parameters to zero\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "    \n",
    "    ####################\n",
    "    #     ENCODING     #\n",
    "    ####################\n",
    "\n",
    "    # Iterate over the sequence words and run every word through the encoder\n",
    "    for i in range(input_seqs.shape[1]):\n",
    "        # Run the i-th word of the input sequence through the encoder.\n",
    "        # As a result we will get the prediction (output), the hidden state (hn) and the cell state (cn).\n",
    "        # The hidden state and cell state will be used as inputs in the next round\n",
    "        output, (hn_enc, cn_enc) = encoder(\n",
    "            input_seqs[:, i].unsqueeze(dim=1),\n",
    "            coordinates[:, i],\n",
    "            positions[:, i:i+1],\n",
    "            (hn_enc, cn_enc)\n",
    "        )\n",
    "        # Save encoder outputs and states for current word\n",
    "        encoder_outputs[:, i:i+1, :] = output\n",
    "        encoder_hidden_states[:, i, :] = seq2seq_lstm.concat_hidden_states(hn)\n",
    "\n",
    "    ####################\n",
    "    #     DECODING     #\n",
    "    ####################\n",
    "    \n",
    "    accuracy = 0.0\n",
    "\n",
    "    # The first words that we be presented to the model is the '<start>' token\n",
    "    prediction = target_seqs[:, 0]\n",
    "    \n",
    "    # The initial hidden state of the decoder is the final hidden state of the encoder\n",
    "    hn_dec, cn_dec = hn_enc, cn_enc\n",
    "    \n",
    "    # Iterate over words of target sequence and run words through the decoder.\n",
    "    # This will produce a prediction for the next word in the sequence\n",
    "    for i in range(1, target_seqs.size(1)):\n",
    "        # Run word i through decoder and get word i+1 and the new hidden state as outputs\n",
    "        if use_teacher_forcing:\n",
    "            output, (hn_dec, cn_dec), attention = decoder(\n",
    "                x=prediction.unsqueeze(dim=1),\n",
    "                coordinates=coordinates[:, i-1],\n",
    "                annotations=encoder_hidden_states,\n",
    "                position=positions[:, i:i+1],\n",
    "                hidden=(hn_dec, cn_dec)\n",
    "            )\n",
    "        else:\n",
    "            output, (hn_dec, cn_dec), attention = decoder(\n",
    "                x=prediction.unsqueeze(dim=1),\n",
    "                coordinates=coordinates[:, i-1],\n",
    "                annotations=encoder_hidden_states,\n",
    "                position=positions[:, i:i+1],\n",
    "                hidden=(hn_dec, cn_dec)\n",
    "            )\n",
    "\n",
    "            # Get the predicted classes of the model\n",
    "            topv, topi = output.topk(1)\n",
    "            prediction = topi.squeeze()    \n",
    "        loss += criterion(output.squeeze(), target_seqs[:, i])\n",
    "        accuracy += float((prediction == target_seqs[:, i]).sum() / (target_seqs.size(0)*target_seqs.size(1)))\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print_every = 100\n",
    "    if not epoch % print_every:\n",
    "        _accuracy = sum(accuracies[-print_every:]) / print_every\n",
    "        print(f\"LOSS after epoch {epoch}\", loss.item() / (target_seqs.size(1)), \"ACCURACY\", _accuracy)\n",
    "\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    accuracy = 0.0\n",
    "\n",
    "    # Update weights of encoder and decoder\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e3605-8f59-42ee-b260-5bb500518d00",
   "metadata": {},
   "source": [
    "#### Save model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67bec2-ccfd-4492-bd1a-e51fad94fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "model_data = {\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length\n",
    "}\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'encoder_model_state_dict': encoder.state_dict(),\n",
    "    'decoder_model_state_dict': decoder.state_dict(),\n",
    "    'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "    'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    \"history\": history,\n",
    "    \"lr\": lr,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"bidirectional\": bidirectional,\n",
    "}, \"model_\" + date_time + \".pt\")\n",
    "\n",
    "\n",
    "with open(\"training_\" + date_time + '.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56c813-13d3-45a2-b9e1-47fed03bcf3c",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "We run our input sequences through the model and get output seuences. Then we decode the output sequences with the Vocabulary class and get our final latex code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0fb8ff7-8d9a-4c28-8f95-264703cfe636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_seqs, coordinates, target_seqs):\n",
    "    vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "    vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "    predictions = torch.zeros(target_seqs.shape)\n",
    "    attention_matrix = torch.zeros((input_seqs.shape[0], input_seqs.shape[1], input_seqs.shape[1]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize the encoder hidden state and cell state with zeros\n",
    "        hn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "        cn = encoder.initHidden(input_seqs.shape[0], device=features.device)\n",
    "        \n",
    "        _hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        last_n_states = 2 if bidirectional else 1\n",
    "        encoder_hidden_states = torch.zeros((input_seqs.shape[0], max_length, _hidden_size)).to(device)\n",
    "        encoder_outputs = torch.zeros((input_seqs.shape[0], max_length, _hidden_size)).to(device)\n",
    "\n",
    "        # Iterate over the sequence words and run every word through the encoder\n",
    "        for i in range(input_seqs.size(1)):\n",
    "            output, (hn, cn) = encoder(\n",
    "                input_seqs[:, i].unsqueeze(dim=1),\n",
    "                coordinates[:, i],\n",
    "                positions[:, i:i+1],\n",
    "                (hn, cn)\n",
    "            )\n",
    "            encoder_outputs[:, i:i+1, :] = output\n",
    "            encoder_hidden_states[:, i:i+1, :] = seq2seq_lstm.concat_hidden_states(hn[-last_n_states:]).unsqueeze(dim=1)\n",
    "\n",
    "        # Predict tokens of the target sequence by running the hidden state through\n",
    "        # the decoder\n",
    "        for i in range(0, target_seqs.size(1)):\n",
    "            output, (hn, cn), attention = decoder(\n",
    "                x=target_seqs[:, i].unsqueeze(dim=1),\n",
    "                coordinates=coordinates[:, i],\n",
    "                annotations=encoder_hidden_states,\n",
    "                position=positions[:, i:i+1],\n",
    "                hidden=(hn, cn)\n",
    "            )\n",
    "            # Select the indices of the most likely tokens\n",
    "            predicted_char = torch.argmax(output, dim=2)\n",
    "            predictions[:, i] = torch.argmax(output, dim=2).squeeze()\n",
    "            attention_matrix[:, :, i:i+1] = attention\n",
    "        \n",
    "        return predictions, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b4224ca-ab02-437b-a0ea-5704b1c01daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10]), torch.Size([2, 10, 10]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction, attention_matrix = predict(input_seqs[0:2], coordinates[0:2], target_seqs[0:2])\n",
    "prediction.shape, attention_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1d6896f-5818-4718-a584-0aa59e9226de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e9b9c54040>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(attention_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476c3b2-d8ad-4506-8ace-814d9e9eefcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_swapped = g.random_swap(input_seqs[0], i=2).unsqueeze(dim=0)\n",
    "coords_swapped = g.random_swap(coordinates[0], i=2).unsqueeze(dim=0)\n",
    "prediction_swapped = predict(in_swapped, coords_swapped, target_seqs[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0408b19-3288-48ac-905b-88eb6ef4017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs[0:1] == in_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb4a03-69bd-45e1-960f-244da1c18417",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction == prediction_swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf18d6-94da-42ce-8bad-86a9f2d16d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random sequence and its prediction from the model\n",
    "import random\n",
    "\n",
    "vocab_in = Vocabulary(vocab_filename=\"seqgen/vocab_in.txt\")\n",
    "vocab_out = Vocabulary(vocab_filename=\"seqgen/vocab_out.txt\")\n",
    "\n",
    "predictions, attention_matrix = predict(input_seqs, coordinates, target_seqs)\n",
    "\n",
    "i = random.randint(0, predictions.size(0))\n",
    "print(\"MODEL INPUT\", vocab_in.decode_sequence(input_seqs[i].cpu().numpy()))\n",
    "print(\"MODEL OUTPUT\", vocab_out.decode_sequence(predictions[i].cpu().numpy()))\n",
    "print(\"TARGET OUTPUT\", vocab_out.decode_sequence(target_seqs[i][1:].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeae19b-77d4-4715-8068-9822af23009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = vocab_out.decode_sequence(predictions[i].cpu().numpy())\n",
    "prediction = list(filter(lambda x: x != '<end>', prediction))\n",
    "prediction = \"\".join(prediction)\n",
    "print(\"MODEL OUTPUT\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b7aff-cecf-476a-bd3c-ae737ff8aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(attention_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05457f9c-56b4-4aaa-ae20-c4e8b120ad62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
